{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Climate Data Dissemination System The Climate Data Dissemination System (CDDS) is a Python-based system that manages the reprocessing of HadGEM3 and UKESM1 climate model data into a standards compliant ( CMOR ) form suitable for publication and sharing. The primary driver behind CDDS was the CMIP6 project and CDDS was used, and is continuing to be used, to deliver a large amount of data to the Centre for Environmental Data Archival (CEDA) for publication to ESGF. CDDS has recently been adapted to allow for the easy addition of both models and projects, provided that they follow the structure for CMIP6, i.e. via predefined activities (MIPS), source ids (models) and experiments. As of version 2.2.4 CDDS supports production of data for CMIP6 and for a Met Office internal project GCModelDev where Met Office scientists are encouraged to request additional activities and experiments that can be used to support their science. Note that the GCModelDev project is not intended to prepare data for publication -- anyone wanting to publish data or prepare it for an external project is encouraged to contact the CDDS team or start a discussion here. Contact You can contact the CDDS team via cdds@metoffice.gov.uk. CMIP6 Processing Simulation tickets for CMIP6 work can be raised on the CDDS Trac system (SRS login required) via Open a new operational simulation ticket (v3.0) CMIP6 Operational Procedure CMIP6 Simulation Ticket Review Procedure","title":"About"},{"location":"#climate-data-dissemination-system","text":"The Climate Data Dissemination System (CDDS) is a Python-based system that manages the reprocessing of HadGEM3 and UKESM1 climate model data into a standards compliant ( CMOR ) form suitable for publication and sharing. The primary driver behind CDDS was the CMIP6 project and CDDS was used, and is continuing to be used, to deliver a large amount of data to the Centre for Environmental Data Archival (CEDA) for publication to ESGF. CDDS has recently been adapted to allow for the easy addition of both models and projects, provided that they follow the structure for CMIP6, i.e. via predefined activities (MIPS), source ids (models) and experiments. As of version 2.2.4 CDDS supports production of data for CMIP6 and for a Met Office internal project GCModelDev where Met Office scientists are encouraged to request additional activities and experiments that can be used to support their science. Note that the GCModelDev project is not intended to prepare data for publication -- anyone wanting to publish data or prepare it for an external project is encouraged to contact the CDDS team or start a discussion here.","title":"Climate Data Dissemination System"},{"location":"#contact","text":"You can contact the CDDS team via cdds@metoffice.gov.uk.","title":"Contact"},{"location":"#cmip6-processing","text":"Simulation tickets for CMIP6 work can be raised on the CDDS Trac system (SRS login required) via Open a new operational simulation ticket (v3.0) CMIP6 Operational Procedure CMIP6 Simulation Ticket Review Procedure","title":"CMIP6 Processing"},{"location":"changes_cdds/","text":"Release 3.1.0rc1, February 11, 2025 MIP Convert and CMOR log files are copied into the cylc task directory and are accessible via cylc review (CDDSO-495) Cylc workflows have been integrated into the CDDS code base rather than being checked out from the roses-u repository (CDDSO-494) Processing for multiple streams is now handled within a single Cylc workflow (CDDSO-302) A validation tool has been added for the model parameters file SCRIPT_NAME_HERE (CDDSO-511) CDDS Prepare will remove MIP Convert templates from the configure proc directory to avoid inconsistencies in processing. A --reconfigure command line argument has been added to enable the regeneration of the mip convert config templates from the command line (CDDSO-593) Ancillary STASH variables can now be configured via the CDDS Plugins (CDDSO-582) Added support for HadGEM3-GC50-N640ORCA12 model, which includes allowing for alternative coordinate bounds variables in NEMO output files (CDDSO-543) Updated CMOR to v3.9.0 (CDDSO-589) Migrated content from Sphinx docs to mkdocs (CDDSO-598, CDDSO-600) Release 3.0.6, January 21, 2025 Pass model parameters file to CDDS suite during convert process (CDDSO-578) Handling quarterly data during the extract process correctly (CDDSO-568) Add documentation to deployment page (CDDSO-556) Allow QC file size limit to be overloaded in the request file (CDDSO-577, CDDSO-587) Fix checkout processing workflow on JASMIN (CDDSO-574) Add documentation for model parameters file (CDDSO-545) Release 3.0.5rc1, December 5, 2024 (only to be used for LESFMIP) Ensured that quarterly PP files, i.e. those that have 3 months of output at monthly/daily frequency can be extracted and picked up by the wrapper to MIP convert (CDDSO-560, CDDSO-568) Release 3.0.4, November 19, 2024 Enabled sub-annual slicing through the slicing option in the common section of the Request config file. Streams can individually be set to have month slicing where large amounts of memory usage are expected (e.g. hourly data) while the default is year , i.e. annual slicing (CDDSO-342) Corrected bug preventing QC from completing for CMIP6 like processing introduced in CDDS v3.0.2 (CDDSO-532) Changed concatenation code to handle monthly and daily datasets in the same stream where the first two facets of the file name are not unique to a dataset (CDDSO-548) Updated HadREM3 sizing info (CDDSO-548) Release 3.0.3, October 24, 2024 Grid descriptions can now be modified through model parameter files (CDDSO-534, CDDSO-528) The inclusion of rotated coordinates in output files can now be forces through the introduction of the force_coordinate_rotation flag into the Request config files (CDDSO-529) Global attribute caching has been introduced to improve QC speed (CDDSO-514) Release 3.0.2, September 27, 2024 Restored the approved variables file argument to the move_in_mass script (CDDSO-523) Ensure that hourly data and submonthly data is correctly recognised by CDDS when running MIP Convert (CDDSO-512) Prevent Extract tasks from failing when assessing moose command block size if some data is already present (CDDSO-515) Correctly handle leap years (Gregorian calendar) during concatenation task (CDDSO-513) Correctly handle sub annual chunk size for files in concatenation setup task (CDDSO-504) Correctly overload model parameters file when reading request file (CDDSO-508) Add functionality to remove halo columns and rows from atmosphere data (CDDSO-521) Correct parsing of filename facet for CORDEX in QC (CDDSO-519) Ensure clear checking of time coordinate in QC with the corresponding frequency and length in QC (CDDSO-517) Release 3.0.1, August 21, 2024 Verbose mode is not added as default option when running CDDS Convert workflow to avoid Cylc issue (CDDSO-490) Added new options to delete pre-existing proc and data directories in request.cfg (CDDS-408) Changes to port CDDS to Azure (CDDS-481, CDDSO-497) QC and archive will now fail if nothing is to check or to archive (CDDSO-459) Improved handling of failed moo commands (CDDSO-487) Extract will now fail when it cannot identify the chunking for moo commands (CDDSO-487) Prepare to accept variables like 6hrPlevPt/zg7h (CDDSO-473) The sim review script now accepts the request config file as its input argument (CDDSO-461) Added a new script cdds_clean to clean out the CDDS Convert workflows (CDDSO-450) Added support for CORDEX (CDDSO-421) Replacde hardcoded paths using the CDDS_ETC variable in tests (CDDSO-492) Extract will now fail if the tape check fails (CDDSO-482) Added validations for many fields within the request config file (CDDSO-292) Allow inheritance of request fields from a template (CDDSO-496) Release 3.0.0, May 15, 2024 The request JSON file, used to control CDDS in previous versions, has been replaced with a config file containing all required options and the command line interfaces to CDDS tools have been correspondingly updated (CDDSO-434, CDDSO-466, CDDSO-424, CDDSO-301, CDDSO-411, CDDSO-299, CDDSO-298, CDDSO-296, CDDSO-297, CDDSO-438, CDDSO-423, CDDSO-386, CDDSO-300, CDDSO-395) CDDS Documentation is now maintained through mkdocs and served via github pages (CDDSO-427, CDDSO-430, CDDSO-409, CDDSO-429, CDDSO-426, CDDSO-425, CDDSO-428, CDDSO-443, CDDSO-446, CDDSO-442) MIP Convert failure will now lead to cylc task failure if the continue_if_mip_convert_failed setting in the request ( [conversion] section) is set to False in the request config file (CDDSO-387) The branch of the CDDS Convert workflow being used by default is controlled by the CDDS_CONVERT_WORKFLOW_BRANCH environment variable (CDDSO-452) Log messages are now sent to standard out by default, and will appear in job.out files within the cylc log directory (CDDSO-34) CMOR has been updated to v3.8 and Python to 3.10 (CDDSO-411) Simplified model config files by removing files_per_year field (CDSSO-307) Release 2.5.10, December 12, 2024 No changes Release 2.5.9, June 25, 2024 Fixed a bug in submission code that made archiving script unusable (CDDSO-477) QC now allows missing standard name attribute if it is defined as empty in MIP Tables (CDDSO-478) Release 2.5.8, February 29, 2024 Added submission queues to support CMIP6Plus publication (CDDSO-413) Removed modified time check from CDDS workflow to avoid workflow freezing when moved to a different cylc server (CDDSO-405) Release 2.5.7, February 21, 2024 Chunking of large moose queries in Extract is now correctly handled (CDDSO-398) QC now supports Conventions field in a more portable way (CDDSO-407) Concatenation code can now cope with monthly files of monthly mean data (CDDSO-405) Extensions and fixes to support CP4A processing via GCModel Dev (CDDSO-354, CDDSO-401, CDDSO-405) Release 2.5.6, February 09, 2024 Added HadGEM3-GC5 and eUKESM1-1-ice model configurations to GCModelDev plugin (CDDSO-390, CDDSO-391) Extraction code now correctly handles coordinate variables when extracting data from NEMO diaptr and scalar files (CDDSO-394) The data version used in archiving can now be specified in the CDDS workflow to support additional runs of CDDS appending to the same data set (CDDSO-393) Release 2.5.5, January 18, 2024 The name of netCDF bounds coordinates variables are now configurable via plugins to support retrieval of NEMO4 ocean data (CDDSO-346) GCModelDev plugin now has an N96 GC3.05 generic model configuration and the list streams for the N216 equivalent has been extended (CDDSO-381) The option to overload model configuration via command line now correctly operates (CDDSO-376, CDDSO-379, CDDSO-380) The create_variables_csv_file script now also outputs the stream for each variable (CDDSO-332) Release 2.5.4, December 13, 2023 Corrected an issue with a legacy configuration file that prevented the submission of CMIP6 data (CDDSO-371) Implement annual ocean diagnostics for the GCOyr MIP table (CDDSO-369) Release 2.5.3, November 22, 2023 Adaptations needed for new CMIP6Plus MIP tables (CDDSO-336, CDDSO-365) Adaptations needed to support use of sub_experiment_ids that are not \"none\" (CDDSO-365, CDDSO-362) Extract can again correctly retrieve sub-daily data (CDDSO-358) The update_directives script in the per-stream suite now correctly calculate upper limits for memory / wall time / local storage (CDDSO-360) Release 2.5.2, October 18, 2023 Polar row masking is now specified directly in the MIP Convert config files (CDDSO-331) Memory, storage and time limits for SLURM jobs are now dynamically calculated after the first three cycles of the processing suite (CDDSO-212) CDDS can now handle hourly data files within a stream (CDDSO-349) The correct filenames for ocean data from ensemble suites are now used again (CDDSO-350) The CDDS suite uses a consistent calendar to the request, which ensures that concatenation processes, QC and archiving are correctly triggered (CDDSO-351) All options are now passed through to CDDS Convert by the CDDS processing workflow (CDDSO-348) Release 2.5.1, August 4, 2023 Ensure that recent versions of the NCO tools used for concatenation of files do not modify metadata, leading to publication issues (CDDSO-327) Fix creation_date validation bug in QC when data is written on the 31st of a month (CDDSO-326) Release 2.5.0, July 27, 2023 Migrate CDDS to Cylc 8 (CDDSO-282, CDDSO-285, CDDSO-184) Introduce gregorian calendar support (CDDSO-231, CDDSO-235, CDDSO-240, CDDSO-244) Plugin and code development to support Regional models for projects such as CORDEX (CDDSO-233, CDDSO-68, CDDSO-74, CDDSO-261, CDDSO-149) CDDS now uses ISO datetime format for all time values in CDDS and MIP Convert config files (CDDSO-313, CDDSO-45) Relocate test and etc directories to support alternate platforms and migration to azure SPICE (CDDSO-131, CDDSO-273) Replace pep8 with pycodestyle (CDDSO-207) Separated extract validation into a separate script & task (CDDSO-243) Introduced retries to workflow tasks when submission fails (CDDSO-314) Amalgamated CDDS processing and ensemble processing workflows. Previous tools based on u-cq805 and u-cr273 are deprecated (CDDSO-319) Updated CMOR to version 3.7.2, which required iris update to 3.4.1 (CDDSO-321) Release 2.4.7, June 29, 2023 Fix a bug in the dehaloing script that prevented it from reading default command line arguments (CDDSO-308) Release 2.4.6, June 23, 2023 Fix a bug in the extraction code where the non-uniform allocation of data in a simulation to a large number of tapes can lead to failure of cdds extract (CDDSO-245, CDDSO-289) Correct a bug in the HadGEM3 model config files that prevented extraction of data from the diaptr sub-stream in the NEMO ocean data for the GCModelDev project (CDSO-293) Release 2.4.5, May 22, 2023 Added HighResMIP models to CMIP6 plugin (CDDSO-158, cherry-picked from main) Release 2.4.4, May 4, 2023 Added options to use a \"relaxed\" approach to MIP name (activity_id), and experiment id, that does not force checks from the CVs. This will allow users to process data as any experiment id they like, but at the risk of typos due to the lack of checks against a controlled vocabulary (CDDSO-253 , CDDSO-267, CDDSO-264) prepare_alter_variable_list can now perform insert commands again as plugin was not being loaded, which was needed for setting streams (CDDSO-269) checkout_processing_suite now sets the streams in the suite based on those in the request file. (CDDSO-271) Release 2.4.3, March 31, 2023 Add support for retries in the run_extract task within the CDDS Convert Suite (CDDSO-258) Release 2.4.2, March 1, 2023 Add support for filepaths with ensemble_id for netCDF model output (CDDSO-238). Release 2.4.1, January 18, 2023 Create a Cylc workflow for end to end processing (CDDSO-198, 200) Create a Cylc workflow for ensemble processing (CDDSO-199) Implement the checkout_processing_suite script for checking a copy of the CDDS Processing Suite (CDDSO-210) Create a Rose app for generating request.json files (CDDSO-3) Fix a bug where cdds_convert expect external plugins environmental variables to be set even if no plugin was used (CDDSO-216) Release 2.4.0, September 12, 2022 Refactoring of all CDDS packages into one package excluding MIP Convert (CDDSO-132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144) Move test execution from nose to pytest (CDDSO-128) Add sphinx docs (CDDSO-169) diaptr files for HadGEM3-GC31-MM can now be extracted from MASS (CDDSO-191) Release 2.3.2, September 01, 2022 (cdds_common): Default stream information for variables now held by Project level plugins (CDDSO-143) (cdds_configure): Log message issued when a stream cannot be found for a variable is now CRITICAL (CDDSO-178) (cdds_configure): GCAmon and GCLmon MIP tables from GCModelDev are now included in dictionary of default grids and will not be ignored (CDDSO-178) (cdds_convert): Logger now set up before configure step is called allowing log information to be output (CDDSO-177) (cdds_convert): MIP Era now correctly propagated through to CDDS rose suite (CDDSO-180) (hadsdk): Change log message to CRITICAl for missing grid/stream. (hadsdk): Prepare a MIP convert branch to produce sample decadal data with additional seasonal forecasting metadata Release 2.3.1, June 29, 2022 (cdds_common): Added auto-genereated table of CDDS variable mappings (CDDSO-120) (cdds_common): Fix inconsistency in case between mip era and plugin name (CDDSO-132) (extract): remove_ocean_haloes script now works after change to load plugin (CDDSO-152) Release 2.3.0, May 24, 2022 (cdds_common): Development moved to github (cdds_common): Version string now includes git commit hash (CDDSO-93) (cdds_configure): Development moved to github (cdds_convert): Development moved to github (cdds_prepare): Development moved to github (cdds_qc): Development moved to github (cdds_qc_plugin_cf17): Development moved to github (cdds_qc_plugin_cmip6): Development moved to github (cdds_transfer): Development moved to github (extract): Development moved to github (hadsdk): Development moved to github (transfer): Development moved to github Release 2.2.5, May 4, 2022 (cdds_common): The atmosphere timestep, used in some mappings, is now supplied via the ModelParameters class rather than a dictionary. This is only used for request JSON file creation (#2571) (cdds_configure): For non-CMIP6 projects the correct grid label and grid information is applied for variables where the grid is different to the default for that mip table (#2570) (cdds_prepare): When inserting variables into CMIP6 processing the correct stream information is included (#2569) (hadsdk): For non-CMIP6 projects the correct grid label and grid information is applied for variables where the grid is different to the default for that mip table (#2570) (hadsdk): The atmosphere timestep, used in some mappings, is now supplied via the ModelParameters class rather than a dictionary. This is only used for request JSON file creation (#2571) Release 2.2.4, April 22, 2022 (cdds_common): Added UKESM1-1-LL model to CMIP6 and GCModelDev projects (#2545) (transfer): Removed further calls to the moo test command when archiving data (#2559, #2560) Release 2.2.3, April 7, 2022 (cdds_convert): The default cycling frequency will reduce to match the run bounds length when the run bounds length is smaller than cycling frequency. (#2006) (cdds_prepare): Reintroduce the capability to fall-back on default CMIP6 stream mappings when parsing user variable file (#2555) (cdds_qc): Ignore QC errors caused by non-existant dimensions (#2548) (cdds_qc_plugin_cf17): The implementation of the geographical region checker has been corrected (#2548) (extract): MASS paths with and without the ensemble_id in them are now handled (#2522) (transfer): moose listing commands are used rather than `moo test` in order to limit the number of commands talking to MASS, thereby limiting our vulnerability to transient or load dependent errors, and increases performance (#2553) Release 2.2.2, March 18, 2022 (cdds_common): Addition of GCModelDev plugin with core CMIP6 models (#2519) (cdds_prepare): Streams can now be set on creation of the requested variables file (#2498) (cdds_qc): QC checks now refer to provided controlled vocabulary for non-CMIP6 projects (#2542) (cdds_qc_plugin_cmip6): QC checks now refer to provided controlled vocabulary for non-CMIP6 projects (#2542) Release 2.2.1, February 15, 2022 (cdds_common): UKESM1-ice-LL model information now loaded correctly (#2530) (cdds_convert): --external_plugins argument now passed through to all tasks (#2511) (cdds_qc): CF checks are now correctly filtered for CMIP6 data (#2531) (cdds_qc_plugin_cf17): CF checks are now correctly filtered for CMIP6 data (#2531) Release 2.2.0, February 9, 2022 (cdds_common): New module to take common code across CDDS and will ultimately replace hadsdk (#2460) (cdds_common): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_common): Update to use python 3.8 (#2438) (cdds_configure): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_configure): Update to use python 3.8 (#2438) (cdds_configure): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_convert): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_convert): Update to use python 3.8 (#2438) (cdds_convert): Enable the use of CDDS for ensemble class simulations (#2501, #2471) (cdds_convert): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_convert): When file concatenation is not required, i.e. when a single cycle covers the whole duration of the processing, files are now moved to the correct destination rather than being left in the _concat directory (#2521) (cdds_prepare): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_prepare): Update to use python 3.8 (#2438) (cdds_prepare): Enable the use of CDDS for ensemble class simulations (#2501, #2471) (cdds_prepare): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_qc): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_qc): Update to use python 3.8 (#2438) (cdds_qc): CDDS QC now uses the community version of IOOS/compliance-checker rather than a fork (#2381) (cdds_qc): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_qc): Fixed a bug where if the string _concat was used in the branch name two tests in cdds_qc would fail (#2521) (cdds_qc_plugin_cf17): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_qc_plugin_cf17): Update to use python 3.8 (#2438) (cdds_qc_plugin_cf17): CDDS QC now uses the community version of IOOS/compliance-checker rather than a fork (#2381) (cdds_qc_plugin_cf17): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_qc_plugin_cmip6): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_qc_plugin_cmip6): Update to use python 3.8 (#2438) (cdds_qc_plugin_cmip6): CDDS QC now uses the community version of IOOS/compliance-checker rather than a fork (#2381) (cdds_qc_plugin_cmip6): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_transfer): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_transfer): Update to use python 3.8 (#2438) (extract): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (extract): Update to use python 3.8 (#2438) (extract): Enable the use of CDDS for ensemble class simulations (#2501, #2471) (extract): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (hadsdk): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (hadsdk): Update to use python 3.8 (#2438) (hadsdk): Retired umfilelist and all connected code (#2438) (transfer): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (transfer): Update to use python 3.8 (#2438) Release 2.1.2, November 25, 2021 (extract): Extract will now repeat creating stream directories if the first time fails because of intermittent filesystem issues (#2429) (hadsdk): Fixed an import bug in the mip_table_editor script (#2463) Release 2.1.1, October 26, 2021 (cdds_prepare): prepare_generate_variable_list can now generate requested variables files using a user supplied list and the MIP tables being used, i.e. without reference to a Data Request (#2358) (extract): Added the path_reformatter command line tool from the cdds_utils directory (#2415) (hadsdk): Improved error logging for the write_rose_suite_request_json script (#2390) (transfer): Added scripts for listing submission queues and resending stored messages that failed during submission (#2427, #2428) Release 2.1.0, September 6, 2021 (cdds_configure): generate_user_config_files is now run by cdds_convert by default (#1902, #2425) (cdds_convert): The Extract, Configure, QC and archiving steps are now by default part of the conversion suite run by cdds_convert (#1902, #2383, #2425, #2432) (cdds_prepare): The --alternate_experiment_id argument no longer triggers CRITICAL issues` (#2380) (cdds_qc): qc_run_and_report is now run as part of the conversion suite by cdds_convert by default (#1902, #2425, #2432) (extract): cdds_extract is now run as part of the conversion suite by cdds_convert by default (#1902, #2425) (transfer): cdds_store is now run as part of the conversion suite by cdds_convert by default (#1902, #2425, #2432) (transfer): cdds_sim_review now handles per stream logs, qc reports and approved variables files (#1685) Release 2.0.7, July 15, 2021 (extract): The remove_ocean_haloes command line tool is now available to remove halo rows and columns from ocean data files (#1161) Release 2.0.6, June 29, 2021 (cdds_prepare): Added functionality to prepare_generate_variable_list to automatically deactivate variables following a set of rules hosted in the CDDS repository (default) or in a text file. This \\\"auto deactivation\\\" can be skipped using the --no_auto_deactivation argument and the --auto_deactivation_file_name option can be used to use a local file rather than the repository rules (#2405) Release 2.0.5, June 11, 2021 (cdds_qc): Floating point comparison will now not cause time contiguity checks to fail (#2396) Release 2.0.4, May 17, 2021 (cdds_prepare): Allow for correct handling of packages where there are multiple mips / activity ids. The primary, i.e. first, mip will be used for all directory and dataset ids (#2369) (cdds_prepare): Altered output of create_cdds_directory_structure to be a little more user friendly (#2369) (cdds_transfer): Allow for correct handling of packages where there are multiple mips / activity ids. The primary, i.e. first, mip will be used for all directory and dataset ids (#2369) (hadsdk): Allow for correct handling of packages where there are multiple mips / activity ids. The primary, i.e. first, mip will be used for all directory and dataset ids (#2369) (transfer): Allow for correct handling of packages where there are multiple mips / activity ids. The primary, i.e. first, mip will be used for all directory and dataset ids (#2369) Release 2.0.3, April 28, 2021 (transfer): CDDS store can now prepend files to an embargoed dataset provided the correct data version argument is provided (#2278) Release 2.0.2, April 22, 2021 (cdds_convert): Implemented changes necessary for conda deployment of CDDS code on JASMIN (#2240) (cdds_qc): QC can now use a user specified directory with MIP tables provided by a command line option (#2331) (hadsdk): Inventory can now be constructed or updated from CREPP database dump on JASMIN (#2148) (hadsdk): Corrected masking of polar rows for the ORCA12 grid (#2211) (hadsdk): Implemented changes necessary for conda deployment of CDDS code on JASMIN (#2240) Release 2.0.1, March 25, 2021 (cdds_qc): QC now correctly interprets the values within string coordinates, such as basin name (#2284) (extract): When running for a particular stream the stream name is now included in the log file name (#2014) (hadsdk): write_rose_suite_request_json can now create request files for the UKESM1-ice-LL model (#2264) (hadsdk): write_rose_suite_request_json now allows the apt stream (#2283) (transfer): CDDS store can now recover gracefully when continuing archiving following task failure due to issues with MASS (#2205) (transfer): CDDS store can now correctly handle pre-pending files to datasets and there are clearer error messages when issues arise (#2222) Release 2.0.0, February 24, 2021 (cdds_configure): Updated CDDS codebase to Python 3.6. (cdds_convert): Updated CDDS codebase to Python 3.6. (cdds_prepare): Updated CDDS codebase to Python 3.6. (cdds_qc): Updated CDDS codebase to Python 3.6. (cdds_qc_plugin_cf17): Updated CDDS codebase to Python 3.6. (cdds_qc_plugin_cmip6): Updated CDDS codebase to Python 3.6. (cdds_transfer): Updated CDDS codebase to Python 3.6. (extract): Updated CDDS codebase to Python 3.6. (hadsdk): Updated CDDS codebase to Python 3.6. (transfer): Updated CDDS codebase to Python 3.6. Release 1.6.5, February 22, 2021 (cdds_qc): Fixed a bug in diurnal cycle diagnostics checker (#2254). Release 1.6.4, February 11, 2021 (hadsdk): write_rose_suite_request_json now correctly handles fields that are commented in the rose-suite.info file, an issue that affects the use of this script for amip-like simulations in v1.6.3 (#2232) Release 1.6.3, February 9, 2021 (cdds_qc): Implemented support for diurnal cycle diagnostics (1hrCM frequency) (#1894) (hadsdk): Errors in rose suite metadata (rose-suite.info file) now cause write_rose_suite_request_json to fail rather than warn and branch dates are correctly handled (#1477) (hadsdk): Users can now override the start and end dates for processing when creating request files with write_rose_suite_request_json (#2212) (transfer): The failure of [moo put commands is now handled more clearly (#2212) Release 1.6.2, January 11, 2021 (cdds_qc_plugin_cmip6): The cell_measures validation is now properly processing variables with --MODEL flag (#2193) Release 1.6.1, November 26, 2020 (cdds_qc_plugin_cmip6): The cell_measures validation is now properly processing global averages (#2171) (extract): Fixed a bug in the validation report, where the expected number of files would be printed instead of the actual file count (#2174) Release 1.6.0, November 05, 2020 (cdds_configure): All exceptions caught at the top script level are now logged as critical errors (#1968) (cdds_convert): All exceptions caught at the top script level are now logged as critical errors (#1968) (cdds_convert): Implemented code changes needed to support ARCHER filepath format and run CDDS on JASMIN (#2013) (cdds_convert): cdds_convert tasks now will fail if individual concatenation tasks do not succeed (#2045) (cdds_prepare): As a default option, prepare_generate_variable_list now performs CMIP6 inventory check to determine which variables have already been produced, deactivating them automatically (#1899) (cdds_prepare): Added requested ensemble size to the requested_variable_file file (#1900) (cdds_prepare): The write_rose_suite_request_json script now correctly writes the name of the rose suite branch in the [request json file (#2001) (cdds_prepare): All exceptions caught at the top script level are now logged as critical errors (#1968) (cdds_qc): All exceptions caught at the top script level are now logged as critical errors (#1968) (cdds_qc_plugin_cmip6): Consistency checks for netCDF variable attributes are now correctly applied (#2001) (cdds_qc_plugin_cmip6): Replaced the CMIP6 Controlled Vocabulary tables with the ones bundled with CMOR (#1808) (cdds_transfer): All exceptions caught at the top script level are now logged as critical errors (#1968) (extract): All exceptions caught at the top script level are now logged as critical errors (#1968) (hadsdk): Implemented a new command line tool search_inventory for exploring archived datasets and checking their publication status (#1903) (hadsdk): Implemented code changes needed to support ARCHER filepath format and run CDDS on JASMIN (#2013) Release 1.5.5, October 20, 2020 (cdds_prepare): The requested variable list can now be constructed using an alternative experiment_id via a new command line argument to prepare_generate_variable_list . This should only be used under advice from the CDDS team (#2057) (cdds_transfer): move_in_mass now correctly includes variables, where the and name used in the output file is different, when submitting data for publication (#2049). Release 1.5.4, October 7, 2020 (cdds_convert): Updated file sizing configuration for the high-resolution MM model now yields files under 20 Gb (#2037) Release 1.5.3, September 16, 2020 (cdds_convert): Increased memory limits, TMPDIR storage and changed cycling frequencies (#2012) Release 1.5.2, September 4, 2020 (cdds_qc_plugin_cmip6): Fixed a bug that prevented from validation of some global attributes of simulations without a parent experiment (#2000) Release 1.5.1, August 20, 2020 (cdds_convert): The concatenation processes should no longer leave temporary netCDF files behind when the batch job hits its wall time (#1895) (cdds_convert): The check on the use of TMPDIR storage in the MIP Convert tasks is now correctly applied (#1943) (cdds_convert): The cycling frequencies for the conversion process can now been overridden from the command line (#1944) (extract): Information on the validation of the is now written to \\<stream>_validation.txt as well as to the log files (#1811) (transfer): cdds_store can now archive CF sites datawith frequency subhrPt (#1945) (transfer): cdds_store will now raise CRITICAL log messages, rather than ERROR when something goes wrong (#1934) Release 1.5.0, July 2, 2020 (cdds_configure): Settings and files needed for production of CF sites data are now correctly written to the MIP Convert template files (#1838) (cdds_convert): CDDS can now produce CF Sites data (#1838) (cdds_prepare): Added do-not-produce information to the requested variables file via a new producible attribute for each variable (#681) (cdds_prepare): Added tool to export variable information from the requested variables file to a CSV or text file; create_variables_table_file (#1793) (cdds_qc): CDDS QC can now check CF sites (subhr frequency) data (#1783) (extract): CDDS Extract now correctly reports failure through error codes and e-mails (#1807) (extract): CDDS Extract can now correctly retrieve daily MEDUSA data (#1803) Release 1.4.5, June 16, 2020 (hadsdk): Institution names and branching dates are now correctly processed and written by the write_rose_suite_request_json script (#1848, #1849) Release 1.4.4, June 1, 2020 (cdds_convert): Memory limits for ap4 stream have been extended to 12G to avoid task failure (#1843) (cdds_convert): The copying of files to $TMPDIR will now be retried twice and if all attempts fail the task will fail rather than allowing CRITICAL errors from MIP Convert (#1839) Release 1.4.3, May 12, 2020 (cdds_convert): CDDS can now process data for the model UKESM1-ice-LL (#1513) (cdds_convert): Added --scale-memory-limits argument to CDDS Convert to allow memory limits to be altered at run time if necessary (#1806) (cdds_prepare): CDDS can now process data for the model UKESM1-ice-LL (#1513) (cdds_prepare): Requested variable lists can now be generated using data request version 01.00.32 (#1800) (cdds_qc): CDDS can now process data for the model UKESM1-ice-LL (#1513) (cdds_qc_plugin_cf17): CDDS can now process data for the model UKESM1-ice-LL (#1513) (cdds_qc_plugin_cmip6): CDDS can now process data for the model UKESM1-ice-LL (#1513) (cdds_transfer): CDDS can now process data for the model UKESM1-ice-LL (#1513) (extract): CDDS can now process data for the model UKESM1-ice-LL (#1513) (hadsdk): CDDS can now process data for the model UKESM1-ice-LL (#1513) (hadsdk): Added write_rose_suite_request_json to construct a request JSON file from the rose-suite.info file within a CMIP6 suite (#1732) (transfer): CDDS can now process data for the model UKESM1-ice-LL (#1513) Release 1.4.2, April 30, 2020 (cdds_qc_plugin_cmip6): CDDS QC now handles variables where the cell_measures variable attribute is optional (#1801) (hadsdk): write_request_json now gives a useful error message when there is inconsistent data in CREM for a package (#1765) Release 1.4.1, April 28, 2020 (cdds_convert): CDDS Convert now does not raise an AttributeError relating to the --email-notifications argument when run (#1780) (cdds_qc): CDDS QC can now check hourly frequency datasets and does not raise a KeyError exception (#1782) Release 1.4.0, April 23, 2020 (cdds_convert): Sub-daily data can now be produced (#825, #1577, #1704) (cdds_convert): The memory requested for different tasks within the suites is now controlled on a per grid/stream basis (#941, #1474) (cdds_convert): E-mail alerts now issued by suites launched by CDDS Convert when completing or stalling (#1669) (cdds_qc): Variables where a coordinate has text values, e.g. Omon/hfbasin , can now be properly validated (#1456) (cdds_qc): Datasets at sub-daily frequencies can now be validated (#1578) (cdds_qc): CRITICAL failures are now logged for each dataset that fails a test (#1681) (extract): Extract can now extract data from sub-daily PP streams (#359) (extract): Extract processes for individual streams can now be run in separate tasks (#1619) (hadsdk): When writing request JSON files from CREM, warnings are issued if base dates are not 1850-01-01 (#1703) Release 1.3.4, March 27, 2020 (cdds_prepare): prepare_alter_variable_list now includes additional variable metadata allowing cdds_store_spice to operate correctly for these variables (#1621) (cdds_qc): QC has better support for processing a single stream using the --stream command line option (#1622) (cdds_transfer): move_in_mass now only lists directories in MASS connected with the Request provided, significantly reducing run time (#1649). (transfer): Errors arising from MOO failures are now properly handled and logged (#1701) (transfer): Transfer now supports processing variables with different output names to their variable ID, and has better support for processing a single stream using the --stream command line option (#1622) Release 1.3.3, February 25, 2020 (cdds_convert): cdds_convert script now correctly returns a non-zero error code on error (#1501). (cdds_convert): The conversion process can now be limited to specified streams via the --streams command line argument (#1594) (cdds_convert): The conditions under which an organise_files_final task have been modified to avoid scheduling duplicate file management processes (#1585) (extract): Extract will no longer log CRITICAL errors when processing data streams containing no retrievable variables (#1583, #1354). Release 1.3.2, January 27, 2020 (cdds_convert): CRITICAL error messages from MIP Convert are correctly captured in the critical_issues.log file, and known exceptions from within MIP Convert will no longer lead to task failure in CDDS Convert suites (#1533). (cdds_qc): Implemented file size checker (#1530) (extract): Clarified CRITICAL messages when PP files are found to be incomplete (#1528) (extract): Extract does not now raise an error when attempting to interpret the request file for HadGEM3-GC31-MM packages (#1563) (transfer): cdds_store now handles experiment ids including the - character (#1556) Release 1.3.1, January 20, 2020 (cdds_configure): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_convert): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_prepare): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_qc): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_qc_plugin_cf17): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_qc_plugin_cmip6): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_transfer): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (extract): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (hadsdk): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (transfer): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (transfer): cdds_store_spice now passes all arguments through to spice job (#1511) (transfer): cdds_store behaves correctly when there are no files to archive (#1520) Release 1.3.0, January 14, 2020 (cdds_prepare): The new format of the approved variables file is now handled (#1175). (cdds_prepare): Added EmonZ/sltbasin to the list of known good variables to avoid CRITICAL issue for UKESM1 processing (#1437) (cdds_qc): A column with dataset filepaths has been added to the approved variables file (#1175) (cdds_qc): The quality checking can now be restricted to a single stream (#1248) (cdds_qc): CDDS QC now validates parent information when the branch_method is no parent (#1453) (cdds_transfer): The new format of the approved variables file is now handled (#1175). (cdds_transfer): Added new archiving tool cdds_store to replace send_to_mass and cdds_transfer_spice (#1384) (extract): CDDS Extract no longer connects to CREM to retrieve request information (#1079, #1080). (extract): --root_proc_dir and --root_data_dir arguments are now correctly interpreted by the cdds_extract_spice command (#1478). (extract): Error messages related to unreadable pp files are now being correctly logged (#1154). (transfer): Added new archiving tool cdds_store to replace send_to_mass and cdds_transfer_spice (#1384) Release 1.2.3, November 26, 2019 (hadsdk): thetaot and sltbasin can now be produced after correcting their stream (#1009) Release 1.2.2, November 13, 2019 (cdds_prepare): The create_cdds_directory_structure script now sets permissions on $CDDS_PROC_DIR/archive/log appropriately so users don\\'t have to (#1347) (cdds_qc): The data request version can now be overridden from the command line when running qc_run_and_report (#1375) (cdds_transfer): The failure to submit a RabbitMQ message, which triggers the publication process, is now accompanied by a critical log message (#1346) (extract): CDDS Extract will now log WARNING rather than CRITICAL messages when there are no variables to be extracted for a stream (#1354) (extract): All moose commands are now non-resumable, in order to avoid cases where MASS outages lead to conflicts with CDDS Extract processes (#1374) Release 1.2.1, October 29, 2019 (cdds_convert): The cdds_convert.log file is now written to the convert/log processing directory rather than the working directory (as seen at v1.2.0) (#1341) (cdds_convert): CDDS Convert now ignores streams where there are no to be processed rather than raising a KeyError (#1282) (cdds_qc): The validation of the CMIP6 license in now compares the license global attribute against the text provided in the request.json file (#1297) (extract): A bug in the validation process, introduced in v1.2.0, that resulted in all NEMO model output files being incorrectly identified as unreadable, and then deleted, has been fixed (#1333) Release 1.2.1, October 28, 2019 (hadsdk): PP header fixes are now correctly applied when producing information required to construct query files for moo select commands in CDDS Extract. This enables the correct extraction of data for clisccp (#1330) Release 1.2.0, October 17, 2019 (cdds_configure): The cdds_configure script is now called generate_user_config_files (#1043) (cdds_configure): The --mohc and --root_config command line options for the generate_user_config_files script have now been removed (all arguments provided by the CDDS configuration files are now provided via command line options, see below; #1043) (cdds_configure): The --data_request_version , --template_name , --root_proc_dir and root_data_dir command line options for the generate_user_config_files script have now been added (#1043, #1070) (cdds_convert): The --root_config command line option for the cdds_convert script has now been removed (all arguments provided by the CDDS configuration files are now provided via command line options, see below; #1070) (cdds_convert): The --root_proc_dir and root_data_dir command line options for the cdds_convert script have now been added (#1070) (cdds_prepare): The --root_config command line option for the create_cdds_directory_structure and prepare_generate_variable_list scripts has now been removed (all arguments provided by the CDDS configuration files are now provided via command line options, see below; #1164) (cdds_prepare): The --root_proc_dir and root_data_dir command line options for the create_cdds_directory_structure script have now been added (#1164) (cdds_prepare): The --data_request_version , --data_request_base_dir , --mapping_status , --root_proc_dir and root_data_dir command line options for the prepare_generate_variable_list script have now been added (#1164) (cdds_prepare): Error RuntimeError: requested_variables targeted by rule 0 already active or RuntimeError: requested_variables targeted by rule 0 already inactive no longer occurs. Instead the comment describing the change will be added to the log and comments in the and the state will remain unchanged (#1210) (cdds_prepare): Error ExperimentNotFoundError: Experiment name \"<experiment identifier>\" not found in this version of the data request no longer occurs when calling the prepare_generate_variable_list script when it is looking in the version of the used for model configuration. Instead if the is not defined in that version, a fallback and the associated with that will be used for comparison with the current version of the data request (#1256) (cdds_prepare): Additional , for which the description in the data request has changed between the versions used to configure the model and perform the processing, can now be produced (#1018) (cdds_prepare): Ocean that use with constraints can now be produced (#995) (cdds_qc): The --root_config command line option for the qc_run_and_report script has now been removed (all arguments provided by the CDDS configuration files are now provided via command line options, see below; #1167) (cdds_qc): The standard_names_dir , controlled_vocabulary_dir , --root_proc_dir and root_data_dir command line options for the qc_run_and_report script have now been added (#1167) (cdds_qc_plugin_cmip6): CDDS QC now confirms that the parent_experiment_id attribute in the is consistent with the Request and , allowing for the choice of parent_experiment_id where there are options within the (#1083) (cdds_transfer): The send_to_mass and cdds_transfer_spice scripts now raise critical log messages rather than error log messages, where appropriate (#1045) (extract): CDDS Extract now handles TSSC_SPANS_TOO_MANY_RESOURCES errors raised by moo commands that attempt to access files on too many tapes (#814) (hadsdk): The mip_era and --root_config command line options for the write_request_json script have now been removed (all arguments provided by the CDDS configuration files are now provided via command line options; #1078) (hadsdk): The --root_config command line option for the check_request_json_against_crem script has now been removed (all arguments provided by the CDDS configuration files are now provided via command line options; #1078) (hadsdk): The value of the parent_time_units attribute can now be interpreted by cf_units (#1220) (hadsdk): The write_request_json script now writes the parent_experiment_id to the request JSON file (#1109) Release 1.1.4, September 2, 2019 (cdds_convert): To avoid causing storage issues on SPICE the MIP Convert tasks in the CDDS Convert Rose suites now report the local disk usage (if suite variable \\$STAGING_DIR is set). If this limit is specified then tasks that exceed the limit will fail (#1158). (cdds_convert): To enable HadGEM3-GC31-MM (N216) processing the requests for local temporary storage (\\$TMPDIR), in the MIP Convert tasks within the Rose suites, have been extended (#1158). (extract): When validating model output files, Extract now identifies faulty files with no time records (#992). (hadsdk): Experiment name not found in this version of the data request errors in prepare_generate_variable_list , which occurred for some in HadGEM3, have been resolved (#1189) Release 1.1.3, July 31, 2019 (cdds_qc): Only the directory containing the are searched when looking for files to check (temporary files in other directories are ignored; (#1046) (cdds_qc): The rather than the name used in the filename (the out_name ) is now used when creating the approved list of (#1052) (cdds_transfer): To ensure messages sent to CEDA when withdrawing data contain the correct version date stamp, the version date stamp used in the path in MASS now doesn\\'t change during state changes (#919) (extract): To avoid issues related to extracting a large number of netCDF from MASS, the extraction is now performed in multiple chunks (#991) Release 1.1.2, July 3, 2019 (cdds_convert): Following updates to the MIP Concatenate sizing file to ensure that containing daily output on model levels are now correctly sized (#1002) (hadsdk): Ocean ancillary variables are now correctly recognised in several expressions (#944) Release 1.1.1, June 27, 2019 (cdds_prepare): RFMIP is now included in the list of responded to by default when constructing (#994) (cdds_qc_plugin_cmip6): Fixed a bug in CF Standard Names parser that prevented validation of Emon/ta (#993). (extract): Faulty are now removed when issues are identified (#918). Release 1.1.0, June 12, 2019 (cdds_convert): CDDS Convert now preferentially uses cftime , falling back to using netcdftime if cftime is not available in the environment (#249) (cdds_convert): A temporary local storage allocation is now requested in SPICE job scripts in order to avoid job failures due to lack of disk space (#824) and the settings controlling the temporal extent of the output files has been updated to include daily IPCC critical variables (#824, #883, #921) (cdds_convert): can now be processed from the apm (#922) (cdds_prepare): that do not exist in the version of the used to configure the are now correctly accounted for when determining whether the has changed significantly between the version of the used to setup the and the specified version of the (#249) (cdds_prepare): The list of that are included by default when constructing now includes PAMIP and CDRMIP (plus the correct spelling of AerChemMIP ) (#832) (cdds_prepare): Ocean biogeochemistry field definitions from the model suites are now used when identifying that can be produced (#820) (cdds_qc): The start and end dates in the file name for each are now validated against the (#909) (cdds_qc_plugin_cmip6): The CMIP6 Compliance Checker Plugin now preferentially uses cftime , falling back to using netcdftime if cftime is not available in the environment (#249) (extract): Daily ocean data can now be extracted from MASS (#882) (extract): Extract logs now contain the CRITICAL keyword for critical issues (#903) (extract): Exceeded step memory limit errors in cdds_extract_spice have been resolved (#915) (extract): The production of MEDUSA ocean biogeochemistry are now supported (#582) (hadsdk): Exceeded step memory limit errors in cdds_extract_spice have been resolved (#915) (hadsdk): The production of MEDUSA ocean biogeochemistry are now supported (#582) (hadsdk): HadSDK now preferentially uses cftime , falling back to using netcdftime if cftime is not available in the environment (#249) Release 1.0.5, May 10, 2019 (cdds_prepare): All in the can now be deactivated except those specified (which is useful for testing purposes) using the new prepare_select_variables script (#887) Release 1.0.4, May 2, 2019 (cdds_configure): MIP Convert template files for the ocean are now produced separately for each sub-stream to avoid an issue where ocean files _ may be produced with incorrect spatial coordinates (#871). This change also improves CDDS Convert end-to-end performance by splitting processing into several smaller batch jobs that can be run in parallel. (cdds_convert): Candidate concatenation file is now written to an intermediate directory, so that temporary files do not land up in the output directory and then in the publication pipeline (#849) (cdds_convert): Fixed a bug where after a timeout, a concatenation task would not produce all the expected output (#849) (cdds_transfer): Added command line option to move_in_mass script to allow state changes, such as embargoed to available or available to withdrawn , to be limited to a list of variables (#876) Release 1.0.4, April 30, 2019 (cdds_qc_plugin_cmip6): The validation of the activity_id global attribute in , as written by , now allows for multiple to be specified in the CMIP6 for a particular . For example, data produced for ssp370 should have an activity_id of ScenarioMIP AerChemMIP (#865) (extract): Some unit test utility functions were moved from extract to hadsdk (#865). (hadsdk): Some unit test utility functions were moved from extract to hadsdk (#865) Release 1.0.3, April 18, 2019 (cdds_convert): Mip_convert and mip_concatenate now write to three separate directories: one for mip_convert output, a second for input files to the concatenation tasks and a third for the final concatenation tasks which is where qc and transfer will look for the output (#840) (cdds_convert): Updated the calculation of the concatenation task cycle time for suites that only have 1 scheduled concatenation task due to the short run time (#852) (cdds_transfer): The MOOSE command-id is now reported via run_moo_cmd to allow CEDA\\'s CREPP tools to kill processes left running on the MOOSE controllers that should have ended when the command line MOOSE client exited (#855) Release 1.0.2, April 5, 2019 (hadsdk): The hadsdk.common.netCDF_regexp() function now returns a regular expression that contains 4 match groups, allowing the extraction of NEMO (#834) Release 1.0.1, April 2, 2019 (cdds_configure): The grid label gn is now used for datasets that use on all T/U/V/UV grids (#761) (cdds_convert): The and grid identifiers are now obtained directly from the (#555) (cdds_convert): One Rose suite per is now submitted rather than one Rose suite per request (#710) (cdds_convert): The argument provided to the --rose-suite-branch parameter can now be a path to a checked out version of the Rose suite (for development purposes only; #739) (cdds_convert): The Rose suite now removes directories where outputs are written prior to producing those outputs (#750) (cdds_convert): The concatenation periods are now aligned with the reference date (#762) (cdds_convert): A single is now produced if the concatenation period is more than the run bounds (#778) (cdds_convert): The correct files are now included in each concatenation period (#780) (cdds_convert): The cycles to run MIP Convert are now aligned with the reference date (#781) (cdds_prepare): The significant changes between the in the version of the used to setup HadGEM3 and version 01.00.29 of the were approved and added to KNOWN_GOOD_VARIABLES (#747) (cdds_qc): The request JSON file is now a positional argument to qc_run_and_report (#697) (cdds_qc): Single timestep are now handled appropriately (#769) (cdds_qc): Bound checks related to single depth levels are now ignored by default (the parameter --do_not_filter can be used to report these issues; #792) (cdds_qc): A detailed report can now be generated using the --details parameter (#793) (cdds_qc): An approved list of can now be generated, which can be used to deactivate in the for the next round of processing (#819) (cdds_transfer): The new command line script cdds_transfer_spice can now be used to run CDDS Transfer on SPICE (#688) (cdds_transfer): The name of the directory used to store on MASS can now be specified via the --mass_location parameter (#756) (cdds_transfer): The SPICE logs for cdds_transfer_spice are now written to the correct location when using the --use_proc_dir parameter (#828) (extract): A workaround was implemented to deal with the fact that the moo ls command sometimes returns a truncated list of filenames (#771) (hadsdk): The write_request_json script now works correctly for experiments without parents, and retrieves the correct value of the from CREM (#730) Release 1.0.0, February 1, 2019 (cdds_configure): First implementation of CDDS. (cdds_convert): First implementation of CDDS. (cdds_prepare): First implementation of CDDS. (cdds_qc): First implementation of CDDS. (cdds_qc_plugin_cf17): First implementation of CDDS. (cdds_qc_plugin_cmip6): First implementation of CDDS. (cdds_transfer): First implementation of CDDS. (extract): First implementation of CDDS. (hadsdk): First implementation of CDDS. (transfer): First implementation of CDDS.","title":"Changelog (CDDS)"},{"location":"changes_cdds/#release-310rc1-february-11-2025","text":"MIP Convert and CMOR log files are copied into the cylc task directory and are accessible via cylc review (CDDSO-495) Cylc workflows have been integrated into the CDDS code base rather than being checked out from the roses-u repository (CDDSO-494) Processing for multiple streams is now handled within a single Cylc workflow (CDDSO-302) A validation tool has been added for the model parameters file SCRIPT_NAME_HERE (CDDSO-511) CDDS Prepare will remove MIP Convert templates from the configure proc directory to avoid inconsistencies in processing. A --reconfigure command line argument has been added to enable the regeneration of the mip convert config templates from the command line (CDDSO-593) Ancillary STASH variables can now be configured via the CDDS Plugins (CDDSO-582) Added support for HadGEM3-GC50-N640ORCA12 model, which includes allowing for alternative coordinate bounds variables in NEMO output files (CDDSO-543) Updated CMOR to v3.9.0 (CDDSO-589) Migrated content from Sphinx docs to mkdocs (CDDSO-598, CDDSO-600)","title":"Release 3.1.0rc1, February 11, 2025"},{"location":"changes_cdds/#release-306-january-21-2025","text":"Pass model parameters file to CDDS suite during convert process (CDDSO-578) Handling quarterly data during the extract process correctly (CDDSO-568) Add documentation to deployment page (CDDSO-556) Allow QC file size limit to be overloaded in the request file (CDDSO-577, CDDSO-587) Fix checkout processing workflow on JASMIN (CDDSO-574) Add documentation for model parameters file (CDDSO-545)","title":"Release 3.0.6, January 21, 2025"},{"location":"changes_cdds/#release-305rc1-december-5-2024-only-to-be-used-for-lesfmip","text":"Ensured that quarterly PP files, i.e. those that have 3 months of output at monthly/daily frequency can be extracted and picked up by the wrapper to MIP convert (CDDSO-560, CDDSO-568)","title":"Release 3.0.5rc1, December 5, 2024 (only to be used for LESFMIP)"},{"location":"changes_cdds/#release-304-november-19-2024","text":"Enabled sub-annual slicing through the slicing option in the common section of the Request config file. Streams can individually be set to have month slicing where large amounts of memory usage are expected (e.g. hourly data) while the default is year , i.e. annual slicing (CDDSO-342) Corrected bug preventing QC from completing for CMIP6 like processing introduced in CDDS v3.0.2 (CDDSO-532) Changed concatenation code to handle monthly and daily datasets in the same stream where the first two facets of the file name are not unique to a dataset (CDDSO-548) Updated HadREM3 sizing info (CDDSO-548)","title":"Release 3.0.4, November 19, 2024"},{"location":"changes_cdds/#release-303-october-24-2024","text":"Grid descriptions can now be modified through model parameter files (CDDSO-534, CDDSO-528) The inclusion of rotated coordinates in output files can now be forces through the introduction of the force_coordinate_rotation flag into the Request config files (CDDSO-529) Global attribute caching has been introduced to improve QC speed (CDDSO-514)","title":"Release 3.0.3, October 24, 2024"},{"location":"changes_cdds/#release-302-september-27-2024","text":"Restored the approved variables file argument to the move_in_mass script (CDDSO-523) Ensure that hourly data and submonthly data is correctly recognised by CDDS when running MIP Convert (CDDSO-512) Prevent Extract tasks from failing when assessing moose command block size if some data is already present (CDDSO-515) Correctly handle leap years (Gregorian calendar) during concatenation task (CDDSO-513) Correctly handle sub annual chunk size for files in concatenation setup task (CDDSO-504) Correctly overload model parameters file when reading request file (CDDSO-508) Add functionality to remove halo columns and rows from atmosphere data (CDDSO-521) Correct parsing of filename facet for CORDEX in QC (CDDSO-519) Ensure clear checking of time coordinate in QC with the corresponding frequency and length in QC (CDDSO-517)","title":"Release 3.0.2, September 27, 2024"},{"location":"changes_cdds/#release-301-august-21-2024","text":"Verbose mode is not added as default option when running CDDS Convert workflow to avoid Cylc issue (CDDSO-490) Added new options to delete pre-existing proc and data directories in request.cfg (CDDS-408) Changes to port CDDS to Azure (CDDS-481, CDDSO-497) QC and archive will now fail if nothing is to check or to archive (CDDSO-459) Improved handling of failed moo commands (CDDSO-487) Extract will now fail when it cannot identify the chunking for moo commands (CDDSO-487) Prepare to accept variables like 6hrPlevPt/zg7h (CDDSO-473) The sim review script now accepts the request config file as its input argument (CDDSO-461) Added a new script cdds_clean to clean out the CDDS Convert workflows (CDDSO-450) Added support for CORDEX (CDDSO-421) Replacde hardcoded paths using the CDDS_ETC variable in tests (CDDSO-492) Extract will now fail if the tape check fails (CDDSO-482) Added validations for many fields within the request config file (CDDSO-292) Allow inheritance of request fields from a template (CDDSO-496)","title":"Release 3.0.1, August 21, 2024"},{"location":"changes_cdds/#release-300-may-15-2024","text":"The request JSON file, used to control CDDS in previous versions, has been replaced with a config file containing all required options and the command line interfaces to CDDS tools have been correspondingly updated (CDDSO-434, CDDSO-466, CDDSO-424, CDDSO-301, CDDSO-411, CDDSO-299, CDDSO-298, CDDSO-296, CDDSO-297, CDDSO-438, CDDSO-423, CDDSO-386, CDDSO-300, CDDSO-395) CDDS Documentation is now maintained through mkdocs and served via github pages (CDDSO-427, CDDSO-430, CDDSO-409, CDDSO-429, CDDSO-426, CDDSO-425, CDDSO-428, CDDSO-443, CDDSO-446, CDDSO-442) MIP Convert failure will now lead to cylc task failure if the continue_if_mip_convert_failed setting in the request ( [conversion] section) is set to False in the request config file (CDDSO-387) The branch of the CDDS Convert workflow being used by default is controlled by the CDDS_CONVERT_WORKFLOW_BRANCH environment variable (CDDSO-452) Log messages are now sent to standard out by default, and will appear in job.out files within the cylc log directory (CDDSO-34) CMOR has been updated to v3.8 and Python to 3.10 (CDDSO-411) Simplified model config files by removing files_per_year field (CDSSO-307)","title":"Release 3.0.0, May 15, 2024"},{"location":"changes_cdds/#release-2510-december-12-2024","text":"No changes","title":"Release 2.5.10, December 12, 2024"},{"location":"changes_cdds/#release-259-june-25-2024","text":"Fixed a bug in submission code that made archiving script unusable (CDDSO-477) QC now allows missing standard name attribute if it is defined as empty in MIP Tables (CDDSO-478)","title":"Release 2.5.9, June 25, 2024"},{"location":"changes_cdds/#release-258-february-29-2024","text":"Added submission queues to support CMIP6Plus publication (CDDSO-413) Removed modified time check from CDDS workflow to avoid workflow freezing when moved to a different cylc server (CDDSO-405)","title":"Release 2.5.8, February 29, 2024"},{"location":"changes_cdds/#release-257-february-21-2024","text":"Chunking of large moose queries in Extract is now correctly handled (CDDSO-398) QC now supports Conventions field in a more portable way (CDDSO-407) Concatenation code can now cope with monthly files of monthly mean data (CDDSO-405) Extensions and fixes to support CP4A processing via GCModel Dev (CDDSO-354, CDDSO-401, CDDSO-405)","title":"Release 2.5.7, February 21, 2024"},{"location":"changes_cdds/#release-256-february-09-2024","text":"Added HadGEM3-GC5 and eUKESM1-1-ice model configurations to GCModelDev plugin (CDDSO-390, CDDSO-391) Extraction code now correctly handles coordinate variables when extracting data from NEMO diaptr and scalar files (CDDSO-394) The data version used in archiving can now be specified in the CDDS workflow to support additional runs of CDDS appending to the same data set (CDDSO-393)","title":"Release 2.5.6, February 09, 2024"},{"location":"changes_cdds/#release-255-january-18-2024","text":"The name of netCDF bounds coordinates variables are now configurable via plugins to support retrieval of NEMO4 ocean data (CDDSO-346) GCModelDev plugin now has an N96 GC3.05 generic model configuration and the list streams for the N216 equivalent has been extended (CDDSO-381) The option to overload model configuration via command line now correctly operates (CDDSO-376, CDDSO-379, CDDSO-380) The create_variables_csv_file script now also outputs the stream for each variable (CDDSO-332)","title":"Release 2.5.5, January 18, 2024"},{"location":"changes_cdds/#release-254-december-13-2023","text":"Corrected an issue with a legacy configuration file that prevented the submission of CMIP6 data (CDDSO-371) Implement annual ocean diagnostics for the GCOyr MIP table (CDDSO-369)","title":"Release 2.5.4, December 13, 2023"},{"location":"changes_cdds/#release-253-november-22-2023","text":"Adaptations needed for new CMIP6Plus MIP tables (CDDSO-336, CDDSO-365) Adaptations needed to support use of sub_experiment_ids that are not \"none\" (CDDSO-365, CDDSO-362) Extract can again correctly retrieve sub-daily data (CDDSO-358) The update_directives script in the per-stream suite now correctly calculate upper limits for memory / wall time / local storage (CDDSO-360)","title":"Release 2.5.3, November 22, 2023"},{"location":"changes_cdds/#release-252-october-18-2023","text":"Polar row masking is now specified directly in the MIP Convert config files (CDDSO-331) Memory, storage and time limits for SLURM jobs are now dynamically calculated after the first three cycles of the processing suite (CDDSO-212) CDDS can now handle hourly data files within a stream (CDDSO-349) The correct filenames for ocean data from ensemble suites are now used again (CDDSO-350) The CDDS suite uses a consistent calendar to the request, which ensures that concatenation processes, QC and archiving are correctly triggered (CDDSO-351) All options are now passed through to CDDS Convert by the CDDS processing workflow (CDDSO-348)","title":"Release 2.5.2, October 18, 2023"},{"location":"changes_cdds/#release-251-august-4-2023","text":"Ensure that recent versions of the NCO tools used for concatenation of files do not modify metadata, leading to publication issues (CDDSO-327) Fix creation_date validation bug in QC when data is written on the 31st of a month (CDDSO-326)","title":"Release 2.5.1, August 4, 2023"},{"location":"changes_cdds/#release-250-july-27-2023","text":"Migrate CDDS to Cylc 8 (CDDSO-282, CDDSO-285, CDDSO-184) Introduce gregorian calendar support (CDDSO-231, CDDSO-235, CDDSO-240, CDDSO-244) Plugin and code development to support Regional models for projects such as CORDEX (CDDSO-233, CDDSO-68, CDDSO-74, CDDSO-261, CDDSO-149) CDDS now uses ISO datetime format for all time values in CDDS and MIP Convert config files (CDDSO-313, CDDSO-45) Relocate test and etc directories to support alternate platforms and migration to azure SPICE (CDDSO-131, CDDSO-273) Replace pep8 with pycodestyle (CDDSO-207) Separated extract validation into a separate script & task (CDDSO-243) Introduced retries to workflow tasks when submission fails (CDDSO-314) Amalgamated CDDS processing and ensemble processing workflows. Previous tools based on u-cq805 and u-cr273 are deprecated (CDDSO-319) Updated CMOR to version 3.7.2, which required iris update to 3.4.1 (CDDSO-321)","title":"Release 2.5.0, July 27, 2023"},{"location":"changes_cdds/#release-247-june-29-2023","text":"Fix a bug in the dehaloing script that prevented it from reading default command line arguments (CDDSO-308)","title":"Release 2.4.7, June 29, 2023"},{"location":"changes_cdds/#release-246-june-23-2023","text":"Fix a bug in the extraction code where the non-uniform allocation of data in a simulation to a large number of tapes can lead to failure of cdds extract (CDDSO-245, CDDSO-289) Correct a bug in the HadGEM3 model config files that prevented extraction of data from the diaptr sub-stream in the NEMO ocean data for the GCModelDev project (CDSO-293)","title":"Release 2.4.6, June 23, 2023"},{"location":"changes_cdds/#release-245-may-22-2023","text":"Added HighResMIP models to CMIP6 plugin (CDDSO-158, cherry-picked from main)","title":"Release 2.4.5, May 22, 2023"},{"location":"changes_cdds/#release-244-may-4-2023","text":"Added options to use a \"relaxed\" approach to MIP name (activity_id), and experiment id, that does not force checks from the CVs. This will allow users to process data as any experiment id they like, but at the risk of typos due to the lack of checks against a controlled vocabulary (CDDSO-253 , CDDSO-267, CDDSO-264) prepare_alter_variable_list can now perform insert commands again as plugin was not being loaded, which was needed for setting streams (CDDSO-269) checkout_processing_suite now sets the streams in the suite based on those in the request file. (CDDSO-271)","title":"Release 2.4.4, May 4, 2023"},{"location":"changes_cdds/#release-243-march-31-2023","text":"Add support for retries in the run_extract task within the CDDS Convert Suite (CDDSO-258)","title":"Release 2.4.3, March 31, 2023"},{"location":"changes_cdds/#release-242-march-1-2023","text":"Add support for filepaths with ensemble_id for netCDF model output (CDDSO-238).","title":"Release 2.4.2, March 1, 2023"},{"location":"changes_cdds/#release-241-january-18-2023","text":"Create a Cylc workflow for end to end processing (CDDSO-198, 200) Create a Cylc workflow for ensemble processing (CDDSO-199) Implement the checkout_processing_suite script for checking a copy of the CDDS Processing Suite (CDDSO-210) Create a Rose app for generating request.json files (CDDSO-3) Fix a bug where cdds_convert expect external plugins environmental variables to be set even if no plugin was used (CDDSO-216)","title":"Release 2.4.1, January 18, 2023"},{"location":"changes_cdds/#release-240-september-12-2022","text":"Refactoring of all CDDS packages into one package excluding MIP Convert (CDDSO-132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144) Move test execution from nose to pytest (CDDSO-128) Add sphinx docs (CDDSO-169) diaptr files for HadGEM3-GC31-MM can now be extracted from MASS (CDDSO-191)","title":"Release 2.4.0, September 12, 2022"},{"location":"changes_cdds/#release-232-september-01-2022","text":"(cdds_common): Default stream information for variables now held by Project level plugins (CDDSO-143) (cdds_configure): Log message issued when a stream cannot be found for a variable is now CRITICAL (CDDSO-178) (cdds_configure): GCAmon and GCLmon MIP tables from GCModelDev are now included in dictionary of default grids and will not be ignored (CDDSO-178) (cdds_convert): Logger now set up before configure step is called allowing log information to be output (CDDSO-177) (cdds_convert): MIP Era now correctly propagated through to CDDS rose suite (CDDSO-180) (hadsdk): Change log message to CRITICAl for missing grid/stream. (hadsdk): Prepare a MIP convert branch to produce sample decadal data with additional seasonal forecasting metadata","title":"Release 2.3.2, September 01, 2022"},{"location":"changes_cdds/#release-231-june-29-2022","text":"(cdds_common): Added auto-genereated table of CDDS variable mappings (CDDSO-120) (cdds_common): Fix inconsistency in case between mip era and plugin name (CDDSO-132) (extract): remove_ocean_haloes script now works after change to load plugin (CDDSO-152)","title":"Release 2.3.1, June 29, 2022"},{"location":"changes_cdds/#release-230-may-24-2022","text":"(cdds_common): Development moved to github (cdds_common): Version string now includes git commit hash (CDDSO-93) (cdds_configure): Development moved to github (cdds_convert): Development moved to github (cdds_prepare): Development moved to github (cdds_qc): Development moved to github (cdds_qc_plugin_cf17): Development moved to github (cdds_qc_plugin_cmip6): Development moved to github (cdds_transfer): Development moved to github (extract): Development moved to github (hadsdk): Development moved to github (transfer): Development moved to github","title":"Release 2.3.0, May 24, 2022"},{"location":"changes_cdds/#release-225-may-4-2022","text":"(cdds_common): The atmosphere timestep, used in some mappings, is now supplied via the ModelParameters class rather than a dictionary. This is only used for request JSON file creation (#2571) (cdds_configure): For non-CMIP6 projects the correct grid label and grid information is applied for variables where the grid is different to the default for that mip table (#2570) (cdds_prepare): When inserting variables into CMIP6 processing the correct stream information is included (#2569) (hadsdk): For non-CMIP6 projects the correct grid label and grid information is applied for variables where the grid is different to the default for that mip table (#2570) (hadsdk): The atmosphere timestep, used in some mappings, is now supplied via the ModelParameters class rather than a dictionary. This is only used for request JSON file creation (#2571)","title":"Release 2.2.5, May 4, 2022"},{"location":"changes_cdds/#release-224-april-22-2022","text":"(cdds_common): Added UKESM1-1-LL model to CMIP6 and GCModelDev projects (#2545) (transfer): Removed further calls to the moo test command when archiving data (#2559, #2560)","title":"Release 2.2.4, April 22, 2022"},{"location":"changes_cdds/#release-223-april-7-2022","text":"(cdds_convert): The default cycling frequency will reduce to match the run bounds length when the run bounds length is smaller than cycling frequency. (#2006) (cdds_prepare): Reintroduce the capability to fall-back on default CMIP6 stream mappings when parsing user variable file (#2555) (cdds_qc): Ignore QC errors caused by non-existant dimensions (#2548) (cdds_qc_plugin_cf17): The implementation of the geographical region checker has been corrected (#2548) (extract): MASS paths with and without the ensemble_id in them are now handled (#2522) (transfer): moose listing commands are used rather than `moo test` in order to limit the number of commands talking to MASS, thereby limiting our vulnerability to transient or load dependent errors, and increases performance (#2553)","title":"Release 2.2.3, April 7, 2022"},{"location":"changes_cdds/#release-222-march-18-2022","text":"(cdds_common): Addition of GCModelDev plugin with core CMIP6 models (#2519) (cdds_prepare): Streams can now be set on creation of the requested variables file (#2498) (cdds_qc): QC checks now refer to provided controlled vocabulary for non-CMIP6 projects (#2542) (cdds_qc_plugin_cmip6): QC checks now refer to provided controlled vocabulary for non-CMIP6 projects (#2542)","title":"Release 2.2.2, March 18, 2022"},{"location":"changes_cdds/#release-221-february-15-2022","text":"(cdds_common): UKESM1-ice-LL model information now loaded correctly (#2530) (cdds_convert): --external_plugins argument now passed through to all tasks (#2511) (cdds_qc): CF checks are now correctly filtered for CMIP6 data (#2531) (cdds_qc_plugin_cf17): CF checks are now correctly filtered for CMIP6 data (#2531)","title":"Release 2.2.1, February 15, 2022"},{"location":"changes_cdds/#release-220-february-9-2022","text":"(cdds_common): New module to take common code across CDDS and will ultimately replace hadsdk (#2460) (cdds_common): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_common): Update to use python 3.8 (#2438) (cdds_configure): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_configure): Update to use python 3.8 (#2438) (cdds_configure): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_convert): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_convert): Update to use python 3.8 (#2438) (cdds_convert): Enable the use of CDDS for ensemble class simulations (#2501, #2471) (cdds_convert): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_convert): When file concatenation is not required, i.e. when a single cycle covers the whole duration of the processing, files are now moved to the correct destination rather than being left in the _concat directory (#2521) (cdds_prepare): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_prepare): Update to use python 3.8 (#2438) (cdds_prepare): Enable the use of CDDS for ensemble class simulations (#2501, #2471) (cdds_prepare): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_qc): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_qc): Update to use python 3.8 (#2438) (cdds_qc): CDDS QC now uses the community version of IOOS/compliance-checker rather than a fork (#2381) (cdds_qc): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_qc): Fixed a bug where if the string _concat was used in the branch name two tests in cdds_qc would fail (#2521) (cdds_qc_plugin_cf17): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_qc_plugin_cf17): Update to use python 3.8 (#2438) (cdds_qc_plugin_cf17): CDDS QC now uses the community version of IOOS/compliance-checker rather than a fork (#2381) (cdds_qc_plugin_cf17): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_qc_plugin_cmip6): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_qc_plugin_cmip6): Update to use python 3.8 (#2438) (cdds_qc_plugin_cmip6): CDDS QC now uses the community version of IOOS/compliance-checker rather than a fork (#2381) (cdds_qc_plugin_cmip6): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (cdds_transfer): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (cdds_transfer): Update to use python 3.8 (#2438) (extract): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (extract): Update to use python 3.8 (#2438) (extract): Enable the use of CDDS for ensemble class simulations (#2501, #2471) (extract): Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) (hadsdk): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (hadsdk): Update to use python 3.8 (#2438) (hadsdk): Retired umfilelist and all connected code (#2438) (transfer): Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) (transfer): Update to use python 3.8 (#2438)","title":"Release 2.2.0, February 9, 2022"},{"location":"changes_cdds/#release-212-november-25-2021","text":"(extract): Extract will now repeat creating stream directories if the first time fails because of intermittent filesystem issues (#2429) (hadsdk): Fixed an import bug in the mip_table_editor script (#2463)","title":"Release 2.1.2, November 25, 2021"},{"location":"changes_cdds/#release-211-october-26-2021","text":"(cdds_prepare): prepare_generate_variable_list can now generate requested variables files using a user supplied list and the MIP tables being used, i.e. without reference to a Data Request (#2358) (extract): Added the path_reformatter command line tool from the cdds_utils directory (#2415) (hadsdk): Improved error logging for the write_rose_suite_request_json script (#2390) (transfer): Added scripts for listing submission queues and resending stored messages that failed during submission (#2427, #2428)","title":"Release 2.1.1, October 26, 2021"},{"location":"changes_cdds/#release-210-september-6-2021","text":"(cdds_configure): generate_user_config_files is now run by cdds_convert by default (#1902, #2425) (cdds_convert): The Extract, Configure, QC and archiving steps are now by default part of the conversion suite run by cdds_convert (#1902, #2383, #2425, #2432) (cdds_prepare): The --alternate_experiment_id argument no longer triggers CRITICAL issues` (#2380) (cdds_qc): qc_run_and_report is now run as part of the conversion suite by cdds_convert by default (#1902, #2425, #2432) (extract): cdds_extract is now run as part of the conversion suite by cdds_convert by default (#1902, #2425) (transfer): cdds_store is now run as part of the conversion suite by cdds_convert by default (#1902, #2425, #2432) (transfer): cdds_sim_review now handles per stream logs, qc reports and approved variables files (#1685)","title":"Release 2.1.0, September 6, 2021"},{"location":"changes_cdds/#release-207-july-15-2021","text":"(extract): The remove_ocean_haloes command line tool is now available to remove halo rows and columns from ocean data files (#1161)","title":"Release 2.0.7, July 15, 2021"},{"location":"changes_cdds/#release-206-june-29-2021","text":"(cdds_prepare): Added functionality to prepare_generate_variable_list to automatically deactivate variables following a set of rules hosted in the CDDS repository (default) or in a text file. This \\\"auto deactivation\\\" can be skipped using the --no_auto_deactivation argument and the --auto_deactivation_file_name option can be used to use a local file rather than the repository rules (#2405)","title":"Release 2.0.6, June 29, 2021"},{"location":"changes_cdds/#release-205-june-11-2021","text":"(cdds_qc): Floating point comparison will now not cause time contiguity checks to fail (#2396)","title":"Release 2.0.5, June 11, 2021"},{"location":"changes_cdds/#release-204-may-17-2021","text":"(cdds_prepare): Allow for correct handling of packages where there are multiple mips / activity ids. The primary, i.e. first, mip will be used for all directory and dataset ids (#2369) (cdds_prepare): Altered output of create_cdds_directory_structure to be a little more user friendly (#2369) (cdds_transfer): Allow for correct handling of packages where there are multiple mips / activity ids. The primary, i.e. first, mip will be used for all directory and dataset ids (#2369) (hadsdk): Allow for correct handling of packages where there are multiple mips / activity ids. The primary, i.e. first, mip will be used for all directory and dataset ids (#2369) (transfer): Allow for correct handling of packages where there are multiple mips / activity ids. The primary, i.e. first, mip will be used for all directory and dataset ids (#2369)","title":"Release 2.0.4, May 17, 2021"},{"location":"changes_cdds/#release-203-april-28-2021","text":"(transfer): CDDS store can now prepend files to an embargoed dataset provided the correct data version argument is provided (#2278)","title":"Release 2.0.3, April 28, 2021"},{"location":"changes_cdds/#release-202-april-22-2021","text":"(cdds_convert): Implemented changes necessary for conda deployment of CDDS code on JASMIN (#2240) (cdds_qc): QC can now use a user specified directory with MIP tables provided by a command line option (#2331) (hadsdk): Inventory can now be constructed or updated from CREPP database dump on JASMIN (#2148) (hadsdk): Corrected masking of polar rows for the ORCA12 grid (#2211) (hadsdk): Implemented changes necessary for conda deployment of CDDS code on JASMIN (#2240)","title":"Release 2.0.2, April 22, 2021"},{"location":"changes_cdds/#release-201-march-25-2021","text":"(cdds_qc): QC now correctly interprets the values within string coordinates, such as basin name (#2284) (extract): When running for a particular stream the stream name is now included in the log file name (#2014) (hadsdk): write_rose_suite_request_json can now create request files for the UKESM1-ice-LL model (#2264) (hadsdk): write_rose_suite_request_json now allows the apt stream (#2283) (transfer): CDDS store can now recover gracefully when continuing archiving following task failure due to issues with MASS (#2205) (transfer): CDDS store can now correctly handle pre-pending files to datasets and there are clearer error messages when issues arise (#2222)","title":"Release 2.0.1, March 25, 2021"},{"location":"changes_cdds/#release-200-february-24-2021","text":"(cdds_configure): Updated CDDS codebase to Python 3.6. (cdds_convert): Updated CDDS codebase to Python 3.6. (cdds_prepare): Updated CDDS codebase to Python 3.6. (cdds_qc): Updated CDDS codebase to Python 3.6. (cdds_qc_plugin_cf17): Updated CDDS codebase to Python 3.6. (cdds_qc_plugin_cmip6): Updated CDDS codebase to Python 3.6. (cdds_transfer): Updated CDDS codebase to Python 3.6. (extract): Updated CDDS codebase to Python 3.6. (hadsdk): Updated CDDS codebase to Python 3.6. (transfer): Updated CDDS codebase to Python 3.6.","title":"Release 2.0.0, February 24, 2021"},{"location":"changes_cdds/#release-165-february-22-2021","text":"(cdds_qc): Fixed a bug in diurnal cycle diagnostics checker (#2254).","title":"Release 1.6.5, February 22, 2021"},{"location":"changes_cdds/#release-164-february-11-2021","text":"(hadsdk): write_rose_suite_request_json now correctly handles fields that are commented in the rose-suite.info file, an issue that affects the use of this script for amip-like simulations in v1.6.3 (#2232)","title":"Release 1.6.4, February 11, 2021"},{"location":"changes_cdds/#release-163-february-9-2021","text":"(cdds_qc): Implemented support for diurnal cycle diagnostics (1hrCM frequency) (#1894) (hadsdk): Errors in rose suite metadata (rose-suite.info file) now cause write_rose_suite_request_json to fail rather than warn and branch dates are correctly handled (#1477) (hadsdk): Users can now override the start and end dates for processing when creating request files with write_rose_suite_request_json (#2212) (transfer): The failure of [moo put commands is now handled more clearly (#2212)","title":"Release 1.6.3, February 9, 2021"},{"location":"changes_cdds/#release-162-january-11-2021","text":"(cdds_qc_plugin_cmip6): The cell_measures validation is now properly processing variables with --MODEL flag (#2193)","title":"Release 1.6.2, January 11, 2021"},{"location":"changes_cdds/#release-161-november-26-2020","text":"(cdds_qc_plugin_cmip6): The cell_measures validation is now properly processing global averages (#2171) (extract): Fixed a bug in the validation report, where the expected number of files would be printed instead of the actual file count (#2174)","title":"Release 1.6.1, November 26, 2020"},{"location":"changes_cdds/#release-160-november-05-2020","text":"(cdds_configure): All exceptions caught at the top script level are now logged as critical errors (#1968) (cdds_convert): All exceptions caught at the top script level are now logged as critical errors (#1968) (cdds_convert): Implemented code changes needed to support ARCHER filepath format and run CDDS on JASMIN (#2013) (cdds_convert): cdds_convert tasks now will fail if individual concatenation tasks do not succeed (#2045) (cdds_prepare): As a default option, prepare_generate_variable_list now performs CMIP6 inventory check to determine which variables have already been produced, deactivating them automatically (#1899) (cdds_prepare): Added requested ensemble size to the requested_variable_file file (#1900) (cdds_prepare): The write_rose_suite_request_json script now correctly writes the name of the rose suite branch in the [request json file (#2001) (cdds_prepare): All exceptions caught at the top script level are now logged as critical errors (#1968) (cdds_qc): All exceptions caught at the top script level are now logged as critical errors (#1968) (cdds_qc_plugin_cmip6): Consistency checks for netCDF variable attributes are now correctly applied (#2001) (cdds_qc_plugin_cmip6): Replaced the CMIP6 Controlled Vocabulary tables with the ones bundled with CMOR (#1808) (cdds_transfer): All exceptions caught at the top script level are now logged as critical errors (#1968) (extract): All exceptions caught at the top script level are now logged as critical errors (#1968) (hadsdk): Implemented a new command line tool search_inventory for exploring archived datasets and checking their publication status (#1903) (hadsdk): Implemented code changes needed to support ARCHER filepath format and run CDDS on JASMIN (#2013)","title":"Release 1.6.0, November 05, 2020"},{"location":"changes_cdds/#release-155-october-20-2020","text":"(cdds_prepare): The requested variable list can now be constructed using an alternative experiment_id via a new command line argument to prepare_generate_variable_list . This should only be used under advice from the CDDS team (#2057) (cdds_transfer): move_in_mass now correctly includes variables, where the and name used in the output file is different, when submitting data for publication (#2049).","title":"Release 1.5.5, October 20, 2020"},{"location":"changes_cdds/#release-154-october-7-2020","text":"(cdds_convert): Updated file sizing configuration for the high-resolution MM model now yields files under 20 Gb (#2037)","title":"Release 1.5.4, October 7, 2020"},{"location":"changes_cdds/#release-153-september-16-2020","text":"(cdds_convert): Increased memory limits, TMPDIR storage and changed cycling frequencies (#2012)","title":"Release 1.5.3, September 16, 2020"},{"location":"changes_cdds/#release-152-september-4-2020","text":"(cdds_qc_plugin_cmip6): Fixed a bug that prevented from validation of some global attributes of simulations without a parent experiment (#2000)","title":"Release 1.5.2, September 4, 2020"},{"location":"changes_cdds/#release-151-august-20-2020","text":"(cdds_convert): The concatenation processes should no longer leave temporary netCDF files behind when the batch job hits its wall time (#1895) (cdds_convert): The check on the use of TMPDIR storage in the MIP Convert tasks is now correctly applied (#1943) (cdds_convert): The cycling frequencies for the conversion process can now been overridden from the command line (#1944) (extract): Information on the validation of the is now written to \\<stream>_validation.txt as well as to the log files (#1811) (transfer): cdds_store can now archive CF sites datawith frequency subhrPt (#1945) (transfer): cdds_store will now raise CRITICAL log messages, rather than ERROR when something goes wrong (#1934)","title":"Release 1.5.1, August 20, 2020"},{"location":"changes_cdds/#release-150-july-2-2020","text":"(cdds_configure): Settings and files needed for production of CF sites data are now correctly written to the MIP Convert template files (#1838) (cdds_convert): CDDS can now produce CF Sites data (#1838) (cdds_prepare): Added do-not-produce information to the requested variables file via a new producible attribute for each variable (#681) (cdds_prepare): Added tool to export variable information from the requested variables file to a CSV or text file; create_variables_table_file (#1793) (cdds_qc): CDDS QC can now check CF sites (subhr frequency) data (#1783) (extract): CDDS Extract now correctly reports failure through error codes and e-mails (#1807) (extract): CDDS Extract can now correctly retrieve daily MEDUSA data (#1803)","title":"Release 1.5.0, July 2, 2020"},{"location":"changes_cdds/#release-145-june-16-2020","text":"(hadsdk): Institution names and branching dates are now correctly processed and written by the write_rose_suite_request_json script (#1848, #1849)","title":"Release 1.4.5, June 16, 2020"},{"location":"changes_cdds/#release-144-june-1-2020","text":"(cdds_convert): Memory limits for ap4 stream have been extended to 12G to avoid task failure (#1843) (cdds_convert): The copying of files to $TMPDIR will now be retried twice and if all attempts fail the task will fail rather than allowing CRITICAL errors from MIP Convert (#1839)","title":"Release 1.4.4, June 1, 2020"},{"location":"changes_cdds/#release-143-may-12-2020","text":"(cdds_convert): CDDS can now process data for the model UKESM1-ice-LL (#1513) (cdds_convert): Added --scale-memory-limits argument to CDDS Convert to allow memory limits to be altered at run time if necessary (#1806) (cdds_prepare): CDDS can now process data for the model UKESM1-ice-LL (#1513) (cdds_prepare): Requested variable lists can now be generated using data request version 01.00.32 (#1800) (cdds_qc): CDDS can now process data for the model UKESM1-ice-LL (#1513) (cdds_qc_plugin_cf17): CDDS can now process data for the model UKESM1-ice-LL (#1513) (cdds_qc_plugin_cmip6): CDDS can now process data for the model UKESM1-ice-LL (#1513) (cdds_transfer): CDDS can now process data for the model UKESM1-ice-LL (#1513) (extract): CDDS can now process data for the model UKESM1-ice-LL (#1513) (hadsdk): CDDS can now process data for the model UKESM1-ice-LL (#1513) (hadsdk): Added write_rose_suite_request_json to construct a request JSON file from the rose-suite.info file within a CMIP6 suite (#1732) (transfer): CDDS can now process data for the model UKESM1-ice-LL (#1513)","title":"Release 1.4.3, May 12, 2020"},{"location":"changes_cdds/#release-142-april-30-2020","text":"(cdds_qc_plugin_cmip6): CDDS QC now handles variables where the cell_measures variable attribute is optional (#1801) (hadsdk): write_request_json now gives a useful error message when there is inconsistent data in CREM for a package (#1765)","title":"Release 1.4.2, April 30, 2020"},{"location":"changes_cdds/#release-141-april-28-2020","text":"(cdds_convert): CDDS Convert now does not raise an AttributeError relating to the --email-notifications argument when run (#1780) (cdds_qc): CDDS QC can now check hourly frequency datasets and does not raise a KeyError exception (#1782)","title":"Release 1.4.1, April 28, 2020"},{"location":"changes_cdds/#release-140-april-23-2020","text":"(cdds_convert): Sub-daily data can now be produced (#825, #1577, #1704) (cdds_convert): The memory requested for different tasks within the suites is now controlled on a per grid/stream basis (#941, #1474) (cdds_convert): E-mail alerts now issued by suites launched by CDDS Convert when completing or stalling (#1669) (cdds_qc): Variables where a coordinate has text values, e.g. Omon/hfbasin , can now be properly validated (#1456) (cdds_qc): Datasets at sub-daily frequencies can now be validated (#1578) (cdds_qc): CRITICAL failures are now logged for each dataset that fails a test (#1681) (extract): Extract can now extract data from sub-daily PP streams (#359) (extract): Extract processes for individual streams can now be run in separate tasks (#1619) (hadsdk): When writing request JSON files from CREM, warnings are issued if base dates are not 1850-01-01 (#1703)","title":"Release 1.4.0, April 23, 2020"},{"location":"changes_cdds/#release-134-march-27-2020","text":"(cdds_prepare): prepare_alter_variable_list now includes additional variable metadata allowing cdds_store_spice to operate correctly for these variables (#1621) (cdds_qc): QC has better support for processing a single stream using the --stream command line option (#1622) (cdds_transfer): move_in_mass now only lists directories in MASS connected with the Request provided, significantly reducing run time (#1649). (transfer): Errors arising from MOO failures are now properly handled and logged (#1701) (transfer): Transfer now supports processing variables with different output names to their variable ID, and has better support for processing a single stream using the --stream command line option (#1622)","title":"Release 1.3.4, March 27, 2020"},{"location":"changes_cdds/#release-133-february-25-2020","text":"(cdds_convert): cdds_convert script now correctly returns a non-zero error code on error (#1501). (cdds_convert): The conversion process can now be limited to specified streams via the --streams command line argument (#1594) (cdds_convert): The conditions under which an organise_files_final task have been modified to avoid scheduling duplicate file management processes (#1585) (extract): Extract will no longer log CRITICAL errors when processing data streams containing no retrievable variables (#1583, #1354).","title":"Release 1.3.3, February 25, 2020"},{"location":"changes_cdds/#release-132-january-27-2020","text":"(cdds_convert): CRITICAL error messages from MIP Convert are correctly captured in the critical_issues.log file, and known exceptions from within MIP Convert will no longer lead to task failure in CDDS Convert suites (#1533). (cdds_qc): Implemented file size checker (#1530) (extract): Clarified CRITICAL messages when PP files are found to be incomplete (#1528) (extract): Extract does not now raise an error when attempting to interpret the request file for HadGEM3-GC31-MM packages (#1563) (transfer): cdds_store now handles experiment ids including the - character (#1556)","title":"Release 1.3.2, January 27, 2020"},{"location":"changes_cdds/#release-131-january-20-2020","text":"(cdds_configure): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_convert): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_prepare): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_qc): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_qc_plugin_cf17): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_qc_plugin_cmip6): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (cdds_transfer): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (extract): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (hadsdk): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (transfer): Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) (transfer): cdds_store_spice now passes all arguments through to spice job (#1511) (transfer): cdds_store behaves correctly when there are no files to archive (#1520)","title":"Release 1.3.1, January 20, 2020"},{"location":"changes_cdds/#release-130-january-14-2020","text":"(cdds_prepare): The new format of the approved variables file is now handled (#1175). (cdds_prepare): Added EmonZ/sltbasin to the list of known good variables to avoid CRITICAL issue for UKESM1 processing (#1437) (cdds_qc): A column with dataset filepaths has been added to the approved variables file (#1175) (cdds_qc): The quality checking can now be restricted to a single stream (#1248) (cdds_qc): CDDS QC now validates parent information when the branch_method is no parent (#1453) (cdds_transfer): The new format of the approved variables file is now handled (#1175). (cdds_transfer): Added new archiving tool cdds_store to replace send_to_mass and cdds_transfer_spice (#1384) (extract): CDDS Extract no longer connects to CREM to retrieve request information (#1079, #1080). (extract): --root_proc_dir and --root_data_dir arguments are now correctly interpreted by the cdds_extract_spice command (#1478). (extract): Error messages related to unreadable pp files are now being correctly logged (#1154). (transfer): Added new archiving tool cdds_store to replace send_to_mass and cdds_transfer_spice (#1384)","title":"Release 1.3.0, January 14, 2020"},{"location":"changes_cdds/#release-123-november-26-2019","text":"(hadsdk): thetaot and sltbasin can now be produced after correcting their stream (#1009)","title":"Release 1.2.3, November 26, 2019"},{"location":"changes_cdds/#release-122-november-13-2019","text":"(cdds_prepare): The create_cdds_directory_structure script now sets permissions on $CDDS_PROC_DIR/archive/log appropriately so users don\\'t have to (#1347) (cdds_qc): The data request version can now be overridden from the command line when running qc_run_and_report (#1375) (cdds_transfer): The failure to submit a RabbitMQ message, which triggers the publication process, is now accompanied by a critical log message (#1346) (extract): CDDS Extract will now log WARNING rather than CRITICAL messages when there are no variables to be extracted for a stream (#1354) (extract): All moose commands are now non-resumable, in order to avoid cases where MASS outages lead to conflicts with CDDS Extract processes (#1374)","title":"Release 1.2.2, November 13, 2019"},{"location":"changes_cdds/#release-121-october-29-2019","text":"(cdds_convert): The cdds_convert.log file is now written to the convert/log processing directory rather than the working directory (as seen at v1.2.0) (#1341) (cdds_convert): CDDS Convert now ignores streams where there are no to be processed rather than raising a KeyError (#1282) (cdds_qc): The validation of the CMIP6 license in now compares the license global attribute against the text provided in the request.json file (#1297) (extract): A bug in the validation process, introduced in v1.2.0, that resulted in all NEMO model output files being incorrectly identified as unreadable, and then deleted, has been fixed (#1333)","title":"Release 1.2.1, October 29, 2019"},{"location":"changes_cdds/#release-121-october-28-2019","text":"(hadsdk): PP header fixes are now correctly applied when producing information required to construct query files for moo select commands in CDDS Extract. This enables the correct extraction of data for clisccp (#1330)","title":"Release 1.2.1, October 28, 2019"},{"location":"changes_cdds/#release-120-october-17-2019","text":"(cdds_configure): The cdds_configure script is now called generate_user_config_files (#1043) (cdds_configure): The --mohc and --root_config command line options for the generate_user_config_files script have now been removed (all arguments provided by the CDDS configuration files are now provided via command line options, see below; #1043) (cdds_configure): The --data_request_version , --template_name , --root_proc_dir and root_data_dir command line options for the generate_user_config_files script have now been added (#1043, #1070) (cdds_convert): The --root_config command line option for the cdds_convert script has now been removed (all arguments provided by the CDDS configuration files are now provided via command line options, see below; #1070) (cdds_convert): The --root_proc_dir and root_data_dir command line options for the cdds_convert script have now been added (#1070) (cdds_prepare): The --root_config command line option for the create_cdds_directory_structure and prepare_generate_variable_list scripts has now been removed (all arguments provided by the CDDS configuration files are now provided via command line options, see below; #1164) (cdds_prepare): The --root_proc_dir and root_data_dir command line options for the create_cdds_directory_structure script have now been added (#1164) (cdds_prepare): The --data_request_version , --data_request_base_dir , --mapping_status , --root_proc_dir and root_data_dir command line options for the prepare_generate_variable_list script have now been added (#1164) (cdds_prepare): Error RuntimeError: requested_variables targeted by rule 0 already active or RuntimeError: requested_variables targeted by rule 0 already inactive no longer occurs. Instead the comment describing the change will be added to the log and comments in the and the state will remain unchanged (#1210) (cdds_prepare): Error ExperimentNotFoundError: Experiment name \"<experiment identifier>\" not found in this version of the data request no longer occurs when calling the prepare_generate_variable_list script when it is looking in the version of the used for model configuration. Instead if the is not defined in that version, a fallback and the associated with that will be used for comparison with the current version of the data request (#1256) (cdds_prepare): Additional , for which the description in the data request has changed between the versions used to configure the model and perform the processing, can now be produced (#1018) (cdds_prepare): Ocean that use with constraints can now be produced (#995) (cdds_qc): The --root_config command line option for the qc_run_and_report script has now been removed (all arguments provided by the CDDS configuration files are now provided via command line options, see below; #1167) (cdds_qc): The standard_names_dir , controlled_vocabulary_dir , --root_proc_dir and root_data_dir command line options for the qc_run_and_report script have now been added (#1167) (cdds_qc_plugin_cmip6): CDDS QC now confirms that the parent_experiment_id attribute in the is consistent with the Request and , allowing for the choice of parent_experiment_id where there are options within the (#1083) (cdds_transfer): The send_to_mass and cdds_transfer_spice scripts now raise critical log messages rather than error log messages, where appropriate (#1045) (extract): CDDS Extract now handles TSSC_SPANS_TOO_MANY_RESOURCES errors raised by moo commands that attempt to access files on too many tapes (#814) (hadsdk): The mip_era and --root_config command line options for the write_request_json script have now been removed (all arguments provided by the CDDS configuration files are now provided via command line options; #1078) (hadsdk): The --root_config command line option for the check_request_json_against_crem script has now been removed (all arguments provided by the CDDS configuration files are now provided via command line options; #1078) (hadsdk): The value of the parent_time_units attribute can now be interpreted by cf_units (#1220) (hadsdk): The write_request_json script now writes the parent_experiment_id to the request JSON file (#1109)","title":"Release 1.2.0, October 17, 2019"},{"location":"changes_cdds/#release-114-september-2-2019","text":"(cdds_convert): To avoid causing storage issues on SPICE the MIP Convert tasks in the CDDS Convert Rose suites now report the local disk usage (if suite variable \\$STAGING_DIR is set). If this limit is specified then tasks that exceed the limit will fail (#1158). (cdds_convert): To enable HadGEM3-GC31-MM (N216) processing the requests for local temporary storage (\\$TMPDIR), in the MIP Convert tasks within the Rose suites, have been extended (#1158). (extract): When validating model output files, Extract now identifies faulty files with no time records (#992). (hadsdk): Experiment name not found in this version of the data request errors in prepare_generate_variable_list , which occurred for some in HadGEM3, have been resolved (#1189)","title":"Release 1.1.4, September 2, 2019"},{"location":"changes_cdds/#release-113-july-31-2019","text":"(cdds_qc): Only the directory containing the are searched when looking for files to check (temporary files in other directories are ignored; (#1046) (cdds_qc): The rather than the name used in the filename (the out_name ) is now used when creating the approved list of (#1052) (cdds_transfer): To ensure messages sent to CEDA when withdrawing data contain the correct version date stamp, the version date stamp used in the path in MASS now doesn\\'t change during state changes (#919) (extract): To avoid issues related to extracting a large number of netCDF from MASS, the extraction is now performed in multiple chunks (#991)","title":"Release 1.1.3, July 31, 2019"},{"location":"changes_cdds/#release-112-july-3-2019","text":"(cdds_convert): Following updates to the MIP Concatenate sizing file to ensure that containing daily output on model levels are now correctly sized (#1002) (hadsdk): Ocean ancillary variables are now correctly recognised in several expressions (#944)","title":"Release 1.1.2, July 3, 2019"},{"location":"changes_cdds/#release-111-june-27-2019","text":"(cdds_prepare): RFMIP is now included in the list of responded to by default when constructing (#994) (cdds_qc_plugin_cmip6): Fixed a bug in CF Standard Names parser that prevented validation of Emon/ta (#993). (extract): Faulty are now removed when issues are identified (#918).","title":"Release 1.1.1, June 27, 2019"},{"location":"changes_cdds/#release-110-june-12-2019","text":"(cdds_convert): CDDS Convert now preferentially uses cftime , falling back to using netcdftime if cftime is not available in the environment (#249) (cdds_convert): A temporary local storage allocation is now requested in SPICE job scripts in order to avoid job failures due to lack of disk space (#824) and the settings controlling the temporal extent of the output files has been updated to include daily IPCC critical variables (#824, #883, #921) (cdds_convert): can now be processed from the apm (#922) (cdds_prepare): that do not exist in the version of the used to configure the are now correctly accounted for when determining whether the has changed significantly between the version of the used to setup the and the specified version of the (#249) (cdds_prepare): The list of that are included by default when constructing now includes PAMIP and CDRMIP (plus the correct spelling of AerChemMIP ) (#832) (cdds_prepare): Ocean biogeochemistry field definitions from the model suites are now used when identifying that can be produced (#820) (cdds_qc): The start and end dates in the file name for each are now validated against the (#909) (cdds_qc_plugin_cmip6): The CMIP6 Compliance Checker Plugin now preferentially uses cftime , falling back to using netcdftime if cftime is not available in the environment (#249) (extract): Daily ocean data can now be extracted from MASS (#882) (extract): Extract logs now contain the CRITICAL keyword for critical issues (#903) (extract): Exceeded step memory limit errors in cdds_extract_spice have been resolved (#915) (extract): The production of MEDUSA ocean biogeochemistry are now supported (#582) (hadsdk): Exceeded step memory limit errors in cdds_extract_spice have been resolved (#915) (hadsdk): The production of MEDUSA ocean biogeochemistry are now supported (#582) (hadsdk): HadSDK now preferentially uses cftime , falling back to using netcdftime if cftime is not available in the environment (#249)","title":"Release 1.1.0, June 12, 2019"},{"location":"changes_cdds/#release-105-may-10-2019","text":"(cdds_prepare): All in the can now be deactivated except those specified (which is useful for testing purposes) using the new prepare_select_variables script (#887)","title":"Release 1.0.5, May 10, 2019"},{"location":"changes_cdds/#release-104-may-2-2019","text":"(cdds_configure): MIP Convert template files for the ocean are now produced separately for each sub-stream to avoid an issue where ocean files _ may be produced with incorrect spatial coordinates (#871). This change also improves CDDS Convert end-to-end performance by splitting processing into several smaller batch jobs that can be run in parallel. (cdds_convert): Candidate concatenation file is now written to an intermediate directory, so that temporary files do not land up in the output directory and then in the publication pipeline (#849) (cdds_convert): Fixed a bug where after a timeout, a concatenation task would not produce all the expected output (#849) (cdds_transfer): Added command line option to move_in_mass script to allow state changes, such as embargoed to available or available to withdrawn , to be limited to a list of variables (#876)","title":"Release 1.0.4, May 2, 2019"},{"location":"changes_cdds/#release-104-april-30-2019","text":"(cdds_qc_plugin_cmip6): The validation of the activity_id global attribute in , as written by , now allows for multiple to be specified in the CMIP6 for a particular . For example, data produced for ssp370 should have an activity_id of ScenarioMIP AerChemMIP (#865) (extract): Some unit test utility functions were moved from extract to hadsdk (#865). (hadsdk): Some unit test utility functions were moved from extract to hadsdk (#865)","title":"Release 1.0.4, April 30, 2019"},{"location":"changes_cdds/#release-103-april-18-2019","text":"(cdds_convert): Mip_convert and mip_concatenate now write to three separate directories: one for mip_convert output, a second for input files to the concatenation tasks and a third for the final concatenation tasks which is where qc and transfer will look for the output (#840) (cdds_convert): Updated the calculation of the concatenation task cycle time for suites that only have 1 scheduled concatenation task due to the short run time (#852) (cdds_transfer): The MOOSE command-id is now reported via run_moo_cmd to allow CEDA\\'s CREPP tools to kill processes left running on the MOOSE controllers that should have ended when the command line MOOSE client exited (#855)","title":"Release 1.0.3, April 18, 2019"},{"location":"changes_cdds/#release-102-april-5-2019","text":"(hadsdk): The hadsdk.common.netCDF_regexp() function now returns a regular expression that contains 4 match groups, allowing the extraction of NEMO (#834)","title":"Release 1.0.2, April 5, 2019"},{"location":"changes_cdds/#release-101-april-2-2019","text":"(cdds_configure): The grid label gn is now used for datasets that use on all T/U/V/UV grids (#761) (cdds_convert): The and grid identifiers are now obtained directly from the (#555) (cdds_convert): One Rose suite per is now submitted rather than one Rose suite per request (#710) (cdds_convert): The argument provided to the --rose-suite-branch parameter can now be a path to a checked out version of the Rose suite (for development purposes only; #739) (cdds_convert): The Rose suite now removes directories where outputs are written prior to producing those outputs (#750) (cdds_convert): The concatenation periods are now aligned with the reference date (#762) (cdds_convert): A single is now produced if the concatenation period is more than the run bounds (#778) (cdds_convert): The correct files are now included in each concatenation period (#780) (cdds_convert): The cycles to run MIP Convert are now aligned with the reference date (#781) (cdds_prepare): The significant changes between the in the version of the used to setup HadGEM3 and version 01.00.29 of the were approved and added to KNOWN_GOOD_VARIABLES (#747) (cdds_qc): The request JSON file is now a positional argument to qc_run_and_report (#697) (cdds_qc): Single timestep are now handled appropriately (#769) (cdds_qc): Bound checks related to single depth levels are now ignored by default (the parameter --do_not_filter can be used to report these issues; #792) (cdds_qc): A detailed report can now be generated using the --details parameter (#793) (cdds_qc): An approved list of can now be generated, which can be used to deactivate in the for the next round of processing (#819) (cdds_transfer): The new command line script cdds_transfer_spice can now be used to run CDDS Transfer on SPICE (#688) (cdds_transfer): The name of the directory used to store on MASS can now be specified via the --mass_location parameter (#756) (cdds_transfer): The SPICE logs for cdds_transfer_spice are now written to the correct location when using the --use_proc_dir parameter (#828) (extract): A workaround was implemented to deal with the fact that the moo ls command sometimes returns a truncated list of filenames (#771) (hadsdk): The write_request_json script now works correctly for experiments without parents, and retrieves the correct value of the from CREM (#730)","title":"Release 1.0.1, April 2, 2019"},{"location":"changes_cdds/#release-100-february-1-2019","text":"(cdds_configure): First implementation of CDDS. (cdds_convert): First implementation of CDDS. (cdds_prepare): First implementation of CDDS. (cdds_qc): First implementation of CDDS. (cdds_qc_plugin_cf17): First implementation of CDDS. (cdds_qc_plugin_cmip6): First implementation of CDDS. (cdds_transfer): First implementation of CDDS. (extract): First implementation of CDDS. (hadsdk): First implementation of CDDS. (transfer): First implementation of CDDS.","title":"Release 1.0.0, February 1, 2019"},{"location":"changes_mip_convert/","text":"Release 3.1.0rc1, February 11, 2025 Ancillary STASH variables can now be configured via the CDDS Plugins and the MIP Convert config file (CDDSO-582) Added new AerChem mappings to GCAERmon table in GCModelDev (CDDSO-483) Release 3.0.6, January 21, 2024 Mapping corrections for HadREM3-GA7-05 (CDDSO-581) including corrections to time sampling for some hourly variables (CDDSO-588) Release 3.0.5rc1, December 5, 2024 (only to be used for LESFMIP) Changes were made to the following mappings, needed for LESFMIP processing, but the changes were not merged into release branches or the main branch. Variables altered; EmonZ/epfy , EmonZ/epfz , EmonZ/vtem , EmonZ/wtem . Release created from branch CDDSO-560_lesfmip-special-release (CDDSO-560). Release 3.0.4, November 19, 2024 Added HadGEM3 mapping for AERday/ua10 , AEday/ua10 that does not use the Heaviside field (CDDSO-547) Altered mapping for HadREM3 mon/tas to use daily data to calculate monthly mean (CDDSO-548) Release 3.0.3, October 24, 2024 Allow forced coordinate rotation. Output data will be forced to include rotated coordinates and true lat-lon coordinates if the force_coordinate_rotation parameter in the request section of the MIP Convert config file is set to True (CDDSO-529) Release 3.0.2, September 27, 2024 Add functionality to remove halo columns and rows from atmosphere data (CDDSO-521) Add Mappings for CP4A (CDDSO-515) Release 3.0.1, August 21, 2024 Separated test reference data by versions in MIP Convert functional tests (CDDSO-400) Added a log identifier to separate log files of multiple functional tests in the same test class (CDDSO-476) Added mappings for CORDEX (CDDSO-421) Release 3.0.0, May 15, 2024 MIP Convert now returns exit codes correctly (CDDSO-387) The field child_base_date in the request section of the MIP Convert Config file has been renamed for clarity to base_date (CDDSO-455) Release 2.5.10, December 12, 2024 Add mappings for the eUKESM1 model (duplicates of the existing UKESM1*.cfg mappings) (CDDSO-572) Release 2.5.9, June 25, 2024 Added new MIP tables (GC3hrPtUV, GCAmon6hr, GCAmonUV, GC3hr, GC1hr) and mappings for CP4A, along with sizing and memory configuration tuning for new diagnostics (CDDSO-414, CDDSO-463). Release 2.5.8, February 28, 2024 No changes Release 2.5.7, February 21, 2024 Additional Mappings for the HadREM3-CP4A-4p5km model (CDDSO-354, CDDSO-405) Release 2.5.6, February 09, 2024 Updated mapping for fco2nat to use anomalous LBTIM code in UKESM model data. This change makes no difference to the output data, but keeps consistency with previous CMIP6 data production (CDDSO-394) Added GCAmon6hr/tas mapping (tas, but with monthly means sampled every 6 hours) to GCModel Dev (CDDSO-389) Added new variables for UKCP to GCModelDev (CDDSO-388) Release 2.5.5, January 18, 2024 Issues with incorrect/inconsistent LBFT codes in PP data (section 3) should no longer break processing (CDDSO-383) Release 2.5.4, December 13, 2023 Fix the reference_date parsing logic for variables defined with a forecast time dimension (CDDSO-363) Implement annual ocean diagnostics for the GCOyr MIP table (CDDSO-369) Release 2.5.3, November 22, 2023 Adaptations needed for new CMIP6Plus MIP tables (CDDSO-336, CDDSO-365) Update to version of CMOR to 3.6.3 (CDDSO-368) Release 2.5.2, October 18, 2023 Polar row masking is now specified directly in the MIP Convert config files (CDDSO-331) Upgraded CMOR to version 3.7.3, which now permits much larger arrays to be passed for writing without segfaulting (CDDSO-357) Release 2.5.1, August 4, 2023 Add zos mapping for HadGEM3-GC3p05-N216ORCA025 (CDDSO-328) Release 2.5.0, July 27, 2023 Updated CMOR to version 3.7.2, which required iris update to 3.4.1 (CDDSO-321) Relocated test data to enable testing on multiple platforms (CDDSO-278) Change date format in config file to ISO datetime (CDDSO-294, CDDSO-313) Release 2.4.6, June 23, 2023 Add Eday/evspsbl mapping to MIP Convert (CDDSO-291) Release 2.4.5, May 22, 2023 MIP Convert can now correctly output data from CICE daily streams where the time coordinate data is faulty, and SIday/siconc can be produced (CDDSO-277) Release 2.4.4, May 4, 2023 No changes Release 2.4.3, March 31, 2023 Add a --relaxed-cmor option to mip_convert . When used certain metadata fields e.g. experiment_id are not checked against the controlled vocabularies (CDDSO-252). All ancillary STASH codes are now correctly treated as time invariant (CDDSO-247). Release 2.4.2, March 1, 2023 Add mappings for zostoga diagnostic in the UKCP18 GC3p05-N216ORCA025 model configuration (CDDSO-239) Release 2.4.1, January 18, 2023 Implement a mask_slice option in configuration file for providing ocean data masks (CDDSO-67, 215) Add support for the UKCP18 GC3p05-N216ORCA025 model and UV grid mappings (CDDSO-222) Release 2.4.0, September 12, 2022 Move test execution from nose to pytest (CDDSO-128) Refactor functional tests (CDDSO-76, 170) Release 2.3.2, September 01, 2022 MIP Convert can now produce files with a forecast time coordinate as introduced at CMOR v3.7.0 (CDDSO-124) Approved mapping for fsitherm (CDDSO-163) Release 2.3.1, June 29, 2022 Add extra pressure levels for UKCP (CDDSO-146) Release 2.3.0, May 24, 2022 Development moved to github Duplicate coordinates error rased when producing certain variables is now not raised (CDDSO-109) Release 2.2.5, May 4, 2022 No changes Release 2.2.4, April 22, 2022 No changes Release 2.2.3, April 7, 2022 No changes Release 2.2.2, March 18, 2022 CV checks for non-CMIP6 processing is now the same as for CMIP6 (#2539) Additional mappings for UKCP (#2518) Release 2.2.1, February 15, 2022 No changes. Release 2.2.0, February 9, 2022 Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) Update to use python 3.8 (#2438) Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) Retired umfilelist and all connected code (#2438) Release 2.1.2, November 25, 2021 mip_convert now can handle LBTIM constraints correctly (#2455) Release 2.1.1, October 26, 2021 Added regional model variable mappings for CORDEX (#2442) Release 2.1.0, September 6, 2021 No changes. Release 2.0.8, August 3, 2021 Refactored mip_convert to improve performance (#2423) Release 2.0.7, July 15, 2021 No changes. Release 2.0.6, June 29, 2021 No changes. Release 2.0.5, June 11, 2021 No changes. Release 2.0.4, May 17, 2021 Added approved mappings for depth integrated upper ocean (top 100m) biogeochemistry variables (#2146) Release 2.0.3, April 28, 2021 No changes. Release 2.0.2, April 22, 2021 Approved mappings for E3hrPt/ts , CFmon/ta , CFday/ta and CFday/ta700 diagnostics (#2303, #2326) Fixed a bug in MIP Convert which prevented it from providing CMOR with correct Controlled Vocabulary file (#2317) Release 2.0.1, March 25, 2021 MIP Convert now allows seasonal mean data to be produced (#943) Release 2.0, February 24, 2021 Updated CDDS codebase to Python 3.6. Release 1.6.5, February 22, 2021 No changes. Release 1.6.4, February 11, 2021 No changes. Release 1.6.3, February 9, 2021 MIP Convert can now produce monthly mean diurnal cycle diagnostics (#349) Added processor for loaddust (#1823) Approved several mappings (#2220) Release 1.6.2, January 11, 2021 Approved mappings for mc , mcd and mcu diagnostics (#2177) Release 1.6.1, November 26, 2020 No changes. Release 1.6.0, November 05, 2020 Fixed processors for vertical integrals of ocean biogeochemical variables (#1192) Corrected jpdftaure* diagnostics related to the cloud area fraction in the atmosphere layer (#558, #2091) Added several new mappings (#1822, #1845, #1846) and updated review statuses (#2090) Release 1.5.5, October 20, 2020 No changes. Release 1.5.4, October 7, 2020 No changes. Release 1.5.3, September 16, 2020 No changes. Release 1.5.2, September 4, 2020 No changes. Release 1.5.1, August 20, 2020 Corrections to vertical coordinate data in some CFsubhr radiation mappings (#1946) Approval of several mappings (#1975) and removal of ok status from one (#1992) Release 1.5.0, July 2, 2020 Corrected mappings for wbptemp and rsut*4co2 (#567, #1502) Added many new mappings (#848, #1691, #1803, #1820) and updated review statuses (#1855) Added mappings for CF sites variables (#1838) zostoga can now be produced for HadGEM3-GC31-MM (#1266) evs can now be marked as active by CDDS Prepare following alterations to the mapping, which avoids an issue when reading the NEMO iodef.xml file (#1419) Removed approved status of sipr , siflfwdrain and siflfwbot mappings pending re-review (#1853) Release 1.4.5, June 16, 2020 No changes. Release 1.4.4, June 1, 2020 No changes. Release 1.4.3, May 12, 2020 CDDS can now process data for the model UKESM1-ice-LL (#1513) Release 1.4.2, April 30, 2020 No changes. Release 1.4.1, April 28, 2020 No changes. Release 1.4.0, April 23, 2020 Added new approved mappings for sub-daily variables (#1704) ocean biogeochemistry (#1575), and FAFMIP (#1504). MIP Convert can now correctly produce sub-daily variables (#1577) Release 1.3.4, March 27, 2020 No changes. Release 1.3.3, February 25, 2020 Processors using mask_copy routine do not now raise a TypeError referring to the numpy copyto() function (#1537) Corrected masking of polar row in data from CICE. Previously ORCA1 sea-ice data on T points and ORCA025 data on UV points was being masked incorrectly (#1624) Release 1.3.2, January 27, 2020 CRITICAL error messages from MIP Convert are correctly captured in the critical_issues.log file, and known exceptions from within MIP Convert will no longer lead to task failure in CDDS Convert suites (#1533). Release 1.3.1, January 20, 2020 Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) Added additional environmental variables setup in MIP Convert, and implemented cube rechunking to avoid performance issues when switching to Iris 2.2 (#1195) Release 1.3.0, January 14, 2020 Added processor to avoid ValueError when processing rsusLut (#889) A number of radiation and ocean biogeochemistry are now available (#1153) Added processor for vtem to mask data below 700 hPa (#1411) Release 1.2.3, November 26, 2019 The variable wtem can now be produced (#875) The parent_activity_id global attribute in the is now correctly determined based on the parent_experiment_id (#1409) Release 1.2.2, November 13, 2019 multiply_cubes processor for emibvoc and emiisop corrected to avoid \\\"No data available for {year}; please check run_bounds\\\" errors when processing more than one year of data at a time (#1343) Release 1.2.1, October 29, 2019 Correction of typo in mapping for wetoa (#1330) Approval of mappings for IPCC important variables rlutcs and rlutcsaf (#1331) Release 1.2.0, October 17, 2019 This operation cannot be performed as there are differing coordinates remaining which cannot be ignored errors that occur when applying the expression have now been resolved due to the addition of a multiply_cubes processor (#275) Warnings related to invalid value encountered in divide are now not issued when processing that use the divide_by_mask processor (#391) The duplication of constants in the original_name attribute in the (which described the expression) has now been removed (#1020) The parent_experiment_id must now be specified in the whenever a parent exists (#1118) on half levels (e.g. phalf ) can now be produced (#899) Implemented compatibility with the 01.00.31 version of the (#1056) A number of , including carbon, cftables, cloud and land are now available (#990, #1016, #1257, #1259) The for drybc , fgco2 and evspsblsoi have now been corrected (#846, #1193, #1257) Release 1.1.4, September 2, 2019 No changes. Release 1.1.3, July 31, 2019 The is now written to the global attributes in the to help with identification (the out_name in the sometimes differs from the ; #1051) Release 1.1.2, July 3, 2019 No changes. Release 1.1.1, June 27, 2019 No changes. Release 1.1.0, June 12, 2019 MIP Convert now preferentially uses cftime , falling back to using netcdftime if cftime is not available in the environment (#249) When replacing constants in the expressions, sometimes a constant would be replaced with the incorrect value when the constant has a name that shares the same root as the name of another constant; the constants are now replaced with the correct value and are explicitly written to the (#928) NEMO on different grid points can now be produced in the same call to MIP Convert (#870) Multiple are now validated correctly against the values in the CVs (#904) Surface ocean biogeochemistry can now be produced (#869) Pseudo zonal mean can now be produced from NEMO diaptr files (#270) The production of MEDUSA ocean biogeochemistry are now supported (#582) The issue related to data being set to zero due to integer division is now resolved (#861) A substantial number of , including carbon, land and ocean biogeochemistry, are now available (e.g. #759, #843, #853) Release 1.0.5, May 10, 2019 No changes. Release 1.0.4, April 30, 2019 No changes. Release 1.0.3, April 5, 2019 No changes. Release 1.0.2, April 5, 2019 No changes. Release 1.0.1, April 2, 2019 Substreams can now be specified in the (#198) A workaround was implemented to deal with the bug in iris.util.new_axis , which causes the fill_value to be reset to the default value (#692) The polar rows in NEMO are now masked prior to applying the expression (#702) The type parameter in the call to cmor.variable is now available (#715) The activity_id and source_type required global attributes are now validated by comparing their values to those in the CV file (#721) The version of the CV file is now recorded in the (#763) Release 1.0.0, February 1, 2019 First implementation of CDDS. Implemented compatibility with v3.4.0 and the 01.00.29 version of the .","title":"Changelog (Mip Convert)"},{"location":"changes_mip_convert/#release-310rc1-february-11-2025","text":"Ancillary STASH variables can now be configured via the CDDS Plugins and the MIP Convert config file (CDDSO-582) Added new AerChem mappings to GCAERmon table in GCModelDev (CDDSO-483)","title":"Release 3.1.0rc1, February 11, 2025"},{"location":"changes_mip_convert/#release-306-january-21-2024","text":"Mapping corrections for HadREM3-GA7-05 (CDDSO-581) including corrections to time sampling for some hourly variables (CDDSO-588)","title":"Release 3.0.6, January 21, 2024"},{"location":"changes_mip_convert/#release-305rc1-december-5-2024-only-to-be-used-for-lesfmip","text":"Changes were made to the following mappings, needed for LESFMIP processing, but the changes were not merged into release branches or the main branch. Variables altered; EmonZ/epfy , EmonZ/epfz , EmonZ/vtem , EmonZ/wtem . Release created from branch CDDSO-560_lesfmip-special-release (CDDSO-560).","title":"Release 3.0.5rc1, December 5, 2024 (only to be used for LESFMIP)"},{"location":"changes_mip_convert/#release-304-november-19-2024","text":"Added HadGEM3 mapping for AERday/ua10 , AEday/ua10 that does not use the Heaviside field (CDDSO-547) Altered mapping for HadREM3 mon/tas to use daily data to calculate monthly mean (CDDSO-548)","title":"Release 3.0.4, November 19, 2024"},{"location":"changes_mip_convert/#release-303-october-24-2024","text":"Allow forced coordinate rotation. Output data will be forced to include rotated coordinates and true lat-lon coordinates if the force_coordinate_rotation parameter in the request section of the MIP Convert config file is set to True (CDDSO-529)","title":"Release 3.0.3, October 24, 2024"},{"location":"changes_mip_convert/#release-302-september-27-2024","text":"Add functionality to remove halo columns and rows from atmosphere data (CDDSO-521) Add Mappings for CP4A (CDDSO-515)","title":"Release 3.0.2, September 27, 2024"},{"location":"changes_mip_convert/#release-301-august-21-2024","text":"Separated test reference data by versions in MIP Convert functional tests (CDDSO-400) Added a log identifier to separate log files of multiple functional tests in the same test class (CDDSO-476) Added mappings for CORDEX (CDDSO-421)","title":"Release 3.0.1, August 21, 2024"},{"location":"changes_mip_convert/#release-300-may-15-2024","text":"MIP Convert now returns exit codes correctly (CDDSO-387) The field child_base_date in the request section of the MIP Convert Config file has been renamed for clarity to base_date (CDDSO-455)","title":"Release 3.0.0, May 15, 2024"},{"location":"changes_mip_convert/#release-2510-december-12-2024","text":"Add mappings for the eUKESM1 model (duplicates of the existing UKESM1*.cfg mappings) (CDDSO-572)","title":"Release 2.5.10, December 12, 2024"},{"location":"changes_mip_convert/#release-259-june-25-2024","text":"Added new MIP tables (GC3hrPtUV, GCAmon6hr, GCAmonUV, GC3hr, GC1hr) and mappings for CP4A, along with sizing and memory configuration tuning for new diagnostics (CDDSO-414, CDDSO-463).","title":"Release 2.5.9, June 25, 2024"},{"location":"changes_mip_convert/#release-258-february-28-2024","text":"No changes","title":"Release 2.5.8, February 28, 2024"},{"location":"changes_mip_convert/#release-257-february-21-2024","text":"Additional Mappings for the HadREM3-CP4A-4p5km model (CDDSO-354, CDDSO-405)","title":"Release 2.5.7, February 21, 2024"},{"location":"changes_mip_convert/#release-256-february-09-2024","text":"Updated mapping for fco2nat to use anomalous LBTIM code in UKESM model data. This change makes no difference to the output data, but keeps consistency with previous CMIP6 data production (CDDSO-394) Added GCAmon6hr/tas mapping (tas, but with monthly means sampled every 6 hours) to GCModel Dev (CDDSO-389) Added new variables for UKCP to GCModelDev (CDDSO-388)","title":"Release 2.5.6, February 09, 2024"},{"location":"changes_mip_convert/#release-255-january-18-2024","text":"Issues with incorrect/inconsistent LBFT codes in PP data (section 3) should no longer break processing (CDDSO-383)","title":"Release 2.5.5, January 18, 2024"},{"location":"changes_mip_convert/#release-254-december-13-2023","text":"Fix the reference_date parsing logic for variables defined with a forecast time dimension (CDDSO-363) Implement annual ocean diagnostics for the GCOyr MIP table (CDDSO-369)","title":"Release 2.5.4, December 13, 2023"},{"location":"changes_mip_convert/#release-253-november-22-2023","text":"Adaptations needed for new CMIP6Plus MIP tables (CDDSO-336, CDDSO-365) Update to version of CMOR to 3.6.3 (CDDSO-368)","title":"Release 2.5.3, November 22, 2023"},{"location":"changes_mip_convert/#release-252-october-18-2023","text":"Polar row masking is now specified directly in the MIP Convert config files (CDDSO-331) Upgraded CMOR to version 3.7.3, which now permits much larger arrays to be passed for writing without segfaulting (CDDSO-357)","title":"Release 2.5.2, October 18, 2023"},{"location":"changes_mip_convert/#release-251-august-4-2023","text":"Add zos mapping for HadGEM3-GC3p05-N216ORCA025 (CDDSO-328)","title":"Release 2.5.1, August 4, 2023"},{"location":"changes_mip_convert/#release-250-july-27-2023","text":"Updated CMOR to version 3.7.2, which required iris update to 3.4.1 (CDDSO-321) Relocated test data to enable testing on multiple platforms (CDDSO-278) Change date format in config file to ISO datetime (CDDSO-294, CDDSO-313)","title":"Release 2.5.0, July 27, 2023"},{"location":"changes_mip_convert/#release-246-june-23-2023","text":"Add Eday/evspsbl mapping to MIP Convert (CDDSO-291)","title":"Release 2.4.6, June 23, 2023"},{"location":"changes_mip_convert/#release-245-may-22-2023","text":"MIP Convert can now correctly output data from CICE daily streams where the time coordinate data is faulty, and SIday/siconc can be produced (CDDSO-277)","title":"Release 2.4.5, May 22, 2023"},{"location":"changes_mip_convert/#release-244-may-4-2023","text":"No changes","title":"Release 2.4.4, May 4, 2023"},{"location":"changes_mip_convert/#release-243-march-31-2023","text":"Add a --relaxed-cmor option to mip_convert . When used certain metadata fields e.g. experiment_id are not checked against the controlled vocabularies (CDDSO-252). All ancillary STASH codes are now correctly treated as time invariant (CDDSO-247).","title":"Release 2.4.3, March 31, 2023"},{"location":"changes_mip_convert/#release-242-march-1-2023","text":"Add mappings for zostoga diagnostic in the UKCP18 GC3p05-N216ORCA025 model configuration (CDDSO-239)","title":"Release 2.4.2, March 1, 2023"},{"location":"changes_mip_convert/#release-241-january-18-2023","text":"Implement a mask_slice option in configuration file for providing ocean data masks (CDDSO-67, 215) Add support for the UKCP18 GC3p05-N216ORCA025 model and UV grid mappings (CDDSO-222)","title":"Release 2.4.1, January 18, 2023"},{"location":"changes_mip_convert/#release-240-september-12-2022","text":"Move test execution from nose to pytest (CDDSO-128) Refactor functional tests (CDDSO-76, 170)","title":"Release 2.4.0, September 12, 2022"},{"location":"changes_mip_convert/#release-232-september-01-2022","text":"MIP Convert can now produce files with a forecast time coordinate as introduced at CMOR v3.7.0 (CDDSO-124) Approved mapping for fsitherm (CDDSO-163)","title":"Release 2.3.2, September 01, 2022"},{"location":"changes_mip_convert/#release-231-june-29-2022","text":"Add extra pressure levels for UKCP (CDDSO-146)","title":"Release 2.3.1, June 29, 2022"},{"location":"changes_mip_convert/#release-230-may-24-2022","text":"Development moved to github Duplicate coordinates error rased when producing certain variables is now not raised (CDDSO-109)","title":"Release 2.3.0, May 24, 2022"},{"location":"changes_mip_convert/#release-225-may-4-2022","text":"No changes","title":"Release 2.2.5, May 4, 2022"},{"location":"changes_mip_convert/#release-224-april-22-2022","text":"No changes","title":"Release 2.2.4, April 22, 2022"},{"location":"changes_mip_convert/#release-223-april-7-2022","text":"No changes","title":"Release 2.2.3, April 7, 2022"},{"location":"changes_mip_convert/#release-222-march-18-2022","text":"CV checks for non-CMIP6 processing is now the same as for CMIP6 (#2539) Additional mappings for UKCP (#2518)","title":"Release 2.2.2, March 18, 2022"},{"location":"changes_mip_convert/#release-221-february-15-2022","text":"No changes.","title":"Release 2.2.1, February 15, 2022"},{"location":"changes_mip_convert/#release-220-february-9-2022","text":"Introduced plugin system for project information (e.g. CMIP6) and model descriptions to enable use of CDDS outside of CMIP6 (#2460, #2461, #2462, #2468, #2494, #2502, #2503, #2504, #2509, #2510, #2512, #2513, #2514) Update to use python 3.8 (#2438) Allow for the use of CDDS beyond CMIP6 (#2449, #2469, #2470) Retired umfilelist and all connected code (#2438)","title":"Release 2.2.0, February 9, 2022"},{"location":"changes_mip_convert/#release-212-november-25-2021","text":"mip_convert now can handle LBTIM constraints correctly (#2455)","title":"Release 2.1.2, November 25, 2021"},{"location":"changes_mip_convert/#release-211-october-26-2021","text":"Added regional model variable mappings for CORDEX (#2442)","title":"Release 2.1.1, October 26, 2021"},{"location":"changes_mip_convert/#release-210-september-6-2021","text":"No changes.","title":"Release 2.1.0, September 6, 2021"},{"location":"changes_mip_convert/#release-208-august-3-2021","text":"Refactored mip_convert to improve performance (#2423)","title":"Release 2.0.8, August 3, 2021"},{"location":"changes_mip_convert/#release-207-july-15-2021","text":"No changes.","title":"Release 2.0.7, July 15, 2021"},{"location":"changes_mip_convert/#release-206-june-29-2021","text":"No changes.","title":"Release 2.0.6, June 29, 2021"},{"location":"changes_mip_convert/#release-205-june-11-2021","text":"No changes.","title":"Release 2.0.5, June 11, 2021"},{"location":"changes_mip_convert/#release-204-may-17-2021","text":"Added approved mappings for depth integrated upper ocean (top 100m) biogeochemistry variables (#2146)","title":"Release 2.0.4, May 17, 2021"},{"location":"changes_mip_convert/#release-203-april-28-2021","text":"No changes.","title":"Release 2.0.3, April 28, 2021"},{"location":"changes_mip_convert/#release-202-april-22-2021","text":"Approved mappings for E3hrPt/ts , CFmon/ta , CFday/ta and CFday/ta700 diagnostics (#2303, #2326) Fixed a bug in MIP Convert which prevented it from providing CMOR with correct Controlled Vocabulary file (#2317)","title":"Release 2.0.2, April 22, 2021"},{"location":"changes_mip_convert/#release-201-march-25-2021","text":"MIP Convert now allows seasonal mean data to be produced (#943)","title":"Release 2.0.1, March 25, 2021"},{"location":"changes_mip_convert/#release-20-february-24-2021","text":"Updated CDDS codebase to Python 3.6.","title":"Release 2.0, February 24, 2021"},{"location":"changes_mip_convert/#release-165-february-22-2021","text":"No changes.","title":"Release 1.6.5, February 22, 2021"},{"location":"changes_mip_convert/#release-164-february-11-2021","text":"No changes.","title":"Release 1.6.4, February 11, 2021"},{"location":"changes_mip_convert/#release-163-february-9-2021","text":"MIP Convert can now produce monthly mean diurnal cycle diagnostics (#349) Added processor for loaddust (#1823) Approved several mappings (#2220)","title":"Release 1.6.3, February 9, 2021"},{"location":"changes_mip_convert/#release-162-january-11-2021","text":"Approved mappings for mc , mcd and mcu diagnostics (#2177)","title":"Release 1.6.2, January 11, 2021"},{"location":"changes_mip_convert/#release-161-november-26-2020","text":"No changes.","title":"Release 1.6.1, November 26, 2020"},{"location":"changes_mip_convert/#release-160-november-05-2020","text":"Fixed processors for vertical integrals of ocean biogeochemical variables (#1192) Corrected jpdftaure* diagnostics related to the cloud area fraction in the atmosphere layer (#558, #2091) Added several new mappings (#1822, #1845, #1846) and updated review statuses (#2090)","title":"Release 1.6.0, November 05, 2020"},{"location":"changes_mip_convert/#release-155-october-20-2020","text":"No changes.","title":"Release 1.5.5, October 20, 2020"},{"location":"changes_mip_convert/#release-154-october-7-2020","text":"No changes.","title":"Release 1.5.4, October 7, 2020"},{"location":"changes_mip_convert/#release-153-september-16-2020","text":"No changes.","title":"Release 1.5.3, September 16, 2020"},{"location":"changes_mip_convert/#release-152-september-4-2020","text":"No changes.","title":"Release 1.5.2, September 4, 2020"},{"location":"changes_mip_convert/#release-151-august-20-2020","text":"Corrections to vertical coordinate data in some CFsubhr radiation mappings (#1946) Approval of several mappings (#1975) and removal of ok status from one (#1992)","title":"Release 1.5.1, August 20, 2020"},{"location":"changes_mip_convert/#release-150-july-2-2020","text":"Corrected mappings for wbptemp and rsut*4co2 (#567, #1502) Added many new mappings (#848, #1691, #1803, #1820) and updated review statuses (#1855) Added mappings for CF sites variables (#1838) zostoga can now be produced for HadGEM3-GC31-MM (#1266) evs can now be marked as active by CDDS Prepare following alterations to the mapping, which avoids an issue when reading the NEMO iodef.xml file (#1419) Removed approved status of sipr , siflfwdrain and siflfwbot mappings pending re-review (#1853)","title":"Release 1.5.0, July 2, 2020"},{"location":"changes_mip_convert/#release-145-june-16-2020","text":"No changes.","title":"Release 1.4.5, June 16, 2020"},{"location":"changes_mip_convert/#release-144-june-1-2020","text":"No changes.","title":"Release 1.4.4, June 1, 2020"},{"location":"changes_mip_convert/#release-143-may-12-2020","text":"CDDS can now process data for the model UKESM1-ice-LL (#1513)","title":"Release 1.4.3, May 12, 2020"},{"location":"changes_mip_convert/#release-142-april-30-2020","text":"No changes.","title":"Release 1.4.2, April 30, 2020"},{"location":"changes_mip_convert/#release-141-april-28-2020","text":"No changes.","title":"Release 1.4.1, April 28, 2020"},{"location":"changes_mip_convert/#release-140-april-23-2020","text":"Added new approved mappings for sub-daily variables (#1704) ocean biogeochemistry (#1575), and FAFMIP (#1504). MIP Convert can now correctly produce sub-daily variables (#1577)","title":"Release 1.4.0, April 23, 2020"},{"location":"changes_mip_convert/#release-134-march-27-2020","text":"No changes.","title":"Release 1.3.4, March 27, 2020"},{"location":"changes_mip_convert/#release-133-february-25-2020","text":"Processors using mask_copy routine do not now raise a TypeError referring to the numpy copyto() function (#1537) Corrected masking of polar row in data from CICE. Previously ORCA1 sea-ice data on T points and ORCA025 data on UV points was being masked incorrectly (#1624)","title":"Release 1.3.3, February 25, 2020"},{"location":"changes_mip_convert/#release-132-january-27-2020","text":"CRITICAL error messages from MIP Convert are correctly captured in the critical_issues.log file, and known exceptions from within MIP Convert will no longer lead to task failure in CDDS Convert suites (#1533).","title":"Release 1.3.2, January 27, 2020"},{"location":"changes_mip_convert/#release-131-january-20-2020","text":"Moved to updated scitools environment production_legacy-os43-1; python v2.7, iris v2.2 (#1512) Added additional environmental variables setup in MIP Convert, and implemented cube rechunking to avoid performance issues when switching to Iris 2.2 (#1195)","title":"Release 1.3.1, January 20, 2020"},{"location":"changes_mip_convert/#release-130-january-14-2020","text":"Added processor to avoid ValueError when processing rsusLut (#889) A number of radiation and ocean biogeochemistry are now available (#1153) Added processor for vtem to mask data below 700 hPa (#1411)","title":"Release 1.3.0, January 14, 2020"},{"location":"changes_mip_convert/#release-123-november-26-2019","text":"The variable wtem can now be produced (#875) The parent_activity_id global attribute in the is now correctly determined based on the parent_experiment_id (#1409)","title":"Release 1.2.3, November 26, 2019"},{"location":"changes_mip_convert/#release-122-november-13-2019","text":"multiply_cubes processor for emibvoc and emiisop corrected to avoid \\\"No data available for {year}; please check run_bounds\\\" errors when processing more than one year of data at a time (#1343)","title":"Release 1.2.2, November 13, 2019"},{"location":"changes_mip_convert/#release-121-october-29-2019","text":"Correction of typo in mapping for wetoa (#1330) Approval of mappings for IPCC important variables rlutcs and rlutcsaf (#1331)","title":"Release 1.2.1, October 29, 2019"},{"location":"changes_mip_convert/#release-120-october-17-2019","text":"This operation cannot be performed as there are differing coordinates remaining which cannot be ignored errors that occur when applying the expression have now been resolved due to the addition of a multiply_cubes processor (#275) Warnings related to invalid value encountered in divide are now not issued when processing that use the divide_by_mask processor (#391) The duplication of constants in the original_name attribute in the (which described the expression) has now been removed (#1020) The parent_experiment_id must now be specified in the whenever a parent exists (#1118) on half levels (e.g. phalf ) can now be produced (#899) Implemented compatibility with the 01.00.31 version of the (#1056) A number of , including carbon, cftables, cloud and land are now available (#990, #1016, #1257, #1259) The for drybc , fgco2 and evspsblsoi have now been corrected (#846, #1193, #1257)","title":"Release 1.2.0, October 17, 2019"},{"location":"changes_mip_convert/#release-114-september-2-2019","text":"No changes.","title":"Release 1.1.4, September 2, 2019"},{"location":"changes_mip_convert/#release-113-july-31-2019","text":"The is now written to the global attributes in the to help with identification (the out_name in the sometimes differs from the ; #1051)","title":"Release 1.1.3, July 31, 2019"},{"location":"changes_mip_convert/#release-112-july-3-2019","text":"No changes.","title":"Release 1.1.2, July 3, 2019"},{"location":"changes_mip_convert/#release-111-june-27-2019","text":"No changes.","title":"Release 1.1.1, June 27, 2019"},{"location":"changes_mip_convert/#release-110-june-12-2019","text":"MIP Convert now preferentially uses cftime , falling back to using netcdftime if cftime is not available in the environment (#249) When replacing constants in the expressions, sometimes a constant would be replaced with the incorrect value when the constant has a name that shares the same root as the name of another constant; the constants are now replaced with the correct value and are explicitly written to the (#928) NEMO on different grid points can now be produced in the same call to MIP Convert (#870) Multiple are now validated correctly against the values in the CVs (#904) Surface ocean biogeochemistry can now be produced (#869) Pseudo zonal mean can now be produced from NEMO diaptr files (#270) The production of MEDUSA ocean biogeochemistry are now supported (#582) The issue related to data being set to zero due to integer division is now resolved (#861) A substantial number of , including carbon, land and ocean biogeochemistry, are now available (e.g. #759, #843, #853)","title":"Release 1.1.0, June 12, 2019"},{"location":"changes_mip_convert/#release-105-may-10-2019","text":"No changes.","title":"Release 1.0.5, May 10, 2019"},{"location":"changes_mip_convert/#release-104-april-30-2019","text":"No changes.","title":"Release 1.0.4, April 30, 2019"},{"location":"changes_mip_convert/#release-103-april-5-2019","text":"No changes.","title":"Release 1.0.3, April 5, 2019"},{"location":"changes_mip_convert/#release-102-april-5-2019","text":"No changes.","title":"Release 1.0.2, April 5, 2019"},{"location":"changes_mip_convert/#release-101-april-2-2019","text":"Substreams can now be specified in the (#198) A workaround was implemented to deal with the bug in iris.util.new_axis , which causes the fill_value to be reset to the default value (#692) The polar rows in NEMO are now masked prior to applying the expression (#702) The type parameter in the call to cmor.variable is now available (#715) The activity_id and source_type required global attributes are now validated by comparing their values to those in the CV file (#721) The version of the CV file is now recorded in the (#763)","title":"Release 1.0.1, April 2, 2019"},{"location":"changes_mip_convert/#release-100-february-1-2019","text":"First implementation of CDDS. Implemented compatibility with v3.4.0 and the 01.00.29 version of the .","title":"Release 1.0.0, February 1, 2019"},{"location":"cdds_components/","text":"List of Components Component Name Description archive The cdds_transfer package enables a user to store the output data products in the MASS archive and make them available for download by the ESGF node run by CEDA. common The cdds_common package will supersedes the HadSDK component. It contains a collection of generic Python code used by one or more of the CDDS components. configure The cdds_configure package enables a user to produce the user configuration file for MIP Convert. convert - extract The extract package enables a user to efficiently extract the climate data that will be used in the dissemination process from MASS. mip_convert The mip_convert package enables a user to produce the output netCDF files for a MIP using model output files. misc - prepare The cdds_prepare package enables a user to create the requested variables list and directory structures in preparation for subsequent CDDS components to be run. qc The cdds_qc package enables a user to check whether the output netCDF files conform to the WGCM CMIP standards. Special Plugin Components Component Name Description QC CF1.7 Plugin The CF1.7 plugin (cdds_qc_plugin_cf17) is an extension of the original CF1.6 checker, providing some additional features and configurability. QC CMIP6 Plugin The CMIP6 Compliance Checker Plugin (cdds_qc_plugin_cmip6) provides a suite of tests related to CMIP6 compliance.","title":"CDDS Components"},{"location":"cdds_components/#list-of-components","text":"Component Name Description archive The cdds_transfer package enables a user to store the output data products in the MASS archive and make them available for download by the ESGF node run by CEDA. common The cdds_common package will supersedes the HadSDK component. It contains a collection of generic Python code used by one or more of the CDDS components. configure The cdds_configure package enables a user to produce the user configuration file for MIP Convert. convert - extract The extract package enables a user to efficiently extract the climate data that will be used in the dissemination process from MASS. mip_convert The mip_convert package enables a user to produce the output netCDF files for a MIP using model output files. misc - prepare The cdds_prepare package enables a user to create the requested variables list and directory structures in preparation for subsequent CDDS components to be run. qc The cdds_qc package enables a user to check whether the output netCDF files conform to the WGCM CMIP standards.","title":"List of Components"},{"location":"cdds_components/#special-plugin-components","text":"Component Name Description QC CF1.7 Plugin The CF1.7 plugin (cdds_qc_plugin_cf17) is an extension of the original CF1.6 checker, providing some additional features and configurability. QC CMIP6 Plugin The CMIP6 Compliance Checker Plugin (cdds_qc_plugin_cmip6) provides a suite of tests related to CMIP6 compliance.","title":"Special Plugin Components"},{"location":"cdds_components/convert/","text":"Warning This documentation is currently under construction and may not be up to date. The cdds.convert component designed to check out, configure and run a copy of the suite, specified in the config. Suite \u200bu-ak283 is currently set up for this purpose. Development workflow To work on suite developments please follow the standard practices set out in the Development Workflow and use the --rose-suite-branch argument to cdds_convert to point at the branch you wish to use when running. Release procedure Create a branch named cdds_ , e.g. cdds_1.0.0 of the suite and make the change shown in changeset of the rose suite u-ak283. Create a branch the config and modify the rose_suite_branch settings, e.g. to cdds_1.0.0@100752 if revision 100752 is appropriate, in the [convert] section of CMIP6.cfg. If a different suite id is being used then the rose_suite setting will also need altering. Review config branch as per standard practises, merge to trunk and update config checkout on disk under the cdds account.","title":"convert"},{"location":"cdds_components/convert/#development-workflow","text":"To work on suite developments please follow the standard practices set out in the Development Workflow and use the --rose-suite-branch argument to cdds_convert to point at the branch you wish to use when running.","title":"Development workflow"},{"location":"cdds_components/convert/#release-procedure","text":"Create a branch named cdds_ , e.g. cdds_1.0.0 of the suite and make the change shown in changeset of the rose suite u-ak283. Create a branch the config and modify the rose_suite_branch settings, e.g. to cdds_1.0.0@100752 if revision 100752 is appropriate, in the [convert] section of CMIP6.cfg. If a different suite id is being used then the rose_suite setting will also need altering. Review config branch as per standard practises, merge to trunk and update config checkout on disk under the cdds account.","title":"Release procedure"},{"location":"cdds_components/extract/","text":"Warning This documentation is currently under construction and may not be up to date. CDDS Extract The Extract package enables a user to extract a subset of climate model output files from the MASS tape archive. This is useful because raw atmosphere and ocean output is stored in massive files, containing multiple fields, and their large volume make all processing very slow. Before data transfer happens, Extract selects only relevant bits of output, necessary to produce the requested, CMORised variables. Since CDDS 2.2, Extract is embedded within the cdds_convert script, and it is no longer necessary to run it as a separate process. It is also worth mentioning that despite working with only a subset of climate model output, data transfer from MASS is typically the longest step in the whole CDDS processing pipeline, can take many days, and is prone to failures caused by MASS misbehaviour. An overview of Extract The user configures the extraction process using the request file. The cdds_extract script then performs the following processing steps: retrieves the data dissemination configuration from the request file. retrieves the requested variables list for the experiment. retrieves the model to MIP mapping for each MIP requested variable logs information on the variables for which authorised mappings are available and variables/mappings that are not available and why. creates directories for holding the retrieved data from MASS, the post-processed data which will be disseminated and the directories used to hold the artifacts used by other cdds processes (e.g. log files) creates appropriate PP and netCDF variable filter files that are used to filter the data retrieved from MASS before it is copied to disk. checks that MASS holds the relevant data collections and constructs MOOSE commands that will retrieve the required model data from these data collections (optionally incorporating the relevant filter files) submits constructed MOOSE commands and reports progress in text logs performs validation on the retrieved data to check it meets processing requirement","title":"extract"},{"location":"cdds_components/extract/#cdds-extract","text":"The Extract package enables a user to extract a subset of climate model output files from the MASS tape archive. This is useful because raw atmosphere and ocean output is stored in massive files, containing multiple fields, and their large volume make all processing very slow. Before data transfer happens, Extract selects only relevant bits of output, necessary to produce the requested, CMORised variables. Since CDDS 2.2, Extract is embedded within the cdds_convert script, and it is no longer necessary to run it as a separate process. It is also worth mentioning that despite working with only a subset of climate model output, data transfer from MASS is typically the longest step in the whole CDDS processing pipeline, can take many days, and is prone to failures caused by MASS misbehaviour.","title":"CDDS Extract"},{"location":"cdds_components/extract/#an-overview-of-extract","text":"The user configures the extraction process using the request file. The cdds_extract script then performs the following processing steps: retrieves the data dissemination configuration from the request file. retrieves the requested variables list for the experiment. retrieves the model to MIP mapping for each MIP requested variable logs information on the variables for which authorised mappings are available and variables/mappings that are not available and why. creates directories for holding the retrieved data from MASS, the post-processed data which will be disseminated and the directories used to hold the artifacts used by other cdds processes (e.g. log files) creates appropriate PP and netCDF variable filter files that are used to filter the data retrieved from MASS before it is copied to disk. checks that MASS holds the relevant data collections and constructs MOOSE commands that will retrieve the required model data from these data collections (optionally incorporating the relevant filter files) submits constructed MOOSE commands and reports progress in text logs performs validation on the retrieved data to check it meets processing requirement","title":"An overview of Extract"},{"location":"cdds_components/qc/","text":"Warning This documentation is currently under construction and may not be up to date.","title":"qc"},{"location":"cdds_components/transfer/","text":"The following is from the CDDS Trac pages and relate to the construction of queues on the RabbitMQ server at CEDA Adding a Queue Description We need to add a new type of queue (to the existing set of \"moose\" and \"admin\"), or an additional queue to the original set (\"moose.available\", \"moose.withdrawn\", \"admin.critical\", \"admin.info\"). We have agreed with CEDA to set up queues per project, e.g. CMIP6_available and CMIP6_withdrawn. There is also a testing set of queues to avoid functional unit tests in cdds_transfer from nuking the queues. Implementation There's no method in the API to create queues directly, although invoking message methods (e.g. publishing a message) will create durable queues automatically. If you want to create the queues in advance, you can do so using pika calls. Example code import os import pika from cdds_transfer import config credentials_file = os . path . expandvars ( '$HOME/.cdds_credentials' ) cfg = config . Config ([ credentials_file ]) # get rabbit connection details from the rabbit section of the # cdds credentials file rabbit_cfg = dict ( cfg . _cp . items ( 'rabbit' )) # Prefix to use to construct queue names mip_era = 'CMIP6' # Name of the exchange on the rabbit server that directs # messages to queues. exchange = 'dds' credentials = pika . PlainCredentials ( rabbit_cfg [ 'userid' ], rabbit_cfg [ 'password' ]) connection_parameters = pika . ConnectionParameters ( rabbit_cfg [ 'host' ], rabbit_cfg [ 'port' ], rabbit_cfg [ 'vhost' ], credentials = credentials , ssl = False ) connection = pika . BlockingConnection ( connection_parameters ) channel = connection . channel () channel . exchange_declare ( exchange = exchange , exchange_type = \"direct\" , durable = True ) for queue in [ \"available\" , \"withdrawn\" ]: queue_name = 'moose. {} _ {} ' . format ( mip_era , queue ) channel . queue_declare ( queue = queue_name , durable = True ) channel . queue_bind ( exchange = exchange , queue = queue_name , routing_key = queue_name ) channel . close () connection . close () Clearing a Queue Description You need to connect to a remote queue and clear out all the messages it contains, optionally keeping back-up copies of the messages you remove in the local message store. Implementation Note: this page describes how to use the API to clear a queue. You can also use RabbitMQ's web management interface to purge messages from a queue, if you have the necessary access permissions. create a cdds_transfer.config.Config object to wrap your local configuration. You will need to configure Rabbit, and you may also need to configure your local directories (if you want to save copies of messages to the local message store). create a cdds_transfer.msg.Queue object to point to the remote queue create a cdds_transfer.msg.Communication object loop over all the messages returned by get_all_messages in reverse order1: invoke the remove_message method (optional) invoke the store_message method to save a copy of the message in the local message store Note: if you don't keep a back-up copy of a message that you delete, there is no way it can be recovered. If you accidentally delete a message without a back-up copy, you will need to regenerate and resend the message. 1 Messages are identified by their delivery_tag, but this is not a unique identifier for a message it is just the position in the queue, so if you attempt to remove delivery_tag 1 then 2 then 3 the change of delivery_tag following each removal will result in the removal of the first, third and fifth messages from the original queue state. Performing the removals in reverse order avoids this problem. This highlights an issue with using the delivery_tag for message identification. Example code from cdds_transfer import config , msg cfg = config . Config ([ '.cdds_credentials' ]) queue = msg . Queue ( 'moose' , 'CMIP6_available' ) comm = msg . Communication ( cfg ) for message in comm . get_all_messages ( queue )[:: - 1 ]: # reverse order comm . remove_message ( queue , message ) # comm.store_message(message) # optional Removing a queue Description A queue needs to be deleted, either because one with a new name and the same function has been created, or because the queue was created by mistake. Implementation The API does not provide a method for deleting a queue. Instead, you will either need to use the RabbitMQ web API, or use the delete_queue method provided by pika to delete the queue. Example code import os import pika from cdds_transfer import config credentials_file = os . path . expandvars ( '$HOME/.cdds_credentials' ) cfg = config . Config ([ credentials_file ]) # get rabbit connection details from the rabbit section of the # cdds credentials file rabbit_cfg = dict ( cfg . _cp . items ( 'rabbit' )) # Name of the queue to be deleted queue_name = 'queue_to_be_deleted' # Set up and open connection credentials = pika . PlainCredentials ( rabbit_cfg [ 'userid' ], rabbit_cfg [ 'password' ]) connection_parameters = pika . ConnectionParameters ( rabbit_cfg [ 'host' ], rabbit_cfg [ 'port' ], rabbit_cfg [ 'vhost' ], credentials = credentials , ssl = False ) connection = pika . BlockingConnection ( connection_parameters ) channel = connection . channel () # Delete the queue channel . queue_delete ( queue = queue_name ) # Close down the connection channel . close () connection . close ()","title":"transfer"},{"location":"cdds_components/transfer/#adding-a-queue","text":"","title":"Adding a Queue"},{"location":"cdds_components/transfer/#description","text":"We need to add a new type of queue (to the existing set of \"moose\" and \"admin\"), or an additional queue to the original set (\"moose.available\", \"moose.withdrawn\", \"admin.critical\", \"admin.info\"). We have agreed with CEDA to set up queues per project, e.g. CMIP6_available and CMIP6_withdrawn. There is also a testing set of queues to avoid functional unit tests in cdds_transfer from nuking the queues.","title":"Description"},{"location":"cdds_components/transfer/#implementation","text":"There's no method in the API to create queues directly, although invoking message methods (e.g. publishing a message) will create durable queues automatically. If you want to create the queues in advance, you can do so using pika calls.","title":"Implementation"},{"location":"cdds_components/transfer/#example-code","text":"import os import pika from cdds_transfer import config credentials_file = os . path . expandvars ( '$HOME/.cdds_credentials' ) cfg = config . Config ([ credentials_file ]) # get rabbit connection details from the rabbit section of the # cdds credentials file rabbit_cfg = dict ( cfg . _cp . items ( 'rabbit' )) # Prefix to use to construct queue names mip_era = 'CMIP6' # Name of the exchange on the rabbit server that directs # messages to queues. exchange = 'dds' credentials = pika . PlainCredentials ( rabbit_cfg [ 'userid' ], rabbit_cfg [ 'password' ]) connection_parameters = pika . ConnectionParameters ( rabbit_cfg [ 'host' ], rabbit_cfg [ 'port' ], rabbit_cfg [ 'vhost' ], credentials = credentials , ssl = False ) connection = pika . BlockingConnection ( connection_parameters ) channel = connection . channel () channel . exchange_declare ( exchange = exchange , exchange_type = \"direct\" , durable = True ) for queue in [ \"available\" , \"withdrawn\" ]: queue_name = 'moose. {} _ {} ' . format ( mip_era , queue ) channel . queue_declare ( queue = queue_name , durable = True ) channel . queue_bind ( exchange = exchange , queue = queue_name , routing_key = queue_name ) channel . close () connection . close ()","title":"Example code"},{"location":"cdds_components/transfer/#clearing-a-queue","text":"","title":"Clearing a Queue"},{"location":"cdds_components/transfer/#description_1","text":"You need to connect to a remote queue and clear out all the messages it contains, optionally keeping back-up copies of the messages you remove in the local message store.","title":"Description"},{"location":"cdds_components/transfer/#implementation_1","text":"Note: this page describes how to use the API to clear a queue. You can also use RabbitMQ's web management interface to purge messages from a queue, if you have the necessary access permissions. create a cdds_transfer.config.Config object to wrap your local configuration. You will need to configure Rabbit, and you may also need to configure your local directories (if you want to save copies of messages to the local message store). create a cdds_transfer.msg.Queue object to point to the remote queue create a cdds_transfer.msg.Communication object loop over all the messages returned by get_all_messages in reverse order1: invoke the remove_message method (optional) invoke the store_message method to save a copy of the message in the local message store Note: if you don't keep a back-up copy of a message that you delete, there is no way it can be recovered. If you accidentally delete a message without a back-up copy, you will need to regenerate and resend the message. 1 Messages are identified by their delivery_tag, but this is not a unique identifier for a message it is just the position in the queue, so if you attempt to remove delivery_tag 1 then 2 then 3 the change of delivery_tag following each removal will result in the removal of the first, third and fifth messages from the original queue state. Performing the removals in reverse order avoids this problem. This highlights an issue with using the delivery_tag for message identification.","title":"Implementation"},{"location":"cdds_components/transfer/#example-code_1","text":"from cdds_transfer import config , msg cfg = config . Config ([ '.cdds_credentials' ]) queue = msg . Queue ( 'moose' , 'CMIP6_available' ) comm = msg . Communication ( cfg ) for message in comm . get_all_messages ( queue )[:: - 1 ]: # reverse order comm . remove_message ( queue , message ) # comm.store_message(message) # optional","title":"Example code"},{"location":"cdds_components/transfer/#removing-a-queue","text":"","title":"Removing a queue"},{"location":"cdds_components/transfer/#description_2","text":"A queue needs to be deleted, either because one with a new name and the same function has been created, or because the queue was created by mistake.","title":"Description"},{"location":"cdds_components/transfer/#implementation_2","text":"The API does not provide a method for deleting a queue. Instead, you will either need to use the RabbitMQ web API, or use the delete_queue method provided by pika to delete the queue.","title":"Implementation"},{"location":"cdds_components/transfer/#example-code_2","text":"import os import pika from cdds_transfer import config credentials_file = os . path . expandvars ( '$HOME/.cdds_credentials' ) cfg = config . Config ([ credentials_file ]) # get rabbit connection details from the rabbit section of the # cdds credentials file rabbit_cfg = dict ( cfg . _cp . items ( 'rabbit' )) # Name of the queue to be deleted queue_name = 'queue_to_be_deleted' # Set up and open connection credentials = pika . PlainCredentials ( rabbit_cfg [ 'userid' ], rabbit_cfg [ 'password' ]) connection_parameters = pika . ConnectionParameters ( rabbit_cfg [ 'host' ], rabbit_cfg [ 'port' ], rabbit_cfg [ 'vhost' ], credentials = credentials , ssl = False ) connection = pika . BlockingConnection ( connection_parameters ) channel = connection . channel () # Delete the queue channel . queue_delete ( queue = queue_name ) # Close down the connection channel . close () connection . close ()","title":"Example code"},{"location":"developer_documentation/","text":"Warning This documentation is currently under construction and may not be up to date. Welcome to the cdds developer documentation. Contributing to CDDS The development work for CDDS is managed via JIRA","title":"Developer Documentation"},{"location":"developer_documentation/#contributing-to-cdds","text":"The development work for CDDS is managed via JIRA","title":"Contributing to CDDS"},{"location":"developer_documentation/building_documentation/","text":"Warning This documentation is currently under construction and may not be up to date. Modifying and Building the CDDS Documentation This page gives an overview of the general philosophy of the CDDS documentation. The current documentation is based upon mkdocs and is built and managed using two main packages. mkdocs-material - A particular mkdocs theme which also extends the base functionality. mike - A tool for managing multiple independent versions of documentation on a particular branch. For most purposes it is only neccessary to reference the mkdocs-material package documentation and the mike documentation. Working with mkdocs The source files are written in markdown, and are kept in the docs directory of the repository. Configuration of the building of mkdocs is done using the mkdocs.yml file. Whilst editing or adding documentation you can preview changes in realtime using by starting a local server. mkdocs serve To generate the actual site that can be uploaded to a web server you would use. mkdocs build However, it should not be needed to run this command directly in order to build and deploy the docs. This is done using the mike package (see next section). Working with mike It is worth familiarising with the overview of mike here. In short though, mike makes it relatively straightforward to manage multiple versions of docs by running the mkdocs build command and automatically commiting this to the gh-pages branch. mike deploy 3.0 Similarly to mkdocs serve , you can use the following command to start a local server to preview the docs. mike serve However, rather than displaying the docs in your CWD , this will serve the docs from the gh-pages branch. Managing Versions and Deployment in Practice Deploying the documentation in practice","title":"Documentation"},{"location":"developer_documentation/building_documentation/#modifying-and-building-the-cdds-documentation","text":"This page gives an overview of the general philosophy of the CDDS documentation. The current documentation is based upon mkdocs and is built and managed using two main packages. mkdocs-material - A particular mkdocs theme which also extends the base functionality. mike - A tool for managing multiple independent versions of documentation on a particular branch. For most purposes it is only neccessary to reference the mkdocs-material package documentation and the mike documentation.","title":"Modifying and Building the CDDS Documentation"},{"location":"developer_documentation/building_documentation/#working-with-mkdocs","text":"The source files are written in markdown, and are kept in the docs directory of the repository. Configuration of the building of mkdocs is done using the mkdocs.yml file. Whilst editing or adding documentation you can preview changes in realtime using by starting a local server. mkdocs serve To generate the actual site that can be uploaded to a web server you would use. mkdocs build However, it should not be needed to run this command directly in order to build and deploy the docs. This is done using the mike package (see next section).","title":"Working with mkdocs"},{"location":"developer_documentation/building_documentation/#working-with-mike","text":"It is worth familiarising with the overview of mike here. In short though, mike makes it relatively straightforward to manage multiple versions of docs by running the mkdocs build command and automatically commiting this to the gh-pages branch. mike deploy 3.0 Similarly to mkdocs serve , you can use the following command to start a local server to preview the docs. mike serve However, rather than displaying the docs in your CWD , this will serve the docs from the gh-pages branch.","title":"Working with mike"},{"location":"developer_documentation/building_documentation/#managing-versions-and-deployment-in-practice","text":"Deploying the documentation in practice","title":"Managing Versions and Deployment in Practice"},{"location":"developer_documentation/coding_guidelines/","text":"Warning This documentation is currently under construction and may not be up to date. Python Code Style Guide Use the Style Guide for Python Code Exception - Limit all lines to a maximum of 120 characters for docstrings and comments. (79 in the the PEP 8 guidelines reference ). All CDDS packages should contain a package_name/package_name/tests/test_coding_standards.py module that uses the pycodestyle package to check PEP 8 conformance. In addition to this, it is recommended to use a tool that checks for errors in Python code, coding standard (e.g., PEP 8) conformance and general code quality e.g., pylint . However, some PEP 8 guidelines are not checked by these tools; please also: Be consistent within a module or function reference . In Python, single-quoted strings and double-quoted strings are the same; pick a rule and stick to it reference . Use Python's implied line continuation inside parentheses, brackets and braces to wrap long lines, rather than using a backslash reference . Add a new line before a binary operator, rather than after reference . Use blank lines in functions, sparingly, to indicate logical sections reference . Always use double quote characters for triple-quoted strings \"\"\" to be consistent with the docstring conventions in PEP 257 and PEP 8. Write comments using complete sentences reference . Use the https://docs.python.org/3.8/library/stdtypes.html#str.format str.format() method to perform a string formatting operation, e.g., 'Coordinates: {latitude}, {longitude}'.format(latitude='37.24N', longitude='-115.81W') , since it is the new standard in Python 3 and is preferred to the % formatting. Include the line (c) British Crown Copyright [year of creation in the form <YYYY>]-[year of last modification in the form <YYYY>], Met Office. as a comment at the top of every module. Naming Conventions Abstract Base Classes should have a name that contains Abstract for clarity. Nouns should be used when naming classes. Use descriptive names that clearly convey a meaning; refrain from using overly general / ambiguous names e.g., data. Use the .cfg extension for configuration files, e.g. those read by configparser. Imports Use absolute imports https://www.python.org/dev/peps/pep-0328/#rationale-for-absolute-imports as they are recommended by PEP 8. Also, being able to tell immediately where the function comes from greatly improves code readability and comprehension (Readability Counts). Example: import my_package.my_subpackage.my_module and use the my_module.my_function syntax, e.g., import os; os.walk. from my_package.my_subpackage.my_module import my_function and use my_function directly. group imports, with a blank line between each group, in the following order: standard library imports, related third party imports, local application/library specific imports [reference]. Within each import group (see above), order the imports alphabetically. place module level \"dunders\" after the module docstring but before any import statements except from __future__ imports [reference]. Use the pattern import numpy as np Typing Use https://docs.python.org/3/library/typing.html for adding type hints and annotations. Use mypy for running static code analysis using aforementioned type hints. Docstrings Warning Historically CDDS used the NumpyDoc format for all docstrings but as of 2022 the PEP-257 format was adopted. This means there is currently a mix of docstring formats used throughout the CDDS code. Docstrings should now be written using https://docutils.sourceforge.io/rst.html as recommended by https://www.python.org/dev/peps/pep-0287/, and is rendered using Sphinx. An detailed example of the reStructuredText style docstrings can be found here https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html Use double backticks `` around argument names so that they are rendered as code in the HTML produced by Sphinx Use the appropriate substitutions for glossary terms. Make use of the docstring conventions http://www.python.org/dev/peps/pep-0257. The docstring is a phrase ending in a period and prescribes the function or method's effect as a command, not as a description [reference]. It is not necessary to write docstrings for non-public classes, methods and functions, see cdds/pylintrc and mip_convert/pylintrc. (The maintenance overhead is reduced when refactoring non-public classes, methods and functions). Below is an example reStructuredText Docstring Example docstring incorporating all of the guidelines above. def my_function ( my_param1 : float , my_param2 : str ) -> int : \"\"\" Return something. Here's a longer description about the something that is returned. It's so long, it goes over one line! :param my_param1: Description of the first parameter ``my_param1``. :type my_param1: float :param my_param2: Description of the second parameter ``my_param2``. :type my_param2: string :raises ValueError If ``my_param1`` is less than 0. :return: Description of anonymous integer return value. :rtype: int \"\"\" Doctests Where appropriate, a https://docs.python.org/3.8/library/doctest.html should be included in an Examples section of the docstring https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard. When multiple examples are provided, they should be separated by blank lines. Comments explaining the examples should have blank lines both above and below them. Entry Point Scripts Script names should not have an extension, should be lowercase, and with words separated by underscores as necessary to improve readability, e.g., just_do_it. It is recommended that scripts call a main() function located in an importable module so that it is possible to run the code in the script from the Python interpreter / a test module e.g., import my_module; my_module.main(). https://docs.python.org/3.8/library/argparse.html should be used to parse command line options. def main ( args ): # Parse the arguments. args = parse_args ( args ) # Create the configured logger. configure_logger ( args . log_name , args . log_level , args . append_log ) # Retrieve the logger. logger = logging . getLogger ( __name__ ) try : exit_code = my_func ( args ) except BaseException as exc : exit_code = 1 logger . exception ( exc ) return exit_code Bash Scripts Use the Google Styleguide for bash scripts as recommended by the Cylc documentation Run scripts through Shellcheck for catching possible bad practice. (The latest version can be installed using conda for easier access and improvements over the centrally installed version)","title":"Coding Guidelines"},{"location":"developer_documentation/coding_guidelines/#python-code","text":"","title":"Python Code"},{"location":"developer_documentation/coding_guidelines/#style-guide","text":"Use the Style Guide for Python Code Exception - Limit all lines to a maximum of 120 characters for docstrings and comments. (79 in the the PEP 8 guidelines reference ). All CDDS packages should contain a package_name/package_name/tests/test_coding_standards.py module that uses the pycodestyle package to check PEP 8 conformance. In addition to this, it is recommended to use a tool that checks for errors in Python code, coding standard (e.g., PEP 8) conformance and general code quality e.g., pylint . However, some PEP 8 guidelines are not checked by these tools; please also: Be consistent within a module or function reference . In Python, single-quoted strings and double-quoted strings are the same; pick a rule and stick to it reference . Use Python's implied line continuation inside parentheses, brackets and braces to wrap long lines, rather than using a backslash reference . Add a new line before a binary operator, rather than after reference . Use blank lines in functions, sparingly, to indicate logical sections reference . Always use double quote characters for triple-quoted strings \"\"\" to be consistent with the docstring conventions in PEP 257 and PEP 8. Write comments using complete sentences reference . Use the https://docs.python.org/3.8/library/stdtypes.html#str.format str.format() method to perform a string formatting operation, e.g., 'Coordinates: {latitude}, {longitude}'.format(latitude='37.24N', longitude='-115.81W') , since it is the new standard in Python 3 and is preferred to the % formatting. Include the line (c) British Crown Copyright [year of creation in the form <YYYY>]-[year of last modification in the form <YYYY>], Met Office. as a comment at the top of every module.","title":"Style Guide"},{"location":"developer_documentation/coding_guidelines/#naming-conventions","text":"Abstract Base Classes should have a name that contains Abstract for clarity. Nouns should be used when naming classes. Use descriptive names that clearly convey a meaning; refrain from using overly general / ambiguous names e.g., data. Use the .cfg extension for configuration files, e.g. those read by configparser.","title":"Naming Conventions"},{"location":"developer_documentation/coding_guidelines/#imports","text":"Use absolute imports https://www.python.org/dev/peps/pep-0328/#rationale-for-absolute-imports as they are recommended by PEP 8. Also, being able to tell immediately where the function comes from greatly improves code readability and comprehension (Readability Counts). Example: import my_package.my_subpackage.my_module and use the my_module.my_function syntax, e.g., import os; os.walk. from my_package.my_subpackage.my_module import my_function and use my_function directly. group imports, with a blank line between each group, in the following order: standard library imports, related third party imports, local application/library specific imports [reference]. Within each import group (see above), order the imports alphabetically. place module level \"dunders\" after the module docstring but before any import statements except from __future__ imports [reference]. Use the pattern import numpy as np","title":"Imports"},{"location":"developer_documentation/coding_guidelines/#typing","text":"Use https://docs.python.org/3/library/typing.html for adding type hints and annotations. Use mypy for running static code analysis using aforementioned type hints.","title":"Typing"},{"location":"developer_documentation/coding_guidelines/#docstrings","text":"Warning Historically CDDS used the NumpyDoc format for all docstrings but as of 2022 the PEP-257 format was adopted. This means there is currently a mix of docstring formats used throughout the CDDS code. Docstrings should now be written using https://docutils.sourceforge.io/rst.html as recommended by https://www.python.org/dev/peps/pep-0287/, and is rendered using Sphinx. An detailed example of the reStructuredText style docstrings can be found here https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html Use double backticks `` around argument names so that they are rendered as code in the HTML produced by Sphinx Use the appropriate substitutions for glossary terms. Make use of the docstring conventions http://www.python.org/dev/peps/pep-0257. The docstring is a phrase ending in a period and prescribes the function or method's effect as a command, not as a description [reference]. It is not necessary to write docstrings for non-public classes, methods and functions, see cdds/pylintrc and mip_convert/pylintrc. (The maintenance overhead is reduced when refactoring non-public classes, methods and functions). Below is an example reStructuredText Docstring Example docstring incorporating all of the guidelines above. def my_function ( my_param1 : float , my_param2 : str ) -> int : \"\"\" Return something. Here's a longer description about the something that is returned. It's so long, it goes over one line! :param my_param1: Description of the first parameter ``my_param1``. :type my_param1: float :param my_param2: Description of the second parameter ``my_param2``. :type my_param2: string :raises ValueError If ``my_param1`` is less than 0. :return: Description of anonymous integer return value. :rtype: int \"\"\"","title":"Docstrings"},{"location":"developer_documentation/coding_guidelines/#doctests","text":"Where appropriate, a https://docs.python.org/3.8/library/doctest.html should be included in an Examples section of the docstring https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard. When multiple examples are provided, they should be separated by blank lines. Comments explaining the examples should have blank lines both above and below them.","title":"Doctests"},{"location":"developer_documentation/coding_guidelines/#entry-point-scripts","text":"Script names should not have an extension, should be lowercase, and with words separated by underscores as necessary to improve readability, e.g., just_do_it. It is recommended that scripts call a main() function located in an importable module so that it is possible to run the code in the script from the Python interpreter / a test module e.g., import my_module; my_module.main(). https://docs.python.org/3.8/library/argparse.html should be used to parse command line options. def main ( args ): # Parse the arguments. args = parse_args ( args ) # Create the configured logger. configure_logger ( args . log_name , args . log_level , args . append_log ) # Retrieve the logger. logger = logging . getLogger ( __name__ ) try : exit_code = my_func ( args ) except BaseException as exc : exit_code = 1 logger . exception ( exc ) return exit_code","title":"Entry Point Scripts"},{"location":"developer_documentation/coding_guidelines/#bash-scripts","text":"Use the Google Styleguide for bash scripts as recommended by the Cylc documentation Run scripts through Shellcheck for catching possible bad practice. (The latest version can be installed using conda for easier access and improvements over the centrally installed version)","title":"Bash Scripts"},{"location":"developer_documentation/development_practices/","text":"Warning This documentation is currently under construction and may not be up to date. Quickstart","title":"Development Practices"},{"location":"developer_documentation/development_practices/#quickstart","text":"","title":"Quickstart"},{"location":"developer_documentation/development_workflow/","text":"Warning This documentation is currently under construction and may not be up to date. Create a Jira Issue Each change should be described in a Jira issue. Before submitting an issue, make sure that the issue explains the purpose of the changes and what changes should be done. New issues should be created in the backlog and marked as . The team will look each week at the new created issues. A issue will be put into a sprint if everyone of the team understand what to do for the issue and all information are provided to proceed the issue. Before creating a new issue, look through the backlog and current sprint if issue is not already reported. Start Working on an Issue Assign the issue to you Put the issue into In progress Create a new branch for the issue Put the branch name on the Jira issue Create a New Branch Create a issue branch The branch name should be start with the issue number and has a brief description of the task. Use underscores in the branch name to separate words. git checkout -b <issue_number>_<brief_description> The commit message should be: <issue_number>: Create a new branch for <brief_description> Don\u2019t wrap the line of the commit message and don\u2019t use more than 72 characters. If you need a longer commit message. Leave one empty line after the first line and then give a more detailed description (now you can wrap this description). After creating the branch, your working copy is already on this branch. You do not need to run an extra switch command like when using SVN. Create a release branch The branch name should start with release and should have the release number in it. For example, you like to create a release branch for all work for a release 2.2.x , the release_2.2.x . git checkout -b release_<release_number> The commit message should contain the release number in it for example: Create branch for release Don\u2019t wrap the line of the commit message and don\u2019t use more than 72 characters. If you need a longer commit message. Leave one empty line after the first line and then give a more detailed description (now you can wrap this description). After creating the branch, your working copy is already on this branch. You do not need to run an extra switch command like when using SVN. Create a branch from a release branch Switch to the release branch, you want to branch from Then, create a new branch like described at Create-a-issue-branch Update Jira issue Add the branch name to your Jira issue Make Changes Even if you are working on a Jira issue, keep an eye on the issue to notice any new comments on the issues. Code Modify the code on the \u201cissue\u201c branch Regularly, commit your changes locally and remotely using: The commit messages should always start with the issue number and have a short description what changed. Be aware, that the requirements for commit message are: Not more than 72 characters per line The first line should start with the issue number and have a really short brief description If more descriptions are necessary, leave an empty line and then write more text, e.g. CDDSO-109 Merge fixing duplicate coordinates error from cmor * Add functional tests for ARISE model * Simplify mean cube calculation After the first commit: Do your additional changes and then use: git commit --amend That combine latest and the current commit to one commit. Using this one allows use to have only one commit per issue even if you \u201cactual\u201c did multiple commits. A push can be done at any time! (You can still do a git commit --amend after a push, because it always use the latest commit command!) Remote commit: git push origin To be more explicit, you can add the branch name you want to commit to: git push origin <branch_name_to_commit_to> Tests Make sure that unit tests covers your code changes, see Unit Tests If there were changes in CDDS Transfer, also run integration tests with RabbitMQ, see CDDS Transfer Tests With RabbitMQ - Run the unit tests regularly It is a good approach to ensure that every time you push a commit to the repository that the tests pass successfully. Documentation Document your code. Try to be short but precise. Briefly describe the work on the Jira issue Reviews For coding reviews, we use the pull requests functionality of Github Go to Github and the pull requests for the cdds-project: https://github.com/MetOffice/cdds/pulls Click on New pull request (green box on the right) Choose as base branch the branch you branched from (often: main) Choose as compare branch your branch Then, click on Create pull request Write into the message box what you did and other useful information for the reviewers Add to your issue the URL link to the pull request for the reviewers Put your issue into Review Assign it to the main reviewer Mention in the team channel that the issue is now ready to review. So, everyone has the change to review your changes. You need at least an approved of the main review before merging the issue If you need to do any changes, assign the issue back to you and put it back into In progress. If the main reviewer is happy with the changes and no comments or issues are needed to addressed anymore, the issue will moved to Approved and re-assigned to you. Now, you can merge your changes Merge changes You have two possibilities how to merge a branch - via command line or using Github. Merge to parent branch Via Command Line Go to the parent branch to merge to (normally: main or the release branch) git checkout <branch_to_merge_to> Make sure that the branch is up-to-date: git pull Merge the issue branch: git merge <your_branch_to_merge> As commit message for a merge use: <issue-number>: Completed work to <short_brief_description_of_task> You could face merging conflicts that you need to solve before you can end the merge. Using Github Go to the Pull Requests view of your branch in Github If your review is approved, there is a green Merge Branch button at the bottom of the page. Click on it. If the this button does not appear or is not clickable, check if your review is really approved and if there is no conflicts you need to solved first. For solving conflicts, Github provides you help with a UI tool remove branch if working on main Cherry pick changes on main branch (or any other branches) If your parent branch is a release branch, you also need to merge the changes into the main branch. Therefore, cherry picking is the best to do. You can do this after merging your changes into the release branch. Cherry picking into other branches are following the same pattern! Via Command Line Find the commit id of the merge commit into the parent branch, you have done in the previous step: Go to the parent branch git checkout <parent-branch> Find your commit ID: git log This list all the previous commits in decreasing order. Each merge entry looks like: commit <commit-id> (<merge-branches>) Merge: <merge-ids> Author: <author-details> Date: <timestamp> <commit-message> Use the <commit-id> of your merge commit. Remember your ! You will need it for the next steps. Do the cherry pick: Go to the branch you want to cherry-pick to: git checkout <branch-to-cerry-pick-to> Cherry pick: git cherry-pick <commit-id> --no-commit This will add the commit\u2019s changes to your working copy without directly committing them. You need to do a git commit by yourself. Commit the changes and check if everything is fine: Check the changes. If they are fine, commit them: git commit Use the key-word cherry-pick in your commit message to make clear that this commit is a cherry-pick. Check if all (unit) tests succeed! Push the changes Using Github For using cherry-picking, see Cherry-picking a commit in GitHub Desktop - GitHub Docs Close the Jira issue Move the Jira issue to Done","title":"Development Workflow"},{"location":"developer_documentation/development_workflow/#create-a-jira-issue","text":"Each change should be described in a Jira issue. Before submitting an issue, make sure that the issue explains the purpose of the changes and what changes should be done. New issues should be created in the backlog and marked as . The team will look each week at the new created issues. A issue will be put into a sprint if everyone of the team understand what to do for the issue and all information are provided to proceed the issue. Before creating a new issue, look through the backlog and current sprint if issue is not already reported.","title":"Create a Jira Issue"},{"location":"developer_documentation/development_workflow/#start-working-on-an-issue","text":"Assign the issue to you Put the issue into In progress Create a new branch for the issue Put the branch name on the Jira issue","title":"Start Working on an Issue"},{"location":"developer_documentation/development_workflow/#create-a-new-branch","text":"","title":"Create a New Branch"},{"location":"developer_documentation/development_workflow/#create-a-issue-branch","text":"The branch name should be start with the issue number and has a brief description of the task. Use underscores in the branch name to separate words. git checkout -b <issue_number>_<brief_description> The commit message should be: <issue_number>: Create a new branch for <brief_description> Don\u2019t wrap the line of the commit message and don\u2019t use more than 72 characters. If you need a longer commit message. Leave one empty line after the first line and then give a more detailed description (now you can wrap this description). After creating the branch, your working copy is already on this branch. You do not need to run an extra switch command like when using SVN.","title":"Create a issue branch"},{"location":"developer_documentation/development_workflow/#create-a-release-branch","text":"The branch name should start with release and should have the release number in it. For example, you like to create a release branch for all work for a release 2.2.x , the release_2.2.x . git checkout -b release_<release_number> The commit message should contain the release number in it for example: Create branch for release Don\u2019t wrap the line of the commit message and don\u2019t use more than 72 characters. If you need a longer commit message. Leave one empty line after the first line and then give a more detailed description (now you can wrap this description). After creating the branch, your working copy is already on this branch. You do not need to run an extra switch command like when using SVN. Create a branch from a release branch Switch to the release branch, you want to branch from Then, create a new branch like described at Create-a-issue-branch Update Jira issue Add the branch name to your Jira issue","title":"Create a release branch"},{"location":"developer_documentation/development_workflow/#make-changes","text":"Even if you are working on a Jira issue, keep an eye on the issue to notice any new comments on the issues.","title":"Make Changes"},{"location":"developer_documentation/development_workflow/#code","text":"Modify the code on the \u201cissue\u201c branch Regularly, commit your changes locally and remotely using: The commit messages should always start with the issue number and have a short description what changed. Be aware, that the requirements for commit message are: Not more than 72 characters per line The first line should start with the issue number and have a really short brief description If more descriptions are necessary, leave an empty line and then write more text, e.g. CDDSO-109 Merge fixing duplicate coordinates error from cmor * Add functional tests for ARISE model * Simplify mean cube calculation After the first commit: Do your additional changes and then use: git commit --amend That combine latest and the current commit to one commit. Using this one allows use to have only one commit per issue even if you \u201cactual\u201c did multiple commits. A push can be done at any time! (You can still do a git commit --amend after a push, because it always use the latest commit command!) Remote commit: git push origin To be more explicit, you can add the branch name you want to commit to: git push origin <branch_name_to_commit_to>","title":"Code"},{"location":"developer_documentation/development_workflow/#tests","text":"Make sure that unit tests covers your code changes, see Unit Tests If there were changes in CDDS Transfer, also run integration tests with RabbitMQ, see CDDS Transfer Tests With RabbitMQ - Run the unit tests regularly It is a good approach to ensure that every time you push a commit to the repository that the tests pass successfully.","title":"Tests"},{"location":"developer_documentation/development_workflow/#documentation","text":"Document your code. Try to be short but precise. Briefly describe the work on the Jira issue","title":"Documentation"},{"location":"developer_documentation/development_workflow/#reviews","text":"For coding reviews, we use the pull requests functionality of Github Go to Github and the pull requests for the cdds-project: https://github.com/MetOffice/cdds/pulls Click on New pull request (green box on the right) Choose as base branch the branch you branched from (often: main) Choose as compare branch your branch Then, click on Create pull request Write into the message box what you did and other useful information for the reviewers Add to your issue the URL link to the pull request for the reviewers Put your issue into Review Assign it to the main reviewer Mention in the team channel that the issue is now ready to review. So, everyone has the change to review your changes. You need at least an approved of the main review before merging the issue If you need to do any changes, assign the issue back to you and put it back into In progress. If the main reviewer is happy with the changes and no comments or issues are needed to addressed anymore, the issue will moved to Approved and re-assigned to you. Now, you can merge your changes","title":"Reviews"},{"location":"developer_documentation/development_workflow/#merge-changes","text":"You have two possibilities how to merge a branch - via command line or using Github.","title":"Merge changes"},{"location":"developer_documentation/development_workflow/#merge-to-parent-branch","text":"","title":"Merge to parent branch"},{"location":"developer_documentation/development_workflow/#via-command-line","text":"Go to the parent branch to merge to (normally: main or the release branch) git checkout <branch_to_merge_to> Make sure that the branch is up-to-date: git pull Merge the issue branch: git merge <your_branch_to_merge> As commit message for a merge use: <issue-number>: Completed work to <short_brief_description_of_task> You could face merging conflicts that you need to solve before you can end the merge.","title":"Via Command Line"},{"location":"developer_documentation/development_workflow/#using-github","text":"Go to the Pull Requests view of your branch in Github If your review is approved, there is a green Merge Branch button at the bottom of the page. Click on it. If the this button does not appear or is not clickable, check if your review is really approved and if there is no conflicts you need to solved first. For solving conflicts, Github provides you help with a UI tool remove branch if working on main","title":"Using Github"},{"location":"developer_documentation/development_workflow/#cherry-pick-changes-on-main-branch-or-any-other-branches","text":"If your parent branch is a release branch, you also need to merge the changes into the main branch. Therefore, cherry picking is the best to do. You can do this after merging your changes into the release branch. Cherry picking into other branches are following the same pattern!","title":"Cherry pick changes on main branch (or any other branches)"},{"location":"developer_documentation/development_workflow/#via-command-line_1","text":"Find the commit id of the merge commit into the parent branch, you have done in the previous step: Go to the parent branch git checkout <parent-branch> Find your commit ID: git log This list all the previous commits in decreasing order. Each merge entry looks like: commit <commit-id> (<merge-branches>) Merge: <merge-ids> Author: <author-details> Date: <timestamp> <commit-message> Use the <commit-id> of your merge commit. Remember your ! You will need it for the next steps. Do the cherry pick: Go to the branch you want to cherry-pick to: git checkout <branch-to-cerry-pick-to> Cherry pick: git cherry-pick <commit-id> --no-commit This will add the commit\u2019s changes to your working copy without directly committing them. You need to do a git commit by yourself. Commit the changes and check if everything is fine: Check the changes. If they are fine, commit them: git commit Use the key-word cherry-pick in your commit message to make clear that this commit is a cherry-pick. Check if all (unit) tests succeed! Push the changes","title":"Via Command Line"},{"location":"developer_documentation/development_workflow/#using-github_1","text":"For using cherry-picking, see Cherry-picking a commit in GitHub Desktop - GitHub Docs","title":"Using Github"},{"location":"developer_documentation/development_workflow/#close-the-jira-issue","text":"Move the Jira issue to Done","title":"Close the Jira issue"},{"location":"developer_documentation/github/","text":"Warning This documentation is currently under construction and may not be up to date. Github Basics The develop branch is called main . When you start working on the project, first of all you need to get a working copy on your developer environment. To do this, clone the CDDS git repository from Github. git clone git@github.com:MetOffice/CDDS.git Setup up SSH For the cdds-inline-testing project, we use ssh. You can import the ssh key into seahorse. That prevents you to type in your git credentials each time when you use a git command. Therefore, you need to use the SSH repository URL: git clone git@github.com:<permission-group>/<repository-name>.git For the CDDS project, we use ssh. You can import the ssh key into seahorse. That prevents you to type in your git credentials each time when you use a git command. Using SSH Key Generate a SSH key for Github or use an already existing key. You can do it via seahorse or on the command line. Generate SSH key for Github using Seahorse: Start seahorse Click on File in the menu and choose New A creation window should pop up. In that, choose Secure Shell Key. Then click on continue Now, you need to add a description of your key, for example \u201cGithub\u201d. Click on Just Create Key It will ask you for a password. You can also use an empty password but it is highly recommended to use one. Click at OK. You need to confirm your chosen password again. After confirmation, the key will be created. You should see the key in the Open SSH keys tab. Now, open a terminal. You can find the public key that you need for Github in your home folder: cd ~/.ssh Your new created public key is `id_rsa. .pub`` file, where is the highest number of the id_rsa files. To get the key phrase that is needed for Github, simply open the public key file with an editor of your choice, for example with vi: vi id_rsa.<number>.pub The file content should start with ssh-rsa . Add your SSH key to Github Login to Github and go to your account settings Go to the SSH and GPG keys tab Click on the green Add SSH key button Copy your public key phrase of your SSH key into the text box Click Add SSH key A confirm password box should be shown. Enter your Github password (not the SSH key password!) and then the key is added. You will get a email confirming that a new key has been added to your Github account. Clone working copy of cdds-testing-project Use following command to clone the cdds-testing-project: git clone git@github.com:MetOffice/CDDS.git Git Basics View status of your working copy git status This command shows you the status of your working copy: On which branch you are What files are not committed but modified, added or removed If files have any conflicts during a merge, switch, etc. Get latest changes You need to pull the latest changes from upstream by hand. Therefor, use following command: git pull This gets the latest changes from the remote repository for the branch you are currently in. Switching between branches git checkout <branch_name> Make sure that you stash or commit any uncommited local changes first otherwise it could be that you get some awful conflicts. Create a new branch Each ticket should be developed in a new \u201cfeature\u201c branch. Also, bug releases etc. should have their own branches. Switch to the branch you want to branch of. This branch is called parent branch: git checkout <parent_branch_name> Then, pull the changes from upstream. Your parent branch needs to be up to date: git pull Create the branch on your local machine and switch in this branch: git checkout -b <name_of_the_new_branch> Push the branch on github: git push origin <name_of_the_new_branch> You can check if your branch is created remotely: git branch -a This command should list your new created branch. Commit and Push Changes: After you have made your changes you first need to commit them to your local repository afterwards you must push them to the remote repository. Commit Changes If you want to commit your changes locally, run: git commit It opens a vim or gvim, where you can specify your commit message. For the requirements of the commit messages see \u201cCommit Messages\u201c section. If you want to have one commit for each ticket then use for the first commit git commit and for all following commits `git comment --amend``. Push Changes Before pushing any changes you first need to commit them locally! If you want to push your changes, you must tell git on which branch to push them: git push To be absolutely sure, that you push on the right branch, you can add the branch name to the git command: git push origin <name_of_the_branch> Commit Messages Template <ticket-number>: <short ticket description not longer than 72 characters> <some additional description wrap after 72 charachters> Rules Git does not wrap commit messages automatically. That makes it hard to see the whole message in a terminal. So following rules will apply for commit messages: The first line of the commit starts with the ticket number, after that there will be a colon and followed by a short description what the ticket is for: The message should not be longer than 72 characters! Do not wrap this first line If you need more descriptions: Leave one line empty after the first main message Write more descriptions. For better reading try not to write more than 72 characters per line Here, you can wrap the lines Try to be short and precise Viewing history of commits Seeing all full commit messages decreasing history git log Seeing first line of the commit messages decreasing history git log --oneline Viewing branches Seeing the information on the branch you are on: git branch List all child branches git branch --list Reset changes Reset all local changes git reset --hard This discard all local changes to all files permanently. Reset changes of a specific file git reset --hard HEAD <file> HEAD is your current branch. Reset changes of last n commits: git reset --hard HEAD~<number-of-changes> The HEAD is your current branch. Reset to the last commit: git reset --hard HEAD~1 Reset to the previous last commit: git reset --hard HEAD~2 Revert changes Undo changes of a specific commit: Find the commit number: git log --online The commit number is the number before each commit message Revert changes to the commit number: git revert <commit-number> The git revert will undo the given commit but will create a new commit without deleting the older one. The git revert command will not touch the commits in your history, even the reverted ones. Stash changes You can temporarily stashes changes you have made to your working copy. You can work on something else and come back late and re-apply them. It helps you when you need to quickly switch context and work on something else on your local working copy. You can switches between branches your stashed changes will be still available. Stashing your work git stash That stashes your local (uncommited) changes, save them for later use and revert them from your working copy. Re-applying your stashed changes git stash pop This removes the changes from your stash and re-applies them to your working copy. If you do not want that you changes will be removed from the stash use: git stash apply The stash apply command is really helpful if you need to apply the changes on multiple branches. Delete Branches This should be only a well-considered option. You can delete a branch locally or remotely: Delete the branch locally: git branch -D <name_of_branch_to_delete> This deletion command force to delete the branch even if it is un-merged. Delete the branch on github: git push origin --delete <name_of_branch_to_delete>","title":"Working with Github and Git"},{"location":"developer_documentation/github/#github-basics","text":"The develop branch is called main . When you start working on the project, first of all you need to get a working copy on your developer environment. To do this, clone the CDDS git repository from Github. git clone git@github.com:MetOffice/CDDS.git","title":"Github Basics"},{"location":"developer_documentation/github/#setup-up-ssh","text":"For the cdds-inline-testing project, we use ssh. You can import the ssh key into seahorse. That prevents you to type in your git credentials each time when you use a git command. Therefore, you need to use the SSH repository URL: git clone git@github.com:<permission-group>/<repository-name>.git For the CDDS project, we use ssh. You can import the ssh key into seahorse. That prevents you to type in your git credentials each time when you use a git command. Using SSH Key Generate a SSH key for Github or use an already existing key. You can do it via seahorse or on the command line.","title":"Setup up SSH"},{"location":"developer_documentation/github/#generate-ssh-key-for-github-using-seahorse","text":"Start seahorse Click on File in the menu and choose New A creation window should pop up. In that, choose Secure Shell Key. Then click on continue Now, you need to add a description of your key, for example \u201cGithub\u201d. Click on Just Create Key It will ask you for a password. You can also use an empty password but it is highly recommended to use one. Click at OK. You need to confirm your chosen password again. After confirmation, the key will be created. You should see the key in the Open SSH keys tab. Now, open a terminal. You can find the public key that you need for Github in your home folder: cd ~/.ssh Your new created public key is `id_rsa. .pub`` file, where is the highest number of the id_rsa files. To get the key phrase that is needed for Github, simply open the public key file with an editor of your choice, for example with vi: vi id_rsa.<number>.pub The file content should start with ssh-rsa .","title":"Generate SSH key for Github using Seahorse:"},{"location":"developer_documentation/github/#add-your-ssh-key-to-github","text":"Login to Github and go to your account settings Go to the SSH and GPG keys tab Click on the green Add SSH key button Copy your public key phrase of your SSH key into the text box Click Add SSH key A confirm password box should be shown. Enter your Github password (not the SSH key password!) and then the key is added. You will get a email confirming that a new key has been added to your Github account. Clone working copy of cdds-testing-project Use following command to clone the cdds-testing-project: git clone git@github.com:MetOffice/CDDS.git","title":"Add your SSH key to Github"},{"location":"developer_documentation/github/#git-basics","text":"View status of your working copy git status This command shows you the status of your working copy: On which branch you are What files are not committed but modified, added or removed If files have any conflicts during a merge, switch, etc.","title":"Git Basics"},{"location":"developer_documentation/github/#get-latest-changes","text":"You need to pull the latest changes from upstream by hand. Therefor, use following command: git pull This gets the latest changes from the remote repository for the branch you are currently in.","title":"Get latest changes"},{"location":"developer_documentation/github/#switching-between-branches","text":"git checkout <branch_name> Make sure that you stash or commit any uncommited local changes first otherwise it could be that you get some awful conflicts.","title":"Switching between branches"},{"location":"developer_documentation/github/#create-a-new-branch","text":"Each ticket should be developed in a new \u201cfeature\u201c branch. Also, bug releases etc. should have their own branches. Switch to the branch you want to branch of. This branch is called parent branch: git checkout <parent_branch_name> Then, pull the changes from upstream. Your parent branch needs to be up to date: git pull Create the branch on your local machine and switch in this branch: git checkout -b <name_of_the_new_branch> Push the branch on github: git push origin <name_of_the_new_branch> You can check if your branch is created remotely: git branch -a This command should list your new created branch.","title":"Create a new branch"},{"location":"developer_documentation/github/#commit-and-push-changes","text":"After you have made your changes you first need to commit them to your local repository afterwards you must push them to the remote repository.","title":"Commit and Push Changes:"},{"location":"developer_documentation/github/#commit-changes","text":"If you want to commit your changes locally, run: git commit It opens a vim or gvim, where you can specify your commit message. For the requirements of the commit messages see \u201cCommit Messages\u201c section. If you want to have one commit for each ticket then use for the first commit git commit and for all following commits `git comment --amend``.","title":"Commit Changes"},{"location":"developer_documentation/github/#push-changes","text":"Before pushing any changes you first need to commit them locally! If you want to push your changes, you must tell git on which branch to push them: git push To be absolutely sure, that you push on the right branch, you can add the branch name to the git command: git push origin <name_of_the_branch>","title":"Push Changes"},{"location":"developer_documentation/github/#commit-messages","text":"","title":"Commit Messages"},{"location":"developer_documentation/github/#template","text":"<ticket-number>: <short ticket description not longer than 72 characters> <some additional description wrap after 72 charachters>","title":"Template"},{"location":"developer_documentation/github/#rules","text":"Git does not wrap commit messages automatically. That makes it hard to see the whole message in a terminal. So following rules will apply for commit messages: The first line of the commit starts with the ticket number, after that there will be a colon and followed by a short description what the ticket is for: The message should not be longer than 72 characters! Do not wrap this first line If you need more descriptions: Leave one line empty after the first main message Write more descriptions. For better reading try not to write more than 72 characters per line Here, you can wrap the lines Try to be short and precise","title":"Rules"},{"location":"developer_documentation/github/#viewing-history-of-commits","text":"Seeing all full commit messages decreasing history git log Seeing first line of the commit messages decreasing history git log --oneline Viewing branches Seeing the information on the branch you are on: git branch List all child branches git branch --list Reset changes Reset all local changes git reset --hard This discard all local changes to all files permanently. Reset changes of a specific file git reset --hard HEAD <file> HEAD is your current branch. Reset changes of last n commits: git reset --hard HEAD~<number-of-changes> The HEAD is your current branch. Reset to the last commit: git reset --hard HEAD~1 Reset to the previous last commit: git reset --hard HEAD~2 Revert changes Undo changes of a specific commit: Find the commit number: git log --online The commit number is the number before each commit message Revert changes to the commit number: git revert <commit-number> The git revert will undo the given commit but will create a new commit without deleting the older one. The git revert command will not touch the commits in your history, even the reverted ones. Stash changes You can temporarily stashes changes you have made to your working copy. You can work on something else and come back late and re-apply them. It helps you when you need to quickly switch context and work on something else on your local working copy. You can switches between branches your stashed changes will be still available. Stashing your work git stash That stashes your local (uncommited) changes, save them for later use and revert them from your working copy. Re-applying your stashed changes git stash pop This removes the changes from your stash and re-applies them to your working copy. If you do not want that you changes will be removed from the stash use: git stash apply The stash apply command is really helpful if you need to apply the changes on multiple branches.","title":"Viewing history of commits"},{"location":"developer_documentation/github/#delete-branches","text":"This should be only a well-considered option. You can delete a branch locally or remotely: Delete the branch locally: git branch -D <name_of_branch_to_delete> This deletion command force to delete the branch even if it is un-merged. Delete the branch on github: git push origin --delete <name_of_branch_to_delete>","title":"Delete Branches"},{"location":"developer_documentation/jira/","text":"Warning This documentation is currently under construction and may not be up to date. Ticket Types There are several ticket types to track the work of a project. Epics An Epic is a body of work that can be break down into multiple user stories (issues). Epics are too much work for one sprint. An epic is almost always delivered over a set of sprints. Issues The team use issues to track individual pieces of work that must be completed. An issue represents a story (Scrum), a bug or a task. Typically, issues represent things like big features, user requirements and software bugs. The work for an issue must be small enough such that it can done in one sprint. Sub-Tasks Issues can have sub-tasks that are assigned and tracked individually. There are following reasons for creating a sub-tasks: Splitting an issue into even smaller chunks Allowing various aspects of an issue to be assigned to different people Creating a to-do list for an issue Customised Types Each issue type can be customise. Additional, you can add your own issue type. For example: Bug Documentation Only users with admin permission can customise issue types. This feature is available on the project settings menu. Features of an issue Estimation The estimation of an issue can be tracked the velocity of a project. Estimating issues in the backlog helps to predict how long portions of the backlog might take to be delivered. The most popular estimation scheme is story points. Story points measure the complexity of one issue relative to the others. The estimation of a issue can be stored in the \"Estimate\" field. Flag Flagging an issue can be useful for: indicating that the assignee of the ticket can not finish his work because of having too little time. Other team members are welcome to take over. marking an issue as blocked, for example by other not done issues or another team. Priority Issues can have priorities. That allows you to rank your issues and what you should work on next. Time Tracking There is a time tracking possible on each issue. So, you can track the time you spend working on the ticket. To start and stop the time tracking, you can simply click on the time tracking button. Breaks are supported and it shows what time you already spent for this issue. Time tracking means hard discipline. Every time when you do a break, you need to click on the time tracking button of the issue and click again when you re-start working. Labels Labels are tags or keywords that can be added to issues. They let you categorise an issue. It is similar to the hashtag ( # ) used in twitter. They also can used for searching an issue. Fix version The fix version of an issue identifies the version when a issue should be done. Links in issues Issues can be linked to keep track of duplicate or related work. If you want to create a link of another issue in your issue: open the issue and and click on link issue choose the reason why you want to link the issue (for example: 'duplicates') choose the issue that you want to link You can also link confluence pages or a web-site to an issue. Attachment Attachments can be added to an issue. You can add folders, text files, PDFs, images, etc. Sprint The sprint field of an issue identifies the sprint in that the issue should be processed. Status Each issue has an status field that specify the current status of an issue - for example: TO DO In progress Review Approved Done Users Types Reporter The reporter is the person who raised the request. Usually, this is the same person who created the issue, but not always - for example: you can create issues on behalf of someone else. Assignee The assignee is the person who is responsible for completing the issue, including sub-tasks. The assignee is working on the issue. An issue with sub-tasks can assigned to multiple persons. The person of the parent issue is the most responsible person (like a kind of lead). Watcher A watcher is a person that has at least read permissions on the project and keep an eye on the project. It is possible to watching an issue that allows persons to keep an eye on a special issue. The watcher will be informed whenever there is any update or changes in that issue. Start watching a issue Go to the issue Click on the eye symbol on the top right Click on Add watchers and add the new watcher Stop watching a issue Go to the issue Click on the eye symbol on the top right Hover over the name of the watcher you want to remove Click on the X Work with issues Sprints A sprint - also known as an iteration - is a short, well defined time-boxed period when work of a set amount of work should be completed. The most often chosen time period for a sprint is two weeks or one month In Jira, you view sprints on a board and assign issues to the sprints. Each individual issue as a sprint field for seeing the sprint that the issue is part of. Backlog The backlog is a list of tasks that represents the outstanding work in a project. Any issues of the backlog can be add to a sprint. If you raise an issue, you should put it into the backlog. The team decides when it will be put into the sprint Board The board displays all issues of the project that are in the current sprint. It gives you an overview: who is working on which issue the status of each individual issue the assignee of each issue if issues are blocked or not Roadmap A good product road map makes sure that everyone working on the product understands the status of work and are aligned on upcoming priorities. Road maps in Jira enable you to quickly create a timeline of your plans, update your priorities and communicate the status of the work. Releases A release present a point in time of your project. Releases can be used to schedule how features are rolled out or as a way to organise work that has been completed for the project. Jira provides a releases feature. You can add releases to each issue to specify which version contains which feature. The releases page shows how much work has been completed in each release version. Reports There are several plugins that can be used to create a report of the current project status. The most important is the Sprint burn down chart. Sprint burn down chart The burn down chart shows the amount of work remaining on a sprint. The following will help you to understand the burn down chart in Jira: The burn down chart in Jira is board-specific. The vertical axis represents the estimation statistic that you have configured - for example issue count or story points. The burn down chart is based on the board\u2019s column mapping. Only issues with status Done (left-most column) will be considered. The grey guideline represent the optimal sprint burn down. Plugins Jira provides several plugins that can be activated. Code You can connect your Github for the project with the Jira project. In our system, this plugin will be fully configured in the near future. Confluence You can link a confluence space to your Jira project. You can see, add, remove and edit each page in the space using the Jira software.","title":"Working with Jira"},{"location":"developer_documentation/jira/#ticket-types","text":"There are several ticket types to track the work of a project.","title":"Ticket Types"},{"location":"developer_documentation/jira/#epics","text":"An Epic is a body of work that can be break down into multiple user stories (issues). Epics are too much work for one sprint. An epic is almost always delivered over a set of sprints.","title":"Epics"},{"location":"developer_documentation/jira/#issues","text":"The team use issues to track individual pieces of work that must be completed. An issue represents a story (Scrum), a bug or a task. Typically, issues represent things like big features, user requirements and software bugs. The work for an issue must be small enough such that it can done in one sprint.","title":"Issues"},{"location":"developer_documentation/jira/#sub-tasks","text":"Issues can have sub-tasks that are assigned and tracked individually. There are following reasons for creating a sub-tasks: Splitting an issue into even smaller chunks Allowing various aspects of an issue to be assigned to different people Creating a to-do list for an issue","title":"Sub-Tasks"},{"location":"developer_documentation/jira/#customised-types","text":"Each issue type can be customise. Additional, you can add your own issue type. For example: Bug Documentation Only users with admin permission can customise issue types. This feature is available on the project settings menu.","title":"Customised Types"},{"location":"developer_documentation/jira/#features-of-an-issue","text":"","title":"Features of an issue"},{"location":"developer_documentation/jira/#estimation","text":"The estimation of an issue can be tracked the velocity of a project. Estimating issues in the backlog helps to predict how long portions of the backlog might take to be delivered. The most popular estimation scheme is story points. Story points measure the complexity of one issue relative to the others. The estimation of a issue can be stored in the \"Estimate\" field.","title":"Estimation"},{"location":"developer_documentation/jira/#flag","text":"Flagging an issue can be useful for: indicating that the assignee of the ticket can not finish his work because of having too little time. Other team members are welcome to take over. marking an issue as blocked, for example by other not done issues or another team.","title":"Flag"},{"location":"developer_documentation/jira/#priority","text":"Issues can have priorities. That allows you to rank your issues and what you should work on next.","title":"Priority"},{"location":"developer_documentation/jira/#time-tracking","text":"There is a time tracking possible on each issue. So, you can track the time you spend working on the ticket. To start and stop the time tracking, you can simply click on the time tracking button. Breaks are supported and it shows what time you already spent for this issue. Time tracking means hard discipline. Every time when you do a break, you need to click on the time tracking button of the issue and click again when you re-start working.","title":"Time Tracking"},{"location":"developer_documentation/jira/#labels","text":"Labels are tags or keywords that can be added to issues. They let you categorise an issue. It is similar to the hashtag ( # ) used in twitter. They also can used for searching an issue.","title":"Labels"},{"location":"developer_documentation/jira/#fix-version","text":"The fix version of an issue identifies the version when a issue should be done.","title":"Fix version"},{"location":"developer_documentation/jira/#links-in-issues","text":"Issues can be linked to keep track of duplicate or related work. If you want to create a link of another issue in your issue: open the issue and and click on link issue choose the reason why you want to link the issue (for example: 'duplicates') choose the issue that you want to link You can also link confluence pages or a web-site to an issue.","title":"Links in issues"},{"location":"developer_documentation/jira/#attachment","text":"Attachments can be added to an issue. You can add folders, text files, PDFs, images, etc.","title":"Attachment"},{"location":"developer_documentation/jira/#sprint","text":"The sprint field of an issue identifies the sprint in that the issue should be processed.","title":"Sprint"},{"location":"developer_documentation/jira/#status","text":"Each issue has an status field that specify the current status of an issue - for example: TO DO In progress Review Approved Done","title":"Status"},{"location":"developer_documentation/jira/#users-types","text":"","title":"Users Types"},{"location":"developer_documentation/jira/#reporter","text":"The reporter is the person who raised the request. Usually, this is the same person who created the issue, but not always - for example: you can create issues on behalf of someone else.","title":"Reporter"},{"location":"developer_documentation/jira/#assignee","text":"The assignee is the person who is responsible for completing the issue, including sub-tasks. The assignee is working on the issue. An issue with sub-tasks can assigned to multiple persons. The person of the parent issue is the most responsible person (like a kind of lead).","title":"Assignee"},{"location":"developer_documentation/jira/#watcher","text":"A watcher is a person that has at least read permissions on the project and keep an eye on the project. It is possible to watching an issue that allows persons to keep an eye on a special issue. The watcher will be informed whenever there is any update or changes in that issue.","title":"Watcher"},{"location":"developer_documentation/jira/#start-watching-a-issue","text":"Go to the issue Click on the eye symbol on the top right Click on Add watchers and add the new watcher","title":"Start watching a issue"},{"location":"developer_documentation/jira/#stop-watching-a-issue","text":"Go to the issue Click on the eye symbol on the top right Hover over the name of the watcher you want to remove Click on the X","title":"Stop watching a issue"},{"location":"developer_documentation/jira/#work-with-issues","text":"","title":"Work with issues"},{"location":"developer_documentation/jira/#sprints","text":"A sprint - also known as an iteration - is a short, well defined time-boxed period when work of a set amount of work should be completed. The most often chosen time period for a sprint is two weeks or one month In Jira, you view sprints on a board and assign issues to the sprints. Each individual issue as a sprint field for seeing the sprint that the issue is part of.","title":"Sprints"},{"location":"developer_documentation/jira/#backlog","text":"The backlog is a list of tasks that represents the outstanding work in a project. Any issues of the backlog can be add to a sprint. If you raise an issue, you should put it into the backlog. The team decides when it will be put into the sprint","title":"Backlog"},{"location":"developer_documentation/jira/#board","text":"The board displays all issues of the project that are in the current sprint. It gives you an overview: who is working on which issue the status of each individual issue the assignee of each issue if issues are blocked or not","title":"Board"},{"location":"developer_documentation/jira/#roadmap","text":"A good product road map makes sure that everyone working on the product understands the status of work and are aligned on upcoming priorities. Road maps in Jira enable you to quickly create a timeline of your plans, update your priorities and communicate the status of the work.","title":"Roadmap"},{"location":"developer_documentation/jira/#releases","text":"A release present a point in time of your project. Releases can be used to schedule how features are rolled out or as a way to organise work that has been completed for the project. Jira provides a releases feature. You can add releases to each issue to specify which version contains which feature. The releases page shows how much work has been completed in each release version.","title":"Releases"},{"location":"developer_documentation/jira/#reports","text":"There are several plugins that can be used to create a report of the current project status. The most important is the Sprint burn down chart.","title":"Reports"},{"location":"developer_documentation/jira/#sprint-burn-down-chart","text":"The burn down chart shows the amount of work remaining on a sprint. The following will help you to understand the burn down chart in Jira: The burn down chart in Jira is board-specific. The vertical axis represents the estimation statistic that you have configured - for example issue count or story points. The burn down chart is based on the board\u2019s column mapping. Only issues with status Done (left-most column) will be considered. The grey guideline represent the optimal sprint burn down.","title":"Sprint burn down chart"},{"location":"developer_documentation/jira/#plugins","text":"Jira provides several plugins that can be activated.","title":"Plugins"},{"location":"developer_documentation/jira/#code","text":"You can connect your Github for the project with the Jira project. In our system, this plugin will be fully configured in the near future.","title":"Code"},{"location":"developer_documentation/jira/#confluence","text":"You can link a confluence space to your Jira project. You can see, add, remove and edit each page in the space using the Jira software.","title":"Confluence"},{"location":"developer_documentation/managing_environments/","text":"Warning This documentation is currently under construction and may not be up to date. The following covers two particular aspects of managing conda environments. Modification of dependencies in environment.yaml files. Creation and naming of the conda development environments. The changes are made to help Eliminate the confusion when there are mismatches between a cdds version and cdds conda development environment name. Prevent accidental creation of unsolvable environment.yaml files when merging from release branches into main or vice versa. Avoid setup_env_for_devel script pointing to an environment that doesn't match that of the environment.yaml for any given commit. Prevent having to modify existing installed cdds conda development environments. Modifying environment.yaml Files If there are modifications to cdds needed that require a new environment then this should be managed in a consistent way. Examples of such changes could include a package version needs changing, new package(s) added old package(s) removed, a channel usage change In the majority of cases, a change will normally be driven by a change to the cdds code base, so will have an existing JIRA issue. However, if this isn't the case then an issue should still be raised to cover any environment change (E.g. if a change to conda channels is needed but no package version changes thus no code changes are needed.). During the completion of the ticket in question, the process should be as follows. (This example assumes a new package is needed for new functionality within cdds and working on main) Create a new conda environment for development work on this particular change. This could be done on the cdds account, although modifying environments can sometimes be tricky. Initially it would be better to do this on a user account first as this minimises the chance of accidentally modifying any existing cdds development environments. There are two approaches adding a package. Update environment_dev.yaml specifying a particular version if this needed otherwise this can be left out and let the solver figure out the best version to use. Create an environment using environment_dev.yaml and then install the package afterwards using conda install Make the required code changes using this new environment e.g. add the new functionality that makes use of the new package. When ready to create a pull request with these changes, create the next development environment on the cdds account using the correct naming convention cdds-X.X_dev-X Update setup_dev_env_for_development to point to the new environment on the cdds account. Update the .yaml files based on what is in the installed version of cdds Make sure all the tests are still passing. By following this process it makes sure that when the code changes are being reviewed the reviewer does not need to create their own environment. the review will be using the actual environment that will be live once the PR is brought into main. the existing development environment on main doesn't need to be modified in any way. when the PR is brought into main the transition is seamless. it is easy to remove the environment and start again if needs be based on feedback in the review without disrupting anything. Adding new environments should be fairly infrequent procedure, but make sure to be aware so when merging the PR back into By adhering to the above process, it should mean that the state of main is always consistent with the environment.yaml files it contains, and the setup script should also be always be pointing to the correct environment. Environment Changes on Bugfix Releases If at all possible, changes to environments on bugfix branches should be avoided as it will increases the amount of work needed introduces more potential things to go wrong Of course sometimes it is unavoidable so in such cases be aware of the following. merging changes made to an enironment.yaml file on a release branch into main is almost always going to be a bad idea and increase the possibility of producing unsolvable environments unless the files are identical. The normal practice of having a PR which brings changes from a dev branch (that branched from a release branch) into the release branch, and then cherry picking the squashed commit directly onto main may be a bad approach in this case. I would suggest that when bringing such changes into main it would be better to create a new branch off of main, cherry pick the bugfix code changes onto branch but revert environment changes. Managing Development Environments Regarding point two, the naming conventions used for cdds development environments have been somewhat inconsistent and can at times lead to confusion. Here are a selection of cdds conda development environments. cdds-2.1_dev-1 cdds-2.1_dev-1 cdds-2.3.2_dev cdds-2.3_dev cdds-2.4.0_dev cdds-2.4.0_dev-1 cdds-2.4_dev_3 cdds-2.5.0_dev-1 A first step would be sticking with dev environments not being tied to specific hotfix versions. This is something that we have already done for some versions and not for others eg. Going forward I think it would be best to adopt the convention that does not reference a specific hotfix version such as (examples given for cdds version 2.5.0). cdds-2.5_dev-0 or cdds-2.5.X_dev-0 (the X is not meant to be replaced) This helps to avoid any confusion that can arise when there is a mismatch over the cdds version number specified in a given copy of cdds and the development environment that setup_env_for_development points to. Such situations can occur when there have been no updates to the environment for a period of time. It does not necessarily mean Additionally, particular attention should be paid to the change of version number on main following a maj/min release. This should be done immediately after the release. Concrete Example Here is a concrete example of the above starting from a 2.5.0 release. The 2.5.0 release branch v2.5_release is created from main Update main by Incrementing cdds version to 2.6.0 Creating a new environment called cdds-2.6_dev-0 Pointing setup_env_dev_for_development to this new cdds-2.6_dev-0 environment As development proceeds on main over time new development environments are created in line with the practices described earlier in this document. E.g. cdds-2.6_dev-1 - (change - added new package) cdds-2.6_dev-2 - (change - added new package) As development also proceeds on v2.5_release concurrently, and although it is not desirable, it may be necessary for a change to the environment. (For the purpose of this example we will assume the latest development environment was cdds-2.5_dev-1 when the v2.5_release branch was made.) A new environment cdds-2.5_dev-2 is created for this change. A new environment cdds-2.6_dev-3 is needed for development on main if this change needs to be brought over (this will almost always be the case). When main reaches a state ready to release 2.6.0 the process continues as above. The 2.6.0 release branch v2.6_release is created from main Then update main by Incrementing cdds version to 2.7.0 Creating a new environment called cdds-2.7_dev-0 Pointing setup_env_dev_for_development to cdds-2.7_dev-0 The resultant list of environments would look something like this. cdds-2.5_dev-1 cdds-2.6_dev-0 cdds-2.6_dev-1 cdds-2.6_dev-2 cdds-2.5_dev-2 cdds-2.6_dev-3 cdds-2.7_dev-0 A downside of the above convention is that there will be a duplicated environment for every 2.X release which differs only in name. I.e, in the above example cdds-2.7_dev-0 and cdds-2.6_dev-3 should be exactly the same, aside from having different names. However, I think this an acceptable inefficiency for preventing confusion around environment versioning. Conda Good to Know When there are multiple channels defined in an environment.yaml there is an implicit priority based on their order, which if you are not aware of, conda may solve environments in ways you weren\u2019t expecting. See notes on --flexible_solve https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html#strict-channel-priority tldr flexible solve is the default and will prefer packages from higher priority channels even if there are more recent versions in other channels. Make use of mamba and the --dry-run option to speed your workflow up. The same versioned package","title":"Managing Conda Environments"},{"location":"developer_documentation/managing_environments/#modifying-environmentyaml-files","text":"If there are modifications to cdds needed that require a new environment then this should be managed in a consistent way. Examples of such changes could include a package version needs changing, new package(s) added old package(s) removed, a channel usage change In the majority of cases, a change will normally be driven by a change to the cdds code base, so will have an existing JIRA issue. However, if this isn't the case then an issue should still be raised to cover any environment change (E.g. if a change to conda channels is needed but no package version changes thus no code changes are needed.). During the completion of the ticket in question, the process should be as follows. (This example assumes a new package is needed for new functionality within cdds and working on main) Create a new conda environment for development work on this particular change. This could be done on the cdds account, although modifying environments can sometimes be tricky. Initially it would be better to do this on a user account first as this minimises the chance of accidentally modifying any existing cdds development environments. There are two approaches adding a package. Update environment_dev.yaml specifying a particular version if this needed otherwise this can be left out and let the solver figure out the best version to use. Create an environment using environment_dev.yaml and then install the package afterwards using conda install Make the required code changes using this new environment e.g. add the new functionality that makes use of the new package. When ready to create a pull request with these changes, create the next development environment on the cdds account using the correct naming convention cdds-X.X_dev-X Update setup_dev_env_for_development to point to the new environment on the cdds account. Update the .yaml files based on what is in the installed version of cdds Make sure all the tests are still passing. By following this process it makes sure that when the code changes are being reviewed the reviewer does not need to create their own environment. the review will be using the actual environment that will be live once the PR is brought into main. the existing development environment on main doesn't need to be modified in any way. when the PR is brought into main the transition is seamless. it is easy to remove the environment and start again if needs be based on feedback in the review without disrupting anything. Adding new environments should be fairly infrequent procedure, but make sure to be aware so when merging the PR back into By adhering to the above process, it should mean that the state of main is always consistent with the environment.yaml files it contains, and the setup script should also be always be pointing to the correct environment.","title":"Modifying environment.yaml Files"},{"location":"developer_documentation/managing_environments/#environment-changes-on-bugfix-releases","text":"If at all possible, changes to environments on bugfix branches should be avoided as it will increases the amount of work needed introduces more potential things to go wrong Of course sometimes it is unavoidable so in such cases be aware of the following. merging changes made to an enironment.yaml file on a release branch into main is almost always going to be a bad idea and increase the possibility of producing unsolvable environments unless the files are identical. The normal practice of having a PR which brings changes from a dev branch (that branched from a release branch) into the release branch, and then cherry picking the squashed commit directly onto main may be a bad approach in this case. I would suggest that when bringing such changes into main it would be better to create a new branch off of main, cherry pick the bugfix code changes onto branch but revert environment changes.","title":"Environment Changes on Bugfix Releases"},{"location":"developer_documentation/managing_environments/#managing-development-environments","text":"Regarding point two, the naming conventions used for cdds development environments have been somewhat inconsistent and can at times lead to confusion. Here are a selection of cdds conda development environments. cdds-2.1_dev-1 cdds-2.1_dev-1 cdds-2.3.2_dev cdds-2.3_dev cdds-2.4.0_dev cdds-2.4.0_dev-1 cdds-2.4_dev_3 cdds-2.5.0_dev-1 A first step would be sticking with dev environments not being tied to specific hotfix versions. This is something that we have already done for some versions and not for others eg. Going forward I think it would be best to adopt the convention that does not reference a specific hotfix version such as (examples given for cdds version 2.5.0). cdds-2.5_dev-0 or cdds-2.5.X_dev-0 (the X is not meant to be replaced) This helps to avoid any confusion that can arise when there is a mismatch over the cdds version number specified in a given copy of cdds and the development environment that setup_env_for_development points to. Such situations can occur when there have been no updates to the environment for a period of time. It does not necessarily mean Additionally, particular attention should be paid to the change of version number on main following a maj/min release. This should be done immediately after the release.","title":"Managing Development Environments"},{"location":"developer_documentation/managing_environments/#concrete-example","text":"Here is a concrete example of the above starting from a 2.5.0 release. The 2.5.0 release branch v2.5_release is created from main Update main by Incrementing cdds version to 2.6.0 Creating a new environment called cdds-2.6_dev-0 Pointing setup_env_dev_for_development to this new cdds-2.6_dev-0 environment As development proceeds on main over time new development environments are created in line with the practices described earlier in this document. E.g. cdds-2.6_dev-1 - (change - added new package) cdds-2.6_dev-2 - (change - added new package) As development also proceeds on v2.5_release concurrently, and although it is not desirable, it may be necessary for a change to the environment. (For the purpose of this example we will assume the latest development environment was cdds-2.5_dev-1 when the v2.5_release branch was made.) A new environment cdds-2.5_dev-2 is created for this change. A new environment cdds-2.6_dev-3 is needed for development on main if this change needs to be brought over (this will almost always be the case). When main reaches a state ready to release 2.6.0 the process continues as above. The 2.6.0 release branch v2.6_release is created from main Then update main by Incrementing cdds version to 2.7.0 Creating a new environment called cdds-2.7_dev-0 Pointing setup_env_dev_for_development to cdds-2.7_dev-0 The resultant list of environments would look something like this. cdds-2.5_dev-1 cdds-2.6_dev-0 cdds-2.6_dev-1 cdds-2.6_dev-2 cdds-2.5_dev-2 cdds-2.6_dev-3 cdds-2.7_dev-0 A downside of the above convention is that there will be a duplicated environment for every 2.X release which differs only in name. I.e, in the above example cdds-2.7_dev-0 and cdds-2.6_dev-3 should be exactly the same, aside from having different names. However, I think this an acceptable inefficiency for preventing confusion around environment versioning.","title":"Concrete Example"},{"location":"developer_documentation/managing_environments/#conda-good-to-know","text":"When there are multiple channels defined in an environment.yaml there is an implicit priority based on their order, which if you are not aware of, conda may solve environments in ways you weren\u2019t expecting. See notes on --flexible_solve https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html#strict-channel-priority tldr flexible solve is the default and will prefer packages from higher priority channels even if there are more recent versions in other channels. Make use of mamba and the --dry-run option to speed your workflow up. The same versioned package","title":"Conda Good to Know"},{"location":"developer_documentation/mip_convert_functional_tests/","text":"Warning This documentation is currently under construction and may not be up to date. Create a test class (pattern: test_ ) in mip_convert.tests.test_functional. . Import the functional tests class for MIP convert tests from mip_convert.tests.test_functional.test_command_line import AbstractFunctionalTests Extends your test class from the AbstractFuntionalTests Implement the get_test_data function: The function should return the test data as an TestData object (e.g. Cmip6TestData or AriseTestData) The available TestData objects are in mip_convert.tests.test_functional.utils.configurations Specify the necessary individual values of the TestData object (like mip_table, variable, specific_info, etc.) For examples, see tests in mip_covert/tests/test_functional Implement your test method that runs check_convert. This function will trigger MIP convert with your given test data. Mark your test as a slow test by add following annotation to your test method @attr ( 'slow' ) Note For debugging a MIP convert functional test, remove the slow annotation of the test. Afterwards you can run the test as usual in your IDE.","title":"Mip Convert Functional Tests"},{"location":"developer_documentation/nightly_tests/","text":"Warning This documentation is currently under construction and may not be up to date.","title":"Nightly Tests"},{"location":"developer_documentation/pytest/","text":"Warning This documentation is currently under construction and may not be up to date. Pytest is the testing framework used in CDDS . It currently serves two main functions. It acts as the testrunner, whereby it collects all the tests and executes them. It is a unit testing library. The version of pytest currently used within cdds is 7.1.2 Link to the pytest 7.1.x Documentation Link to the pytest 7.1.x API Pytest Configuration [tool:pytest] python_files = test_*.py python_functions = test_* console_output_style = progress addopts = -m 'not slow and not integration and not rabbitMQ and not data_request' markers = slow rabbitMQ data_request integration style Running pytest CDDS already provides a script to run all tests via pytest: Activate the conda environment: source setup_env_for_devel Run the run_all_tests script: ./run_all_tests The script runs all unit tests including all integration and slow tests. It does not run the RabbitMQ tests! Tests failures will be logged in the cdds_test_failures.log file. Run RabbitMQ tests The RabbitMQ tests needs a proper RabbitMQ installation. Only, the server els055 and els056 have RabbitMQ properly installed for CDDS. RabbitMQ needs credentials that are specified in your home directory $HOME/.cdds_credentials and has following content: [rabbit] host = <hostname> port = <port number> userid = <user id> use_plain = true vhost = dds_dev password = <password> For the correct values of the fields, please contact @Matthew Mizielinski . Connect to one of the servers via ssh, e.g: ssh els055 Go to your workspace directory for CDDS. (the same as on your local VM) Activate the conda environment: source setup_env_for_devel Run the test script: ./run_rabbitmq_tests Any test failures will be recorded in the cdds_rabbitmq_test_failures.log file. Run single tests Run all tests in a specific package pytest -s <package-folder> For example: run all tests in mip_convert: pytest -s mip_convert Run all tests in a specific module pytest <test_module.py> For example: Run tests in test_coding_standards.py: pytest test_coding_standards.py Run specific test in a TestCase class One option is to us -k . The -k command line option to specify an expression which implements a sub-string match on the test names: pytest <test_module.py> -k <TestClass> For example run all tests implemented in TestLoadCmipPlugin class in the module test_plugin_loader: pytest test_plugin_loader.py -k TestLoadCmipPlugin Additional, you can also use Node IDs to run all tests of a specific class. How to do this, see next section. Run specific test To run a specific test, pytest use Node IDs that is assigned to each collected test and which consists of the module filename followed by specifiers like class names, function names and parameters from parametrization separated by :: characters. A specific test implemented in a specific TestClass can be run by using pytest <test_module.py>::<TestClass>::<test_method> For example, run the test test_load_cdds_plugin in the module test_plugin_loader.py: pytest test_plugin_loader.py::TestLoadPlugin::test_load_cdds_plugin Node IDs are of the form module.py::class::method or module.py::function. Node IDs control which tests are collected, so module.py::class will select all test methods on the class. Useful Command Line Options There are many other options that can be used to configure pytest. Option Description -h show help message and configuration info -k <expression> only run tests which match the given substring expression -v increase verbosity --no-summary disable summary -r <chars> show extra text summary info as specified by chars: (f)ailded, (E)rror, (s)kipped, (x)failed, (X)passed, (p)assed, (P)assed with output, (a)ll excepted passed (p/P), or (A)ll. (w)arnings are enabled by default. --disable-warnings disable warnings summary --tb=<style> traceback print mode (auto/long/short/line/native/ no) -c <file> load configuration from --collect-only only collect tests, don\u2019t execute them --debug=[DEBUG_FILE_NAME] store internal tracing debug information in this log file. Default is pytestdebug.log Pytest Annotations Annotation Description Link @pytest . mark . skip ( message ) Skip an executing test with the given message. @pytest . mark . parametrize ( input_parameter , test_data ) It allows one to define multiple sets of arguments and fixtures at the test function or class. Parametrizing fixtures and test functions \u2014 pytest documentation Using nosetests attributes We still use the nosetests attributes to specify slow tests, integration tests, etc. These attributes can still be used because of pytest-attrib Test can be run with an -a option: pytest -a slow This runs all slow tests. If you want to not run the slow tests, you can do this as followed: pytest -a \"not slow\" The expression given in the -a argument can be even more complex, for example: pytest -a \"slow and integration\" Plugins Pytest has a large plugin ecosystem. pytest-cov Usage.","title":"Pytest"},{"location":"developer_documentation/pytest/#pytest-configuration","text":"[tool:pytest] python_files = test_*.py python_functions = test_* console_output_style = progress addopts = -m 'not slow and not integration and not rabbitMQ and not data_request' markers = slow rabbitMQ data_request integration style","title":"Pytest Configuration"},{"location":"developer_documentation/pytest/#running-pytest","text":"CDDS already provides a script to run all tests via pytest: Activate the conda environment: source setup_env_for_devel Run the run_all_tests script: ./run_all_tests The script runs all unit tests including all integration and slow tests. It does not run the RabbitMQ tests! Tests failures will be logged in the cdds_test_failures.log file. Run RabbitMQ tests The RabbitMQ tests needs a proper RabbitMQ installation. Only, the server els055 and els056 have RabbitMQ properly installed for CDDS. RabbitMQ needs credentials that are specified in your home directory $HOME/.cdds_credentials and has following content: [rabbit] host = <hostname> port = <port number> userid = <user id> use_plain = true vhost = dds_dev password = <password> For the correct values of the fields, please contact @Matthew Mizielinski . Connect to one of the servers via ssh, e.g: ssh els055 Go to your workspace directory for CDDS. (the same as on your local VM) Activate the conda environment: source setup_env_for_devel Run the test script: ./run_rabbitmq_tests Any test failures will be recorded in the cdds_rabbitmq_test_failures.log file. Run single tests Run all tests in a specific package pytest -s <package-folder> For example: run all tests in mip_convert: pytest -s mip_convert Run all tests in a specific module pytest <test_module.py> For example: Run tests in test_coding_standards.py: pytest test_coding_standards.py Run specific test in a TestCase class One option is to us -k . The -k command line option to specify an expression which implements a sub-string match on the test names: pytest <test_module.py> -k <TestClass> For example run all tests implemented in TestLoadCmipPlugin class in the module test_plugin_loader: pytest test_plugin_loader.py -k TestLoadCmipPlugin Additional, you can also use Node IDs to run all tests of a specific class. How to do this, see next section. Run specific test To run a specific test, pytest use Node IDs that is assigned to each collected test and which consists of the module filename followed by specifiers like class names, function names and parameters from parametrization separated by :: characters. A specific test implemented in a specific TestClass can be run by using pytest <test_module.py>::<TestClass>::<test_method> For example, run the test test_load_cdds_plugin in the module test_plugin_loader.py: pytest test_plugin_loader.py::TestLoadPlugin::test_load_cdds_plugin Node IDs are of the form module.py::class::method or module.py::function. Node IDs control which tests are collected, so module.py::class will select all test methods on the class.","title":"Running pytest"},{"location":"developer_documentation/pytest/#useful-command-line-options","text":"There are many other options that can be used to configure pytest. Option Description -h show help message and configuration info -k <expression> only run tests which match the given substring expression -v increase verbosity --no-summary disable summary -r <chars> show extra text summary info as specified by chars: (f)ailded, (E)rror, (s)kipped, (x)failed, (X)passed, (p)assed, (P)assed with output, (a)ll excepted passed (p/P), or (A)ll. (w)arnings are enabled by default. --disable-warnings disable warnings summary --tb=<style> traceback print mode (auto/long/short/line/native/ no) -c <file> load configuration from --collect-only only collect tests, don\u2019t execute them --debug=[DEBUG_FILE_NAME] store internal tracing debug information in this log file. Default is pytestdebug.log","title":"Useful Command Line Options"},{"location":"developer_documentation/pytest/#pytest-annotations","text":"Annotation Description Link @pytest . mark . skip ( message ) Skip an executing test with the given message. @pytest . mark . parametrize ( input_parameter , test_data ) It allows one to define multiple sets of arguments and fixtures at the test function or class. Parametrizing fixtures and test functions \u2014 pytest documentation","title":"Pytest Annotations"},{"location":"developer_documentation/pytest/#using-nosetests-attributes","text":"We still use the nosetests attributes to specify slow tests, integration tests, etc. These attributes can still be used because of pytest-attrib Test can be run with an -a option: pytest -a slow This runs all slow tests. If you want to not run the slow tests, you can do this as followed: pytest -a \"not slow\" The expression given in the -a argument can be even more complex, for example: pytest -a \"slow and integration\"","title":"Using nosetests attributes"},{"location":"developer_documentation/pytest/#plugins","text":"Pytest has a large plugin ecosystem.","title":"Plugins"},{"location":"developer_documentation/pytest/#pytest-cov","text":"Usage.","title":"pytest-cov"},{"location":"developer_documentation/review_process/","text":"Warning This documentation is currently under construction and may not be up to date. The reviewer and reviewee discuss the type of review required. If time is a factor, focus effort on the High Priority items. Agree whether the reviewer can make corrections related to the Low Priority items, e.g., typos or unused imports, directly to the branch. If the reviewer feels they need more information after an initial look at the code, they request a pair review with the reviewee. The reviewer presents the review to the reviewee (with reasons for priority as necessary), either via the Jira ticket (e.g., if the feedback is minimal), a Confluence page, e-mails, or in person, as appropriate. The reviewee assesses the reviewer's recommendations and makes any appropriate changes to the code. Open a new Jira ticket for any low risk issues that do not need to be resolved immediately. The reviewee documents the important points from the review about why things were done (or not done) for whatever reasons on the Jira ticket (for future reference). The reviewee updates the Coding Guidelines appropriately. Review Checklists High Priority Is the code in version control? Does the code work as expected? Does the code manage the risk around availability of resources such as files, databases, mass (assess the risk though first) Does the code check for common errors? Does the code use exceptions appropriately? Are there corresponding unit tests for the code? Can the unit tests be executed? Is there corresponding documentation for the code? Can the documentation be built? Is the documentation easy to understand? Medium Priority Is the code easy to read and understand? Has repetitive code been avoided? Is the code easy to maintain? Low Priority Does the code comply to the coding standards?","title":"Review Process"},{"location":"developer_documentation/review_process/#review-checklists","text":"","title":"Review Checklists"},{"location":"developer_documentation/review_process/#high-priority","text":"Is the code in version control? Does the code work as expected? Does the code manage the risk around availability of resources such as files, databases, mass (assess the risk though first) Does the code check for common errors? Does the code use exceptions appropriately? Are there corresponding unit tests for the code? Can the unit tests be executed? Is there corresponding documentation for the code? Can the documentation be built? Is the documentation easy to understand?","title":"High Priority"},{"location":"developer_documentation/review_process/#medium-priority","text":"Is the code easy to read and understand? Has repetitive code been avoided? Is the code easy to maintain?","title":"Medium Priority"},{"location":"developer_documentation/review_process/#low-priority","text":"Does the code comply to the coding standards?","title":"Low Priority"},{"location":"developer_documentation/unittests/","text":"Warning This documentation is currently under construction and may not be up to date. Running Unit Tests The CDDS code base includes a comprehensive set of unit and integration tests covering mode of the source code. Tests can be run in several ways. Start by navigating to the root directory of the CDDS source code. Ensure the development environment is correctly setup by running: source setup_env_for_devel Then, you can run on of the following commands to run tests: Run all tests on SPICE: sbatch submit_run_all_tests.batch Run all tests locally: ./run_all_tests Run tests for a particular package cdds pytest cdds pytest mip_convert Run tests for a particular test module: py.test hadsdk/hadsdk/tests/test_common.py Writing Unit Tests On some occasions Coding Guidelines WIP | Doctests may be sufficient as unit tests. Location and naming conventions Unit tests should be located in a parallel directory structure to the modules they are testing! For example: tests for the my_package/my_subpackage/my_module.py module should be located in the my_package/tests/my_subpackage/ directory in a test module with the name test_my_module.py Unit tests should not have docstrings. The log produced by pytest prints the name of the test in the form test_my_test_description (test_my_module.TestMyClass) if the test does not have a docstring, which is more helpful than the information that can fit into the first line of a docstring (which is printed if it exists). The pylint: disable = missing-docstring comment tells pylint not to warn about missing docstrings in the test module. Unit Test Example my_module.py # (C) British Crown Copyright 2015-2016, Met Office. # Please see LICENSE.rst for license details. \"\"\" My module. \"\"\" def my_function (): [ ... ] class MyClass (): def my_method1 ( self ): [ ... ] def my_method2 ( self ): [ ... ] test_my_module.py import unittest class TestMyClass ( unittest . TestCase ): \"\"\" Tests for MyClass in my_module.py. \"\"\" def test_ < class_test_description > ( self ): [ ... ] def test_my_method1_ < test_description > ( self ): [ ... ] def test_my_method2_ < test_description > ( self ): [ ... ] class TestMyFunction ( unittest . TestCase ): \"\"\" Tests for my_function in my_module.py. \"\"\" def test_ < first_test_description > ( self ): [ ... ] def test_ < second_test_description > ( self ): [ ... ] if __name__ == '__main__' : unittest . main ()","title":"Unit Testing"},{"location":"developer_documentation/unittests/#running-unit-tests","text":"The CDDS code base includes a comprehensive set of unit and integration tests covering mode of the source code. Tests can be run in several ways. Start by navigating to the root directory of the CDDS source code. Ensure the development environment is correctly setup by running: source setup_env_for_devel Then, you can run on of the following commands to run tests: Run all tests on SPICE: sbatch submit_run_all_tests.batch Run all tests locally: ./run_all_tests Run tests for a particular package cdds pytest cdds pytest mip_convert Run tests for a particular test module: py.test hadsdk/hadsdk/tests/test_common.py","title":"Running Unit Tests"},{"location":"developer_documentation/unittests/#writing-unit-tests","text":"On some occasions Coding Guidelines WIP | Doctests may be sufficient as unit tests. Location and naming conventions Unit tests should be located in a parallel directory structure to the modules they are testing! For example: tests for the my_package/my_subpackage/my_module.py module should be located in the my_package/tests/my_subpackage/ directory in a test module with the name test_my_module.py Unit tests should not have docstrings. The log produced by pytest prints the name of the test in the form test_my_test_description (test_my_module.TestMyClass) if the test does not have a docstring, which is more helpful than the information that can fit into the first line of a docstring (which is printed if it exists). The pylint: disable = missing-docstring comment tells pylint not to warn about missing docstrings in the test module. Unit Test Example my_module.py # (C) British Crown Copyright 2015-2016, Met Office. # Please see LICENSE.rst for license details. \"\"\" My module. \"\"\" def my_function (): [ ... ] class MyClass (): def my_method1 ( self ): [ ... ] def my_method2 ( self ): [ ... ] test_my_module.py import unittest class TestMyClass ( unittest . TestCase ): \"\"\" Tests for MyClass in my_module.py. \"\"\" def test_ < class_test_description > ( self ): [ ... ] def test_my_method1_ < test_description > ( self ): [ ... ] def test_my_method2_ < test_description > ( self ): [ ... ] class TestMyFunction ( unittest . TestCase ): \"\"\" Tests for my_function in my_module.py. \"\"\" def test_ < first_test_description > ( self ): [ ... ] def test_ < second_test_description > ( self ): [ ... ] if __name__ == '__main__' : unittest . main ()","title":"Writing Unit Tests"},{"location":"developer_documentation/deployment/cdds_installation/","text":"CDDS Installation Warning This page is still work-in-progress! Please work closely with the CDDS team when installing CDDS. On Azure (Met Office) On Premise (Met Office) On Jasmin Note This installation process requires a Conda installation of at least v4.9 , earlier versions will raise errors around the setting of environment variables as part of the Conda environment creation commands Login to the cdds account (please see Science Shared Accounts for more details): bash xsudo -u cdds bash -l Activate Conda environment (to make installation quicker): conda activate Obtain environment file: wget https://github.com/MetOffice/CDDS/blob/<tagname>/environment.yml or download it manually from Github. Update locations pointed to within the environment file: sed -i \"s/<location>/X.Y.Z/\" environment.yml Create environment conda env create -f environment.yml -p $HOME /conda_environments/cdds-X.Y.Z where X.Y.Z is the new version number of CDDS Note This has been updated following the roll out of Conda to MO systems. If the -p option is omitted then the installation will end up under $HOME/.conda and will not be visible to other users. Info If the wheel installation fails then you can end up with #!python rather than the full paths \u2013 this is known to be caused by not having _DEV updated in the packages, possibly due to tagging without pulling the release branch from the repository first Activate environment and set CDDS_ENV_COMMAND variable: conda activate cdds-X.Y.Z conda env config vars set CDDS_ENV_COMMAND = \"conda activate $HOME /conda_environments/cdds-X.Y.Z\" where X.Y.Z is the new version number of CDDS ``` Set platform in the CDDS_PLATFORM variable: conda activate cdds-X.Y.Z conda env config vars set CDDS_PLATFORM = AZURE Confirm environment variables: # get out of cdds environment conda deactivate # load environment again conda activate cdds-X.Y.Z # print environment variables echo $LC_ALL echo $CDDS_ENV_COMMAND echo $CDDS_PARTITION You should see en_GB.UTF-8 for LC_ALL plus the command set above for CDDS_ENV_COMMAND and the CDDS_PARTITION . Note This installation process requires a Conda installation of at least v4.9 , earlier versions will raise errors around the setting of environment variables as part of the Conda environment creation commands Login to the cdds account (please see Science Shared Accounts for more details): bash xsudo -u cdds bash -l Activate Conda mamba environment (to make installation quicker): . $HOME /software/miniconda3/bin/activate mamba Obtain environment file: wget https://github.com/MetOffice/CDDS/blob/<tagname>/environment.yml or download it manually from Github. Update locations pointed to within the environment file: sed -i \"s/<location>/X.Y.Z/\" environment.yml Create environment mamba env create -f environment.yml -p $HOME /software/miniconda3/envs/cdds-X.Y.Z where X.Y.Z is the new version number of CDDS Note This has been updated following the roll out of Conda to MO systems. If the -p option is omitted then the installation will end up under $HOME/.conda and will not be visible to other users. Info If the wheel installation fails then you can end up with #!python rather than the full paths \u2013 this is known to be caused by not having _DEV updated in the packages, possibly due to tagging without pulling the release branch from the repository first Activate environment and set CDDS_ENV_COMMAND variable: conda activate cdds-X.Y.Z conda env config vars set CDDS_ENV_COMMAND = \"source $HOME /software/miniconda3/bin/activate cdds-X.Y.Z\" where X.Y.Z is the new version number of CDDS ``` Set platform in the CDDS_PLATFORM variable: conda activate cdds-X.Y.Z conda env config vars set CDDS_PLATFORM = EXETER Confirm environment variables: # get out of cdds environment conda deactivate # load environment again conda activate cdds-X.Y.Z # print environment variables echo $LC_ALL echo $CDDS_ENV_COMMAND echo $CDDS_PARTITION You should see en_GB.UTF-8 for LC_ALL plus the command set above for CDDS_ENV_COMMAND and the CDDS_PARTITION . Warning This section will be updated soon Info The recommended location for installation is miniconda3 environment under /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3 Login to the sciX JASMIN server. Activate conda without loading an environment . /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3/bin/activate Obtain environment file wget https://github.com/MetOffice/CDDS/blob/<tagname>/environment.yml or download it manually from Github Update locations pointed to within the environment file: sed -i \"s/<location>/X.Y.Z/\" environment.yml Create environment conda env create -f environment.yml -n cdds-X.Y.Z where X.Y.Z is the new version number of CDDS Activate environment and set CDDS_ENV_COMMAND variable conda activate cdds-X.Y.Z conda env config vars set CDDS_ENV_COMMAND = \"source /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3/bin/activate cdds-X.Y.Z\" Set OS partition at CDDS_PARTITION variable conda activate cdds-X.Y.Z conda env config vars set CDDS_PARTITION = Jasmin Install nco library conda install -c conda-forge nco Confirm environment variables: # get out of cdds environment conda deactivate # load environment again conda activate cdds-X.Y.Z # print environment variables echo $LC_ALL echo $CDDS_ENV_COMMAND echo $CDDS_PARITION You should see en_GB.UTF-8 for LC_ALL and the command set above for CDDS_ENV_COMMAND and CDDS_PARTITION .","title":"Installation"},{"location":"developer_documentation/deployment/cdds_installation/#cdds-installation","text":"Warning This page is still work-in-progress! Please work closely with the CDDS team when installing CDDS. On Azure (Met Office) On Premise (Met Office) On Jasmin Note This installation process requires a Conda installation of at least v4.9 , earlier versions will raise errors around the setting of environment variables as part of the Conda environment creation commands Login to the cdds account (please see Science Shared Accounts for more details): bash xsudo -u cdds bash -l Activate Conda environment (to make installation quicker): conda activate Obtain environment file: wget https://github.com/MetOffice/CDDS/blob/<tagname>/environment.yml or download it manually from Github. Update locations pointed to within the environment file: sed -i \"s/<location>/X.Y.Z/\" environment.yml Create environment conda env create -f environment.yml -p $HOME /conda_environments/cdds-X.Y.Z where X.Y.Z is the new version number of CDDS Note This has been updated following the roll out of Conda to MO systems. If the -p option is omitted then the installation will end up under $HOME/.conda and will not be visible to other users. Info If the wheel installation fails then you can end up with #!python rather than the full paths \u2013 this is known to be caused by not having _DEV updated in the packages, possibly due to tagging without pulling the release branch from the repository first Activate environment and set CDDS_ENV_COMMAND variable: conda activate cdds-X.Y.Z conda env config vars set CDDS_ENV_COMMAND = \"conda activate $HOME /conda_environments/cdds-X.Y.Z\" where X.Y.Z is the new version number of CDDS ``` Set platform in the CDDS_PLATFORM variable: conda activate cdds-X.Y.Z conda env config vars set CDDS_PLATFORM = AZURE Confirm environment variables: # get out of cdds environment conda deactivate # load environment again conda activate cdds-X.Y.Z # print environment variables echo $LC_ALL echo $CDDS_ENV_COMMAND echo $CDDS_PARTITION You should see en_GB.UTF-8 for LC_ALL plus the command set above for CDDS_ENV_COMMAND and the CDDS_PARTITION . Note This installation process requires a Conda installation of at least v4.9 , earlier versions will raise errors around the setting of environment variables as part of the Conda environment creation commands Login to the cdds account (please see Science Shared Accounts for more details): bash xsudo -u cdds bash -l Activate Conda mamba environment (to make installation quicker): . $HOME /software/miniconda3/bin/activate mamba Obtain environment file: wget https://github.com/MetOffice/CDDS/blob/<tagname>/environment.yml or download it manually from Github. Update locations pointed to within the environment file: sed -i \"s/<location>/X.Y.Z/\" environment.yml Create environment mamba env create -f environment.yml -p $HOME /software/miniconda3/envs/cdds-X.Y.Z where X.Y.Z is the new version number of CDDS Note This has been updated following the roll out of Conda to MO systems. If the -p option is omitted then the installation will end up under $HOME/.conda and will not be visible to other users. Info If the wheel installation fails then you can end up with #!python rather than the full paths \u2013 this is known to be caused by not having _DEV updated in the packages, possibly due to tagging without pulling the release branch from the repository first Activate environment and set CDDS_ENV_COMMAND variable: conda activate cdds-X.Y.Z conda env config vars set CDDS_ENV_COMMAND = \"source $HOME /software/miniconda3/bin/activate cdds-X.Y.Z\" where X.Y.Z is the new version number of CDDS ``` Set platform in the CDDS_PLATFORM variable: conda activate cdds-X.Y.Z conda env config vars set CDDS_PLATFORM = EXETER Confirm environment variables: # get out of cdds environment conda deactivate # load environment again conda activate cdds-X.Y.Z # print environment variables echo $LC_ALL echo $CDDS_ENV_COMMAND echo $CDDS_PARTITION You should see en_GB.UTF-8 for LC_ALL plus the command set above for CDDS_ENV_COMMAND and the CDDS_PARTITION . Warning This section will be updated soon Info The recommended location for installation is miniconda3 environment under /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3 Login to the sciX JASMIN server. Activate conda without loading an environment . /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3/bin/activate Obtain environment file wget https://github.com/MetOffice/CDDS/blob/<tagname>/environment.yml or download it manually from Github Update locations pointed to within the environment file: sed -i \"s/<location>/X.Y.Z/\" environment.yml Create environment conda env create -f environment.yml -n cdds-X.Y.Z where X.Y.Z is the new version number of CDDS Activate environment and set CDDS_ENV_COMMAND variable conda activate cdds-X.Y.Z conda env config vars set CDDS_ENV_COMMAND = \"source /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3/bin/activate cdds-X.Y.Z\" Set OS partition at CDDS_PARTITION variable conda activate cdds-X.Y.Z conda env config vars set CDDS_PARTITION = Jasmin Install nco library conda install -c conda-forge nco Confirm environment variables: # get out of cdds environment conda deactivate # load environment again conda activate cdds-X.Y.Z # print environment variables echo $LC_ALL echo $CDDS_ENV_COMMAND echo $CDDS_PARITION You should see en_GB.UTF-8 for LC_ALL and the command set above for CDDS_ENV_COMMAND and CDDS_PARTITION .","title":"CDDS Installation"},{"location":"developer_documentation/deployment/documentation/","text":"Documentation CDDS uses an X.Y.Z versioning system for releases but for documentation we only publish the most recent bugfix release. For example, we would publish 1.0.0 as 1.0 and any following bugfix releases e.g., 1.0.1 would overwrite that existing 1.0 deployment. It is also not necessary to only publish the documentation as part of the release process. If documentation needs to be added or corrected this can be done independently of a release, as long as it does not reference any change or functionality that has not yet been released. Deployment Procedure Checkout the required release branch for deployment. git checkout vX.Y_release Confirm the branch is up to date. git pull Active the CDDS environment. source setup_env_for_devel Inspect the current list of deployed versions. mike list Deploy the new/updated version of the documentation. (This command should overwrite any existing existing deployments with the same name.) mike deploy X.Y Verify the new deployment works as expected. mike serve Push the local commit made by mike to the gh-pages branch to GitHub. git push origin gh-pages How to set the latest version The latest version of the documentation pages that is shown by default should be always the latest major release version e.g. 3.1 . If a new major release is done, then you need to tell mike that the latest version has be changed: Deploy the newest latest documentation: mike deploy X.Y latest --update-aliases where X.Y is the version of the major release you like to have as default e.g 3.1 Verify the default documentation is set as expected: mike serve Push the local commit by mike to the gh-pages branch to GitHub: git push origin gh-pages","title":"Documentation"},{"location":"developer_documentation/deployment/documentation/#documentation","text":"CDDS uses an X.Y.Z versioning system for releases but for documentation we only publish the most recent bugfix release. For example, we would publish 1.0.0 as 1.0 and any following bugfix releases e.g., 1.0.1 would overwrite that existing 1.0 deployment. It is also not necessary to only publish the documentation as part of the release process. If documentation needs to be added or corrected this can be done independently of a release, as long as it does not reference any change or functionality that has not yet been released.","title":"Documentation"},{"location":"developer_documentation/deployment/documentation/#deployment-procedure","text":"Checkout the required release branch for deployment. git checkout vX.Y_release Confirm the branch is up to date. git pull Active the CDDS environment. source setup_env_for_devel Inspect the current list of deployed versions. mike list Deploy the new/updated version of the documentation. (This command should overwrite any existing existing deployments with the same name.) mike deploy X.Y Verify the new deployment works as expected. mike serve Push the local commit made by mike to the gh-pages branch to GitHub. git push origin gh-pages","title":"Deployment Procedure"},{"location":"developer_documentation/deployment/documentation/#how-to-set-the-latest-version","text":"The latest version of the documentation pages that is shown by default should be always the latest major release version e.g. 3.1 . If a new major release is done, then you need to tell mike that the latest version has be changed: Deploy the newest latest documentation: mike deploy X.Y latest --update-aliases where X.Y is the version of the major release you like to have as default e.g 3.1 Verify the default documentation is set as expected: mike serve Push the local commit by mike to the gh-pages branch to GitHub: git push origin gh-pages","title":"How to set the latest version"},{"location":"developer_documentation/deployment/infrastructure/","text":"CDDS Infrastructure Warning This page is still work-in-progress!","title":"Infrastructure"},{"location":"developer_documentation/deployment/infrastructure/#cdds-infrastructure","text":"Warning This page is still work-in-progress!","title":"CDDS Infrastructure"},{"location":"developer_documentation/deployment/release_procedure/","text":"Release Procedure Inform users of the upcoming change (if necessary) Info Ask Matthew Mizielinski if it is worth announcing the new version. Make the appropriate changes to the code Changes in the CDDS Project Branch from the appropriate release branch git checkout <release_branch> git checkout -b <ticket_number>_v<tag>_release where <release_branch> is the name of the release branch. If an appropriate release branch doesn't exist, create one: git checkout main git checkout -b v<release_version>_release where <release_version> is e.g. 3.1 . Modify the CDDS code Update the development tag in cdds/cdds/__init__.py and mip_covert/mip_convert/__init__.py _DEV = False Use following command to do that: sed -i \"s/_DEV = True/_DEV = False/\" */*/__init__.py Check _NUMERICAL_VERSION in cdds/cdds/__init__.py and mip_covert/mip_convert/__init__.py . It should be set to the current release version e.g. 3.1.0 (This must include any suffixes e.g. for release candidates) Update any version numbers of dependencies that need updating in setup_env_for_cdds Build the documentation Deploy the new version of the document: mike deploy <last_major_release_version> where last_major_release_version is the last major release version, e.g 3.1 Verify the new deployment works as expected. mike serve Publish the new documentation version: git push origin gh-pages If a major version is released then the new documentation version must be set as default: mike deploy <major_release_version> latest --update-aliases --push For more information, see Documentation . If you have any doubts, please speak to Jared or Matthew. If releasing a new minor version of CDDS, e.g. 3.1.0 , update the development environment name in setup_env_for_devel to point to the new version, e.g. cdds-3.1_dev . Ensure that: All changes since the last release have been described in the relevant CHANGES.md files. These should be added as a separate commit to allow cherry picking onto main later Any new files added since the last release that do not have a .py extension are included in MANIFEST.in and setup.py . Create a pull request for the changes. After the pull request is approved, merge the changes into the release branch, but do not squash merge them . This will allow you cherry-pick release notes from the release branch into main. Warning After changing this version number, the setup script won't work until the new version has been installed centrally in the cdds account. Create a tag Using command line Using GitHub Only those that have admin permissions on the CDDS repository can create tags. Switch to the branch you want to tag (normally, the release branch) and make sure you have pulled changes on github to your local branch \u2013 failure to do this can lead to installation errors that manifest as failure to build wheels Create the tag: git tag <tagname> -a The <tagname> normally is the release version, e.g. v3.1.0 . Push the tag to the branch: git push origin <tagname> To show all tags and check if your tag is successfully created, run: git tag Info Github has a good documentation about release processes, see: Managing releases - GitHub Docs Install the code Follow the instructions provided in the CDDS installation Ensure all the tests pass in the 'real live environment' The test must be executed as cdds user export SRCDIR = $HOME /software/miniconda3/envs/cdds-X.Y.Z/lib/python3.8/site-packages/ echo \"# Executing tests for cdds:\" pytest -s $SRCDIR /cdds --doctest-modules -m 'not slow and not integration and not rabbitMQ and not data_request' pytest -s $SRCDIR /cdds -m slow pytest -s $SRCDIR /cdds -m integration pytest -s $SRCDIR /cdds -m data_request echo \"# Executing tests for mip_convert:\" pytest -s $SRCDIR /mip_convert --doctest-modules -m 'not slow and and not mappings and not superslow' pytest -s $SRCDIR /mip_convert -m mappings pytest -s $SRCDIR /mip_convert -m slow where X.Y.Z is the version number of CDDS. Info Slow unit tests for transfer and cdds_configure will display error messages to standard output. This is intentional, and does not indicate the tests fail (see transfer.tests.test_command_line.TestMainStore.test_transfer_functional_failing_moo() for details). Restore development mode and bump version Update the development tag and version number in <cdds_component>/<cdds_component>/__init__.py : _DEV = True _NUMERICAL_VERSION = '<next_version>' where <next_version> is the next minor version, e.g. 3.1.1 . Commit and push the change directly to the release branch. The commit message should be: <ticket_number>: Restore development mode. Ensure release note changes propagate into the main branch Using cherry-pick Using merge Ensure local copy of both main and release_branch are up to date. On the main branch use the git cherry-pick command to pull in just the CHANGES.md updates with release notes and commit them. If you are unable to use the cherry-pick for the changes then the following may be useful. git merge the release branch into the trunk e.g., git merge v3.1_release --no-commit Inspect the differences in the local copy of the main branch Revert any changes other than to the CHANGES.md file Commit and push changes to the main branch. Important Do not delete the release branch! (expect Matthew Mizielinski told you so) Create Release on GitHub Create a release on github from the tag. Include all major release notes and ensure that all links back to Jira work as expected. Create a discussion announcement from the release. Close Jira ticket Set the status of the Jira ticket to Done . Complete the milestone For completing the milestone, have a chat with Matthew Mizielinski which Jira epic needs to be updated or even closed. Info The list of Milestone epics can be found at the road map page in Jira.","title":"Release Procedure"},{"location":"developer_documentation/deployment/release_procedure/#release-procedure","text":"","title":"Release Procedure"},{"location":"developer_documentation/deployment/release_procedure/#inform-users-of-the-upcoming-change-if-necessary","text":"Info Ask Matthew Mizielinski if it is worth announcing the new version.","title":"Inform users of the upcoming change (if necessary)"},{"location":"developer_documentation/deployment/release_procedure/#make-the-appropriate-changes-to-the-code","text":"","title":"Make the appropriate changes to the code"},{"location":"developer_documentation/deployment/release_procedure/#changes-in-the-cdds-project","text":"","title":"Changes in the CDDS Project"},{"location":"developer_documentation/deployment/release_procedure/#branch-from-the-appropriate-release-branch","text":"git checkout <release_branch> git checkout -b <ticket_number>_v<tag>_release where <release_branch> is the name of the release branch. If an appropriate release branch doesn't exist, create one: git checkout main git checkout -b v<release_version>_release where <release_version> is e.g. 3.1 .","title":"Branch from the appropriate release branch"},{"location":"developer_documentation/deployment/release_procedure/#modify-the-cdds-code","text":"Update the development tag in cdds/cdds/__init__.py and mip_covert/mip_convert/__init__.py _DEV = False Use following command to do that: sed -i \"s/_DEV = True/_DEV = False/\" */*/__init__.py Check _NUMERICAL_VERSION in cdds/cdds/__init__.py and mip_covert/mip_convert/__init__.py . It should be set to the current release version e.g. 3.1.0 (This must include any suffixes e.g. for release candidates) Update any version numbers of dependencies that need updating in setup_env_for_cdds Build the documentation Deploy the new version of the document: mike deploy <last_major_release_version> where last_major_release_version is the last major release version, e.g 3.1 Verify the new deployment works as expected. mike serve Publish the new documentation version: git push origin gh-pages If a major version is released then the new documentation version must be set as default: mike deploy <major_release_version> latest --update-aliases --push For more information, see Documentation . If you have any doubts, please speak to Jared or Matthew. If releasing a new minor version of CDDS, e.g. 3.1.0 , update the development environment name in setup_env_for_devel to point to the new version, e.g. cdds-3.1_dev . Ensure that: All changes since the last release have been described in the relevant CHANGES.md files. These should be added as a separate commit to allow cherry picking onto main later Any new files added since the last release that do not have a .py extension are included in MANIFEST.in and setup.py . Create a pull request for the changes. After the pull request is approved, merge the changes into the release branch, but do not squash merge them . This will allow you cherry-pick release notes from the release branch into main. Warning After changing this version number, the setup script won't work until the new version has been installed centrally in the cdds account.","title":"Modify the CDDS code"},{"location":"developer_documentation/deployment/release_procedure/#create-a-tag","text":"Using command line Using GitHub Only those that have admin permissions on the CDDS repository can create tags. Switch to the branch you want to tag (normally, the release branch) and make sure you have pulled changes on github to your local branch \u2013 failure to do this can lead to installation errors that manifest as failure to build wheels Create the tag: git tag <tagname> -a The <tagname> normally is the release version, e.g. v3.1.0 . Push the tag to the branch: git push origin <tagname> To show all tags and check if your tag is successfully created, run: git tag Info Github has a good documentation about release processes, see: Managing releases - GitHub Docs","title":"Create a tag"},{"location":"developer_documentation/deployment/release_procedure/#install-the-code","text":"Follow the instructions provided in the CDDS installation","title":"Install the code"},{"location":"developer_documentation/deployment/release_procedure/#ensure-all-the-tests-pass-in-the-real-live-environment","text":"The test must be executed as cdds user export SRCDIR = $HOME /software/miniconda3/envs/cdds-X.Y.Z/lib/python3.8/site-packages/ echo \"# Executing tests for cdds:\" pytest -s $SRCDIR /cdds --doctest-modules -m 'not slow and not integration and not rabbitMQ and not data_request' pytest -s $SRCDIR /cdds -m slow pytest -s $SRCDIR /cdds -m integration pytest -s $SRCDIR /cdds -m data_request echo \"# Executing tests for mip_convert:\" pytest -s $SRCDIR /mip_convert --doctest-modules -m 'not slow and and not mappings and not superslow' pytest -s $SRCDIR /mip_convert -m mappings pytest -s $SRCDIR /mip_convert -m slow where X.Y.Z is the version number of CDDS. Info Slow unit tests for transfer and cdds_configure will display error messages to standard output. This is intentional, and does not indicate the tests fail (see transfer.tests.test_command_line.TestMainStore.test_transfer_functional_failing_moo() for details).","title":"Ensure all the tests pass in the 'real live environment'"},{"location":"developer_documentation/deployment/release_procedure/#restore-development-mode-and-bump-version","text":"Update the development tag and version number in <cdds_component>/<cdds_component>/__init__.py : _DEV = True _NUMERICAL_VERSION = '<next_version>' where <next_version> is the next minor version, e.g. 3.1.1 . Commit and push the change directly to the release branch. The commit message should be: <ticket_number>: Restore development mode.","title":"Restore development mode and bump version"},{"location":"developer_documentation/deployment/release_procedure/#ensure-release-note-changes-propagate-into-the-main-branch","text":"Using cherry-pick Using merge Ensure local copy of both main and release_branch are up to date. On the main branch use the git cherry-pick command to pull in just the CHANGES.md updates with release notes and commit them. If you are unable to use the cherry-pick for the changes then the following may be useful. git merge the release branch into the trunk e.g., git merge v3.1_release --no-commit Inspect the differences in the local copy of the main branch Revert any changes other than to the CHANGES.md file Commit and push changes to the main branch. Important Do not delete the release branch! (expect Matthew Mizielinski told you so)","title":"Ensure release note changes propagate into the main branch"},{"location":"developer_documentation/deployment/release_procedure/#create-release-on-github","text":"Create a release on github from the tag. Include all major release notes and ensure that all links back to Jira work as expected. Create a discussion announcement from the release.","title":"Create Release on GitHub"},{"location":"developer_documentation/deployment/release_procedure/#close-jira-ticket","text":"Set the status of the Jira ticket to Done .","title":"Close Jira ticket"},{"location":"developer_documentation/deployment/release_procedure/#complete-the-milestone","text":"For completing the milestone, have a chat with Matthew Mizielinski which Jira epic needs to be updated or even closed. Info The list of Milestone epics can be found at the road map page in Jira.","title":"Complete the milestone"},{"location":"developer_documentation/plugins/available_plugins/","text":"Available Plugins CIMP6 Plugin Responsible Code Base CDDS Team Link to code on GitHub The CMIP6 plugin provides all functionality needed for supporting CMIP6 projects. This is the default plugin for CDDS. When no plugin is given, the CMIP6 plugin will be loaded. The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements. CMIP6Plus Plugin Responsible Code Base CDDS Team Link to code on GitHub The CMIP6Plus plugin provides all functionality needed for supporting CMIP6Plus projects. This plugin is loaded if your MIP era in the request configuration file is CMIP6Plus . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements. CORDEX Plugin Responsible Code Base CDDS Team Link to code on GitHub The CORDEX plugin provides all functionality needed for supporting CORDEX project. It is the only plugin that handles regional models and is supported by the CDDS team. This plugin is loaded if your MIP era in the request configuration file is CORDEX . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements. GCModelDev Plugin Responsible Code Base CDDS Team Link to code on GitHub The GCModelDev plugin provides all functionality needed for supporting adhoc use of CDDS. This plugin is loaded if your MIP era in the request configuration file is GCModelDev . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements. EERIE Plugin Responsible Code Base Jon Seddon Link to code on GitHub The EERIE plugin provides all functionality needed for supporting EERIE project. This plugin is loaded if your MIP era in the request configuration file is EERIE . The plugin is implemented by Jon Seddon. ARISE Plugin Responsible Code Base Matthew Mizielinski Link to code on GitHub The ARISE plugin provides all functionality needed for supporting ARISE project. This plugin is loaded if your MIP era in the request configuration file is ARSIE and the module path is given. The plugin is implemented by Matthew Mizielinski.","title":"Available Plugins"},{"location":"developer_documentation/plugins/available_plugins/#available-plugins","text":"","title":"Available Plugins"},{"location":"developer_documentation/plugins/available_plugins/#cimp6-plugin","text":"Responsible Code Base CDDS Team Link to code on GitHub The CMIP6 plugin provides all functionality needed for supporting CMIP6 projects. This is the default plugin for CDDS. When no plugin is given, the CMIP6 plugin will be loaded. The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements.","title":"CIMP6 Plugin"},{"location":"developer_documentation/plugins/available_plugins/#cmip6plus-plugin","text":"Responsible Code Base CDDS Team Link to code on GitHub The CMIP6Plus plugin provides all functionality needed for supporting CMIP6Plus projects. This plugin is loaded if your MIP era in the request configuration file is CMIP6Plus . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements.","title":"CMIP6Plus Plugin"},{"location":"developer_documentation/plugins/available_plugins/#cordex-plugin","text":"Responsible Code Base CDDS Team Link to code on GitHub The CORDEX plugin provides all functionality needed for supporting CORDEX project. It is the only plugin that handles regional models and is supported by the CDDS team. This plugin is loaded if your MIP era in the request configuration file is CORDEX . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements.","title":"CORDEX Plugin"},{"location":"developer_documentation/plugins/available_plugins/#gcmodeldev-plugin","text":"Responsible Code Base CDDS Team Link to code on GitHub The GCModelDev plugin provides all functionality needed for supporting adhoc use of CDDS. This plugin is loaded if your MIP era in the request configuration file is GCModelDev . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements.","title":"GCModelDev Plugin"},{"location":"developer_documentation/plugins/available_plugins/#eerie-plugin","text":"Responsible Code Base Jon Seddon Link to code on GitHub The EERIE plugin provides all functionality needed for supporting EERIE project. This plugin is loaded if your MIP era in the request configuration file is EERIE . The plugin is implemented by Jon Seddon.","title":"EERIE Plugin"},{"location":"developer_documentation/plugins/available_plugins/#arise-plugin","text":"Responsible Code Base Matthew Mizielinski Link to code on GitHub The ARISE plugin provides all functionality needed for supporting ARISE project. This plugin is loaded if your MIP era in the request configuration file is ARSIE and the module path is given. The plugin is implemented by Matthew Mizielinski.","title":"ARISE Plugin"},{"location":"developer_documentation/plugins/plugin_framework/","text":"Plugin Framework Overview The plugin framework was designed using an object-oriented approach. This framework allows you to customise and extend CDDS. Plugins are loaded at runtime from a specific given path. A plugin is a bundle that adds functionality to CDDS such that CDDS can handle a specific model project (like CMIP6 or ARISE). This also allows third party developers to add functionality to CDDS without having access to the source code. The CDDS plugin framework loads the specific plugin into a plugin store that stores it during runtime. This avoids excessive loading of the plugin. The CDDS code gets access to the plugin via the plugin store. The CMIP6 plugin will be loaded by default if no other plugin is given. Plugin Like mentioned, a plugin is a bundle that adds new functionality to CDDS. As a result CDDS can handle a specific model project, also not CMIP6 projects. A CDDS plugin needs to provide all functionality to handle that corresponding project. That includes information about the supported models, girds and streams. The design of a plugin is illustrated in the below diagram: The red boxes represent classes, the green ones enum classes and the yellow ones modules. The blue box represent a singleton class that has the functionality as a cache. A plugin is loaded by the plugin loader and registered to the plugin store. The plugin store caches the plugin during runtime. Each plugin contains information about the supported models, streams and grid labels. The model parameters containing the model information also contains the information about the supported grid that are specified by the grid type (ocean or atmosphere). Available Plugins Name Supported Project Developer Team CMIP6 CMIP6 projects CDDS Team Cordex Cordex projects CDDS Team GCModelDev GCModelDev projects CDDS Team ARISE ARISE projects Matthew Mizielinski","title":"Plugin Framework"},{"location":"developer_documentation/plugins/plugin_framework/#plugin-framework","text":"","title":"Plugin Framework"},{"location":"developer_documentation/plugins/plugin_framework/#overview","text":"The plugin framework was designed using an object-oriented approach. This framework allows you to customise and extend CDDS. Plugins are loaded at runtime from a specific given path. A plugin is a bundle that adds functionality to CDDS such that CDDS can handle a specific model project (like CMIP6 or ARISE). This also allows third party developers to add functionality to CDDS without having access to the source code. The CDDS plugin framework loads the specific plugin into a plugin store that stores it during runtime. This avoids excessive loading of the plugin. The CDDS code gets access to the plugin via the plugin store. The CMIP6 plugin will be loaded by default if no other plugin is given.","title":"Overview"},{"location":"developer_documentation/plugins/plugin_framework/#plugin","text":"Like mentioned, a plugin is a bundle that adds new functionality to CDDS. As a result CDDS can handle a specific model project, also not CMIP6 projects. A CDDS plugin needs to provide all functionality to handle that corresponding project. That includes information about the supported models, girds and streams. The design of a plugin is illustrated in the below diagram: The red boxes represent classes, the green ones enum classes and the yellow ones modules. The blue box represent a singleton class that has the functionality as a cache. A plugin is loaded by the plugin loader and registered to the plugin store. The plugin store caches the plugin during runtime. Each plugin contains information about the supported models, streams and grid labels. The model parameters containing the model information also contains the information about the supported grid that are specified by the grid type (ocean or atmosphere).","title":"Plugin"},{"location":"developer_documentation/plugins/plugin_framework/#available-plugins","text":"Name Supported Project Developer Team CMIP6 CMIP6 projects CDDS Team Cordex Cordex projects CDDS Team GCModelDev GCModelDev projects CDDS Team ARISE ARISE projects Matthew Mizielinski","title":"Available Plugins"},{"location":"developer_documentation/request/request/","text":"Request Architecture Idea: There is a request object that represents the corresponding request configuration file. Each section in the request configuration file is represented by a request section object: Inheritance contains setting to load template for given request Metadata contains all metadata settings like the model ID or the MIP era Common contains common setting like the path to the root data folder or the path to the external plugin. Data contains all settings that are used to archive the data in MASS. Inventory contains all inventory settings. Conversion contains settings that specify how CDDS is run, e.g., skip any steps when running CDDS. Global Attributes contains all attributes that will be in the global attributes section of the CMOR file. Misc contains any settings that do not fit in any other section. There is a class for each section that inherited from a abstract section class that force the implementation of all necessary functionalities. The Inheritance section has a single entry template that indicates a request file to inherit settings from. Parameters in the request file will override those defined in the template file. This allows complicated sets of information, e.g. license text that has to be precisely defined, to be held in one place and then \"inherited\" by request files keeping them relatively simple. Implementation: Classes The request class represents the request configuration. It contains all methods and functionality to modify and manipulate the request configuration. The request class contains for each section a corresponding section object. A section class must inherit by the abstract section class that specifies the methods that should be provided to create and update a particular section. The section classes are implemented as data classes. Data classes are used to easily show the developer what settings can be set in the section. The following class diagram gives an idea of the implementation: A request object can be created from a request configuration or from a rose_suite.info . Also, a section can be created from a request configuration or from a rose_suite.info . All settings items of a section can be asked for. The request object also provides a method to write the configuration into an output configuration file. To add all settings to this output configuration file, each settings object provides a method to add the settings to a configuration parser. An abstract section class is used to make sure that all section objects provide the necessary methods. Modules There is a request.py module that contains the implementation of the request object and the methods to create and manipulate this object. The abstract section class is implemented in its own module request_section.py . This module also contains common methods for a specific section class. Each section class has its own module with *_section.py as name pattern (e.g. common_section.py ). For validations of the request, a validation.py module is provided. Following diagram shows the modules and their dependencies:","title":"Request Architecture"},{"location":"developer_documentation/request/request/#request-architecture","text":"","title":"Request Architecture"},{"location":"developer_documentation/request/request/#idea","text":"There is a request object that represents the corresponding request configuration file. Each section in the request configuration file is represented by a request section object: Inheritance contains setting to load template for given request Metadata contains all metadata settings like the model ID or the MIP era Common contains common setting like the path to the root data folder or the path to the external plugin. Data contains all settings that are used to archive the data in MASS. Inventory contains all inventory settings. Conversion contains settings that specify how CDDS is run, e.g., skip any steps when running CDDS. Global Attributes contains all attributes that will be in the global attributes section of the CMOR file. Misc contains any settings that do not fit in any other section. There is a class for each section that inherited from a abstract section class that force the implementation of all necessary functionalities. The Inheritance section has a single entry template that indicates a request file to inherit settings from. Parameters in the request file will override those defined in the template file. This allows complicated sets of information, e.g. license text that has to be precisely defined, to be held in one place and then \"inherited\" by request files keeping them relatively simple.","title":"Idea:"},{"location":"developer_documentation/request/request/#implementation","text":"","title":"Implementation:"},{"location":"developer_documentation/request/request/#classes","text":"The request class represents the request configuration. It contains all methods and functionality to modify and manipulate the request configuration. The request class contains for each section a corresponding section object. A section class must inherit by the abstract section class that specifies the methods that should be provided to create and update a particular section. The section classes are implemented as data classes. Data classes are used to easily show the developer what settings can be set in the section. The following class diagram gives an idea of the implementation: A request object can be created from a request configuration or from a rose_suite.info . Also, a section can be created from a request configuration or from a rose_suite.info . All settings items of a section can be asked for. The request object also provides a method to write the configuration into an output configuration file. To add all settings to this output configuration file, each settings object provides a method to add the settings to a configuration parser. An abstract section class is used to make sure that all section objects provide the necessary methods.","title":"Classes"},{"location":"developer_documentation/request/request/#modules","text":"There is a request.py module that contains the implementation of the request object and the methods to create and manipulate this object. The abstract section class is implemented in its own module request_section.py . This module also contains common methods for a specific section class. Each section class has its own module with *_section.py as name pattern (e.g. common_section.py ). For validations of the request, a validation.py module is provided. Following diagram shows the modules and their dependencies:","title":"Modules"},{"location":"operational_procedure/","text":"Operational Procedures See the documentation for CMIP6 and GCModelDev .","title":"Operational Procedures"},{"location":"operational_procedure/#operational-procedures","text":"See the documentation for CMIP6 and GCModelDev .","title":"Operational Procedures"},{"location":"operational_procedure/cmip6/","text":"Generating CMORised data with CDDS for CMIP6 / CMIP6 Plus simulations using the CDDS Workflow See also guidance for adhoc generation of CMORised data . Tip Use <script> -h or <script> --help to print information about the script, including available parameters. Example A simulation for the pre-industrial control from UKESM will be used as an example in these instructions. Prerequisites Before running the CDDS Operational Procedure, please ensure that: you own a CDDS operational simulation ticket (see the list of CDDS operational simulation tickets ) that will monitor the processing of a CMIP6 / CMIP6 Plus simulation using CDDS. you belong to the cdds group Tip type groups on the command line to print the groups a user is in you have write permissions to moose:/adhoc/projects/cdds/ on MASS Tip You can check if you have correct permissions by running following command and check if your moose username is included in the access control list output: moo getacl moose:/adhoc/projects/cdds you use a bash shell. CDDS uses Conda which can experience problems running in a shell other than bash. Tip You can check which shell you use by following command: echo $SHELL If the result is not /bin/bash , you can switch to a bash shell by running: /bin/bash If any of the above are not true please contact the CDDS Team for guidance. Packages CDDS is designed to handle a \"package\" of simulation data at one time; a set of variables from a particular simulation run. Multiple \"packages\" can be run for a given simulation to add new or corrected variables to the archive. Each package should be run using a separate processing ( proc ) and data directory. The simplest way to separate two run throughs of CDDS is to use a different package name. This is set either when running the write_request script below or by modifying the request configuration itself. Partial processing of a simulation In certain circumstances it may be desirable to process and submit a subset of an entire simulation, i.e. the first 250 years of the esm-piControl simulation. Please contact the CDDS Team to discuss this prior to starting processing to Get appropriate guidance on the steps needed to correctly construct the requested variables file in CDDS Prepare Arrange for an appropriate Errata to be issued following submission of data sets. What to do when things go wrong On occasion issues will arise with tasks performed by users of CDDS and these will trigger CRITICAL error messages in the logs and usually require user intervention. Many simple issues (MASS/MOOSE or file system problems) can be resolved by re-triggering tasks. When you take any action please ensure that you update your CDDS operational simulation ticket and if support is needed contact the CDDS Team . Set up the CDDS operational simulation ticket Select start work on the CDDS operational simulation ticket (so that the status is in_progress ) to indicate that work is starting. Activate the CDDS install MOHC JASMIN Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.3.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Note The available version numbers for this script can be found here If you wish to deactivate the CDDS environment then you can use the command conda deactivate . Create the request configuration file The request configuration file is constructed from information in the rose-suite.info files within each workflow. Important If the rose-suite.info file contains incorrect information, this will be propagated through CDDS. As such it is critically important that the information in these files is correct To construct the request configuration file take the following steps Set up a working directory mkdir cdds-example-1 cd cdds-example-1 export WORKING_DIR = ` pwd ` Add the location of your working directory to the CDDS operational simulation ticket . Collect required information on the rose workflow for the simulation; workflow id, e.g. u-aw310 branch, e.g. cdds revision Info You can find the revision of the workflow branch for a CMIP6 workflow by using the following command: rosie lookup --prefix = u --query project eq u-cmip6 and id eq u-aw310 and branch eq cdds Create the request configuration file; write_request <workflow id> <branch> <revision> <package name> [ <list of streams> ] -c <path to proc dir> -t <path to data dir> Example Create a request configuration file for the rose suite u-aw310 , branch cdds and package round-20 : write_request u-aw310 cdds 115492 round-20 ap4 ap5 ap6 onm inm \\ -c /project/cdds/proc -t /project/cdds_data Note Be careful when re-running CDDS using the same request configuration file: pre-existing data will cause problems for the extraction tasks and pre-existing logs in the proc directory may cause issues when diagnosing problems. If in doubt use a different package name in the request configuration file. Tip If necessary the start and end dates for processing can be overridden using the --start_date and --end_date arguments. Please consult with the CDDS Team if you believe this is necessary. Info The log file and request configuration file are written to the current working directory Prepare a list of variables to process Warning This method does not refer to the data request or CDDS inventory database (to check which datasets have been previously produced), so care should be taken with the choice of variables. Create a text file with the list of variables or copy and modify an existing list. Each line in the file should have the form <mip table>/<variable name>:<stream> Example For example process the variable tas for the MIP table Amon when processing the ap5 stream: Amon/tas:ap5 Set the value variable_list_file in the request configuration to the path of the created variable file. Note If you are using a workflow with the CMIP6 STASH set up then you can add the default stream to a list of variables using the command stream_mappings --varfile <filename without streams> --outfile <new file with streams> If you are not using a workflow with the CMIP6 STASH configuration then contact us for advice as this process will need to be performed by hand. Configure request configuration Important The request.cfg file contains all information that is needed to process the data through CDDS. The creation of the file does not set all values. So, it must be adjusted manually. You need to adjust your request.cfg : Open the request.cfg via a text editor, e.g. vi or gedit Following values need to be set manually: Value Description variable_list_file Path to your variables file output_mass_root Path to the moose loction where the data should be archived starts with moose: output_mass_suffix Sub-directory in MASS to used when moving data. Note Please check the other values as well and do adjustments as needed. For any help, please contact the CDDS Team . Info The MIP era ( CMIP6 or CIMP6 Plus ) you are using is defined in the value mip_era of the metdata section. Checkout and configure the CDDS workflow Run the following command after replacing values within <> : checkout_processing_workflow <name for processing workflow> \\ <path to request configuration> \\ --workflow_destination . Example Checkout the CDDS processing workflow with the name my-cdds-test and the request file location /home/foo/cdds-example-1/request.cfg : checkout_processing_workflow my-cdds-test \\ /home/foo/cdds-example-1/request.cfg \\ --workflow_destination . Info A directory containing a rose workflow will be placed in a subdirectory under the location specified in --workflow_destination . If this is not specified it will be checked out under ~/roses/ This step is optional: Set some useful environmental variables to access the CDDS directories: export CDDS_PROC_DIR = /<root_proc_dir>/<mip_era>/<mip>/<model_id>_<experiment_id>_<variant_label>/<package>/ export CDDS_DATA_DIR = /<root_data_dir>/<mip_era>/<mip>/<model_id>/<experiment_id>/<variant_label>/<package>/ ls $CDDS_PROC_DIR ls $CDDS_DATA_DIR where you must replace all values within <> . The root_proc_dir and root_data_dir are the values that has been specified in the request configuration. Example Assume: Path to the root proc directory is /home/foo/cdds-example-1/proc . Path to the root data directory is /home/foo/cdds-example-1/data . MIP era is CMIP6 and MIP CMIP . Model ID is UKESM1-0-LL for experiment piControl with variant label r1i1p1f2 and package round-1 Then the command to set the environmental variables is: export CDDS_PROC_DIR = /home/foo/cdds-example-1/data/CMIP6/CMIP/UKESM1-0-LL_piControl_r1i1p1f2/round-1/ export CDDS_DATA_DIR = /home/foo/cdds-example-1/data/CMIP6/CMIP/UKESM1-0-LL/piControl/r1i1p1f2/round-1/ Run the workflow: cd <name for processing workflow> cylc vip . Example If the name of the processing workflow is my-cdds-test , then run: cd my-cdds-test cylc vip . Info Cylc 8 is used for running the processing workflow. You can do this by running following command before running the workflow: export CYLC_VERSION = 8 Monitor conversion workflows For each stream a CDDS Convert workflow will be triggered by the processing workflow. Each of the workflows launched by CDDS Convert requires monitoring. This can be done using the command line tool cylc gui to obtain a window with an updating summary of workflows progress or equivalently the Cylc Review online tools. Conversion workflows will usually be named cdds_<workflow_base_name>_<stream> and each stream will run completely independently. If a workflow has issues, due to task failure, it will stall, and you will receive an e-mail. If you hit issues or are unsure how to proceed update the CDDS operational simulation ticket for your package with anything you believe is relevant (include the location of your working directory) and contact the CDDS Team for advice. The conversion workflows run the following steps run_extract_<stream> Extract Run CDDS Extract for this stream. Runs in long queue with a wall time of 2 days. If there are any issues with extracting data they will be reported in the job.out log file in the workflow and the $CDDS_PROC_DIR/extract/log/cdds_extract_<stream>_<date stamp>.log log file and the task will fail. The extraction task will automatically resubmit 4 times if it fails and manual intervention is required to proceed. Most issues are related to either MASS (i.e. moo commands failing), file system anomalies (failure to create files /directories) or running out of time. Identify issues either by searching for \"CRITICAL\" in the job.out logs in Cylc Review or by using grep CRITICAL $CDDS_PROC_DIR /extract/log/cdds_extract_<stream>_<date stamp>.log If the issue appears to be due to MASS issues you can re-run the failed CDDS Extract job by re-triggering the run_extract_<stream> task via the cylc gui or via the cylc command line tools: cylc trigger cdds_<workflow_base_name>_<stream> run_extract_<stream>:failed If in doubt update your CDDS operational simulation ticket and contact CDDS Team for advice. validate_extract_<stream> Extract Validation Validation of the output is now performed as a separate task from extracting it. This task will report missing or unexpected files and unreadable netcdf files. setup_output_dir_<stream> Setup Output Directory This task will create output directories for conversion output. mip_convert_<stream>_<grid group> MIP Convert Run MIP Convert to produce output files for a small time window for this simulation. Will retry up to 3 times before workflow stalls. CRITICAL issues are appended to $CDDS_PROC_DIR/convert/log/critical_issues.log . These will likely need user action to correct for. So, update your CDDS operational simulation ticket and contact CDDS Team for advice. The CRITICAL log file will not exist if there are no critical issues. A variant named mip_convert_first_<stream>_<grid group> may be launched to align the cycling dates with the concatenation processing. finaliser_<stream> MIP Convert Finaliser This ensures that concatenation tasks are launched once all MIP Convert tasks have been successfully performed for a particular time range. This step should never fail. Note If this task fails, the reason is that the adjustment of the memory and time limits failed. So, please resubmit the task. organise_files_<stream> Organise Files Re-arranges the output files on disk from a directory structure created by the MIP Convert tasks of the form $CDDS_DATA_DIR /output/<stream>_mip_convert/<YYYY-MM-DD>/<grid>/<files> to $CDDS_DATA_DIR /output/<stream>_concat/<MIP table>/<variable name>/<files> Ready for concatenation. A variation named organise_files_final_<stream> does the same thing but at the end of the conversion process. mip_concatenate_setup_<stream> MIP Concatenate Setup This step constructs a list of concatenation jobs that must be performed mip_concatenate_batch_<stream> MIP Concatenate Batch Perform the concatenation commands ( ncrcat ) required to join small files together. Runs in long queue with a wall time of 2 days and can retry up to 3 times before workflow stalls (failures are usually due to running out of time while performing a concatenation). Only one mip_concatenate_batch_<stream> task can run at one time. Issues can be identified using: grep CRITICAL $CDDS_PROC_DIR /convert/log/mip_concatenate_*.log If any critical issues arise or tasks fail update your CDDS operational simulation ticket and contact the CDDS Team for advice. Output data is written to $CDDS_DATA_DIR /output/<stream>/<MIP table>/<variable name>/<files> run_qc_<stream> Quality Check (QC) Run the QC process on output data for this stream Produces a report at: $CDDS_PROC_DIR /qualitycheck/report_<stream>_<datestamp>.json and a list of variables which pass the quality checks at: $CDDS_PROC_DIR /qualitycheck/approved_variables_<stream>_<datestamp>.txt and a log file at: $CDDS_PROC_DIR /qualitycheck/log/qc_run_and_report_<stream>_<datestamp>.log The approved variables file will have one line per successfully produced dataset of the form: <MIP table>/<variable name> ; <Directory containing files> This task will fail if any QC issues are found and will not resubmit. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. run_transfer_<stream> Transfer Archive data for variables that are marked active in the requested variables file produced by CDDS Prepare and have successfully passed the QC checks, i.e. are listed in the approved variables file. Will not automatically retry, even if failure was due to MASS/MOOSE issues. The location in MASS to which these data are archived is determined by the output_mass_suffix argument specified in the request configuration file. Task will fail if There are MASS issues: For example if the following command returns anything there has been a MASS outage and you can re-trigger the task: grep SSC_STORAGE_SYSTEM_UNAVAILABLE $CDDS_PROC_DIR /archive/log/cdds_store_<stream>_<date stamp>.log An attempt is made to archive data that already exists in MASS. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. VERY IMPORTANT Do not delete data from MASS without consultation with Matt Mizielinski . completion_<stream> Completion This is a dummy task that is the last thing to run in the workflow -- this is to allow inter workflow dependencies by allowing the CDDS workflow to monitor whether each per stream workflow has completed. If all goes well the workflow will complete, and you will receive an email confirming that the workflow has shutdown containing content of the form: Message: AUTOMATIC See: http://fcm1/cylc-review/taskjobs/<user id>/<workflow name> Prepare CDDS operational simulation ticket for review & submission Once all workflows for a particular package have completed update your CDDS operational simulation ticket confirming that the Extract, Convert, QC and Transfer tasks have been completed. Note You can check if workflows has completed by using the command cylc gscan or using the cylc review tool. Copy the request JSON file and any logs to $CDDS_PROC_DIR cp request.json *.log $CDDS_PROC_DIR / Add a comment to the CDDS operational simulation ticket specifying the archived data is ready for submission, and include the full path to your request configuration location. Select assign for review to on the CDDS operational simulation ticket (so that the status is reviewing ) and assign the CDDS operational simulation ticket to Matthew Mizielinski by selecting this name from the list. The ticket will then be reviewed according to the CDDS simulation review procedure by members of the CDDS team. Info The review script used by the CDDS team involves running the following command cdds_sim_review <path to the request configuration> checking any CRITICAL issues and following up any other anomalies. Run CDDS Teardown Once the approved ticket has been returned to you following submission, delete the contents of the data directory: cd <path to the data directory> rm -rf input output Delete all workflows used: cdds_clean <path to the request configuration> Update and close the CDDS operational simulation ticket","title":"CMIP6 and CMIP6 Plus"},{"location":"operational_procedure/cmip6/#generating-cmorised-data-with-cdds-for-cmip6-cmip6-plus-simulations-using-the-cdds-workflow","text":"See also guidance for adhoc generation of CMORised data . Tip Use <script> -h or <script> --help to print information about the script, including available parameters. Example A simulation for the pre-industrial control from UKESM will be used as an example in these instructions.","title":"Generating CMORised data with CDDS for CMIP6 / CMIP6 Plus simulations using the CDDS Workflow"},{"location":"operational_procedure/cmip6/#prerequisites","text":"Before running the CDDS Operational Procedure, please ensure that: you own a CDDS operational simulation ticket (see the list of CDDS operational simulation tickets ) that will monitor the processing of a CMIP6 / CMIP6 Plus simulation using CDDS. you belong to the cdds group Tip type groups on the command line to print the groups a user is in you have write permissions to moose:/adhoc/projects/cdds/ on MASS Tip You can check if you have correct permissions by running following command and check if your moose username is included in the access control list output: moo getacl moose:/adhoc/projects/cdds you use a bash shell. CDDS uses Conda which can experience problems running in a shell other than bash. Tip You can check which shell you use by following command: echo $SHELL If the result is not /bin/bash , you can switch to a bash shell by running: /bin/bash If any of the above are not true please contact the CDDS Team for guidance.","title":"Prerequisites"},{"location":"operational_procedure/cmip6/#packages","text":"CDDS is designed to handle a \"package\" of simulation data at one time; a set of variables from a particular simulation run. Multiple \"packages\" can be run for a given simulation to add new or corrected variables to the archive. Each package should be run using a separate processing ( proc ) and data directory. The simplest way to separate two run throughs of CDDS is to use a different package name. This is set either when running the write_request script below or by modifying the request configuration itself.","title":"Packages"},{"location":"operational_procedure/cmip6/#partial-processing-of-a-simulation","text":"In certain circumstances it may be desirable to process and submit a subset of an entire simulation, i.e. the first 250 years of the esm-piControl simulation. Please contact the CDDS Team to discuss this prior to starting processing to Get appropriate guidance on the steps needed to correctly construct the requested variables file in CDDS Prepare Arrange for an appropriate Errata to be issued following submission of data sets.","title":"Partial processing of a simulation"},{"location":"operational_procedure/cmip6/#what-to-do-when-things-go-wrong","text":"On occasion issues will arise with tasks performed by users of CDDS and these will trigger CRITICAL error messages in the logs and usually require user intervention. Many simple issues (MASS/MOOSE or file system problems) can be resolved by re-triggering tasks. When you take any action please ensure that you update your CDDS operational simulation ticket and if support is needed contact the CDDS Team .","title":"What to do when things go wrong"},{"location":"operational_procedure/cmip6/#set-up-the-cdds-operational-simulation-ticket","text":"Select start work on the CDDS operational simulation ticket (so that the status is in_progress ) to indicate that work is starting.","title":"Set up the CDDS operational simulation ticket"},{"location":"operational_procedure/cmip6/#activate-the-cdds-install","text":"MOHC JASMIN Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.3.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Note The available version numbers for this script can be found here If you wish to deactivate the CDDS environment then you can use the command conda deactivate .","title":"Activate the CDDS install"},{"location":"operational_procedure/cmip6/#create-the-request-configuration-file","text":"The request configuration file is constructed from information in the rose-suite.info files within each workflow. Important If the rose-suite.info file contains incorrect information, this will be propagated through CDDS. As such it is critically important that the information in these files is correct To construct the request configuration file take the following steps Set up a working directory mkdir cdds-example-1 cd cdds-example-1 export WORKING_DIR = ` pwd ` Add the location of your working directory to the CDDS operational simulation ticket . Collect required information on the rose workflow for the simulation; workflow id, e.g. u-aw310 branch, e.g. cdds revision Info You can find the revision of the workflow branch for a CMIP6 workflow by using the following command: rosie lookup --prefix = u --query project eq u-cmip6 and id eq u-aw310 and branch eq cdds Create the request configuration file; write_request <workflow id> <branch> <revision> <package name> [ <list of streams> ] -c <path to proc dir> -t <path to data dir> Example Create a request configuration file for the rose suite u-aw310 , branch cdds and package round-20 : write_request u-aw310 cdds 115492 round-20 ap4 ap5 ap6 onm inm \\ -c /project/cdds/proc -t /project/cdds_data Note Be careful when re-running CDDS using the same request configuration file: pre-existing data will cause problems for the extraction tasks and pre-existing logs in the proc directory may cause issues when diagnosing problems. If in doubt use a different package name in the request configuration file. Tip If necessary the start and end dates for processing can be overridden using the --start_date and --end_date arguments. Please consult with the CDDS Team if you believe this is necessary. Info The log file and request configuration file are written to the current working directory","title":"Create the request configuration file"},{"location":"operational_procedure/cmip6/#prepare-a-list-of-variables-to-process","text":"Warning This method does not refer to the data request or CDDS inventory database (to check which datasets have been previously produced), so care should be taken with the choice of variables. Create a text file with the list of variables or copy and modify an existing list. Each line in the file should have the form <mip table>/<variable name>:<stream> Example For example process the variable tas for the MIP table Amon when processing the ap5 stream: Amon/tas:ap5 Set the value variable_list_file in the request configuration to the path of the created variable file. Note If you are using a workflow with the CMIP6 STASH set up then you can add the default stream to a list of variables using the command stream_mappings --varfile <filename without streams> --outfile <new file with streams> If you are not using a workflow with the CMIP6 STASH configuration then contact us for advice as this process will need to be performed by hand.","title":"Prepare a list of variables to process"},{"location":"operational_procedure/cmip6/#configure-request-configuration","text":"Important The request.cfg file contains all information that is needed to process the data through CDDS. The creation of the file does not set all values. So, it must be adjusted manually. You need to adjust your request.cfg : Open the request.cfg via a text editor, e.g. vi or gedit Following values need to be set manually: Value Description variable_list_file Path to your variables file output_mass_root Path to the moose loction where the data should be archived starts with moose: output_mass_suffix Sub-directory in MASS to used when moving data. Note Please check the other values as well and do adjustments as needed. For any help, please contact the CDDS Team . Info The MIP era ( CMIP6 or CIMP6 Plus ) you are using is defined in the value mip_era of the metdata section.","title":"Configure request configuration"},{"location":"operational_procedure/cmip6/#checkout-and-configure-the-cdds-workflow","text":"Run the following command after replacing values within <> : checkout_processing_workflow <name for processing workflow> \\ <path to request configuration> \\ --workflow_destination . Example Checkout the CDDS processing workflow with the name my-cdds-test and the request file location /home/foo/cdds-example-1/request.cfg : checkout_processing_workflow my-cdds-test \\ /home/foo/cdds-example-1/request.cfg \\ --workflow_destination . Info A directory containing a rose workflow will be placed in a subdirectory under the location specified in --workflow_destination . If this is not specified it will be checked out under ~/roses/ This step is optional: Set some useful environmental variables to access the CDDS directories: export CDDS_PROC_DIR = /<root_proc_dir>/<mip_era>/<mip>/<model_id>_<experiment_id>_<variant_label>/<package>/ export CDDS_DATA_DIR = /<root_data_dir>/<mip_era>/<mip>/<model_id>/<experiment_id>/<variant_label>/<package>/ ls $CDDS_PROC_DIR ls $CDDS_DATA_DIR where you must replace all values within <> . The root_proc_dir and root_data_dir are the values that has been specified in the request configuration. Example Assume: Path to the root proc directory is /home/foo/cdds-example-1/proc . Path to the root data directory is /home/foo/cdds-example-1/data . MIP era is CMIP6 and MIP CMIP . Model ID is UKESM1-0-LL for experiment piControl with variant label r1i1p1f2 and package round-1 Then the command to set the environmental variables is: export CDDS_PROC_DIR = /home/foo/cdds-example-1/data/CMIP6/CMIP/UKESM1-0-LL_piControl_r1i1p1f2/round-1/ export CDDS_DATA_DIR = /home/foo/cdds-example-1/data/CMIP6/CMIP/UKESM1-0-LL/piControl/r1i1p1f2/round-1/ Run the workflow: cd <name for processing workflow> cylc vip . Example If the name of the processing workflow is my-cdds-test , then run: cd my-cdds-test cylc vip . Info Cylc 8 is used for running the processing workflow. You can do this by running following command before running the workflow: export CYLC_VERSION = 8","title":"Checkout and configure the CDDS workflow"},{"location":"operational_procedure/cmip6/#monitor-conversion-workflows","text":"For each stream a CDDS Convert workflow will be triggered by the processing workflow. Each of the workflows launched by CDDS Convert requires monitoring. This can be done using the command line tool cylc gui to obtain a window with an updating summary of workflows progress or equivalently the Cylc Review online tools. Conversion workflows will usually be named cdds_<workflow_base_name>_<stream> and each stream will run completely independently. If a workflow has issues, due to task failure, it will stall, and you will receive an e-mail. If you hit issues or are unsure how to proceed update the CDDS operational simulation ticket for your package with anything you believe is relevant (include the location of your working directory) and contact the CDDS Team for advice. The conversion workflows run the following steps run_extract_<stream> Extract Run CDDS Extract for this stream. Runs in long queue with a wall time of 2 days. If there are any issues with extracting data they will be reported in the job.out log file in the workflow and the $CDDS_PROC_DIR/extract/log/cdds_extract_<stream>_<date stamp>.log log file and the task will fail. The extraction task will automatically resubmit 4 times if it fails and manual intervention is required to proceed. Most issues are related to either MASS (i.e. moo commands failing), file system anomalies (failure to create files /directories) or running out of time. Identify issues either by searching for \"CRITICAL\" in the job.out logs in Cylc Review or by using grep CRITICAL $CDDS_PROC_DIR /extract/log/cdds_extract_<stream>_<date stamp>.log If the issue appears to be due to MASS issues you can re-run the failed CDDS Extract job by re-triggering the run_extract_<stream> task via the cylc gui or via the cylc command line tools: cylc trigger cdds_<workflow_base_name>_<stream> run_extract_<stream>:failed If in doubt update your CDDS operational simulation ticket and contact CDDS Team for advice. validate_extract_<stream> Extract Validation Validation of the output is now performed as a separate task from extracting it. This task will report missing or unexpected files and unreadable netcdf files. setup_output_dir_<stream> Setup Output Directory This task will create output directories for conversion output. mip_convert_<stream>_<grid group> MIP Convert Run MIP Convert to produce output files for a small time window for this simulation. Will retry up to 3 times before workflow stalls. CRITICAL issues are appended to $CDDS_PROC_DIR/convert/log/critical_issues.log . These will likely need user action to correct for. So, update your CDDS operational simulation ticket and contact CDDS Team for advice. The CRITICAL log file will not exist if there are no critical issues. A variant named mip_convert_first_<stream>_<grid group> may be launched to align the cycling dates with the concatenation processing. finaliser_<stream> MIP Convert Finaliser This ensures that concatenation tasks are launched once all MIP Convert tasks have been successfully performed for a particular time range. This step should never fail. Note If this task fails, the reason is that the adjustment of the memory and time limits failed. So, please resubmit the task. organise_files_<stream> Organise Files Re-arranges the output files on disk from a directory structure created by the MIP Convert tasks of the form $CDDS_DATA_DIR /output/<stream>_mip_convert/<YYYY-MM-DD>/<grid>/<files> to $CDDS_DATA_DIR /output/<stream>_concat/<MIP table>/<variable name>/<files> Ready for concatenation. A variation named organise_files_final_<stream> does the same thing but at the end of the conversion process. mip_concatenate_setup_<stream> MIP Concatenate Setup This step constructs a list of concatenation jobs that must be performed mip_concatenate_batch_<stream> MIP Concatenate Batch Perform the concatenation commands ( ncrcat ) required to join small files together. Runs in long queue with a wall time of 2 days and can retry up to 3 times before workflow stalls (failures are usually due to running out of time while performing a concatenation). Only one mip_concatenate_batch_<stream> task can run at one time. Issues can be identified using: grep CRITICAL $CDDS_PROC_DIR /convert/log/mip_concatenate_*.log If any critical issues arise or tasks fail update your CDDS operational simulation ticket and contact the CDDS Team for advice. Output data is written to $CDDS_DATA_DIR /output/<stream>/<MIP table>/<variable name>/<files> run_qc_<stream> Quality Check (QC) Run the QC process on output data for this stream Produces a report at: $CDDS_PROC_DIR /qualitycheck/report_<stream>_<datestamp>.json and a list of variables which pass the quality checks at: $CDDS_PROC_DIR /qualitycheck/approved_variables_<stream>_<datestamp>.txt and a log file at: $CDDS_PROC_DIR /qualitycheck/log/qc_run_and_report_<stream>_<datestamp>.log The approved variables file will have one line per successfully produced dataset of the form: <MIP table>/<variable name> ; <Directory containing files> This task will fail if any QC issues are found and will not resubmit. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. run_transfer_<stream> Transfer Archive data for variables that are marked active in the requested variables file produced by CDDS Prepare and have successfully passed the QC checks, i.e. are listed in the approved variables file. Will not automatically retry, even if failure was due to MASS/MOOSE issues. The location in MASS to which these data are archived is determined by the output_mass_suffix argument specified in the request configuration file. Task will fail if There are MASS issues: For example if the following command returns anything there has been a MASS outage and you can re-trigger the task: grep SSC_STORAGE_SYSTEM_UNAVAILABLE $CDDS_PROC_DIR /archive/log/cdds_store_<stream>_<date stamp>.log An attempt is made to archive data that already exists in MASS. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. VERY IMPORTANT Do not delete data from MASS without consultation with Matt Mizielinski . completion_<stream> Completion This is a dummy task that is the last thing to run in the workflow -- this is to allow inter workflow dependencies by allowing the CDDS workflow to monitor whether each per stream workflow has completed. If all goes well the workflow will complete, and you will receive an email confirming that the workflow has shutdown containing content of the form: Message: AUTOMATIC See: http://fcm1/cylc-review/taskjobs/<user id>/<workflow name>","title":"Monitor conversion workflows"},{"location":"operational_procedure/cmip6/#prepare-cdds-operational-simulation-ticket-for-review-submission","text":"Once all workflows for a particular package have completed update your CDDS operational simulation ticket confirming that the Extract, Convert, QC and Transfer tasks have been completed. Note You can check if workflows has completed by using the command cylc gscan or using the cylc review tool. Copy the request JSON file and any logs to $CDDS_PROC_DIR cp request.json *.log $CDDS_PROC_DIR / Add a comment to the CDDS operational simulation ticket specifying the archived data is ready for submission, and include the full path to your request configuration location. Select assign for review to on the CDDS operational simulation ticket (so that the status is reviewing ) and assign the CDDS operational simulation ticket to Matthew Mizielinski by selecting this name from the list. The ticket will then be reviewed according to the CDDS simulation review procedure by members of the CDDS team. Info The review script used by the CDDS team involves running the following command cdds_sim_review <path to the request configuration> checking any CRITICAL issues and following up any other anomalies.","title":"Prepare CDDS operational simulation ticket for review &amp; submission"},{"location":"operational_procedure/cmip6/#run-cdds-teardown","text":"Once the approved ticket has been returned to you following submission, delete the contents of the data directory: cd <path to the data directory> rm -rf input output Delete all workflows used: cdds_clean <path to the request configuration> Update and close the CDDS operational simulation ticket","title":"Run CDDS Teardown"},{"location":"operational_procedure/gcmodeldev/","text":"Generating CMORised data with CDDS for GCModelDev simulations using the CDDS Workflow See also guidance for CMIP6 / CMIP6 Plus generation of CMORised data . Tip Use <script> -h or <script> --help to print information about the script, including available parameters. Example A simulation for the pre-industrial control from HadGEM3 will be used as an example in these instructions. Note The procedure below assumes that you are keeping track of progress using a CDDS progress ticket . For exercises such as CMIP6 this was managed centrally, but as GCModelDev is intended to provide support for ad-hoc processing we recommend you have some form of progress note, you are welcome to use the CDDS Trac for this purpose if you wish. Prerequisites Before running the CDDS Operational Procedure, please ensure that: you have a project framework to work within (see next section) you use a bash shell. CDDS uses Conda which can experience problems running in a shell other than bash. Tip You can check which shell you use by following command: echo $SHELL If the result is not /bin/bash , you can switch to a bash shell by running: /bin/bash If any of the above are not true please contact the CDDS Team for guidance. What to do when things go wrong On occasion issues will arise with tasks performed by users of CDDS and these will trigger CRITICAL error messages in the logs and usually require user intervention. Many simple issues (MASS/MOOSE or file system problems) can be resolved by re-triggering tasks. Support is available via the CDDS Team . The project framework The tools we have were written primarily for CMIP6 to CMORise HadGEM3 and UKESM1 model output, but can be applied to other projects provided an appropriate set of variable and metadata definitions are available. Defining a new project, e.g. CMIP6, requires a reasonable amount of information as does adding an entirely new model configuration and the CDDS team should be involved in discussions to do this if you need it. However, it is straightforward to use an existing project and include new activities and experiments. When run in relaxed mode CDDS will allow you to use any value for the mip (activity id) and experiment_id . We have a general purpose project for users interested in CMORising data for adhoc use called GCModelDev which takes the CMIP6 variable definitions and standards, to which we can add new variables as required. This is not intended for preparing data for immediate publication to locations such as ESGF, but can be used for analysis alongside CMIP6 data and for feeding in to tools that base themselves on the same data structure/standards. Activate the CDDS install MOHC JASMIN Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS progress ticket . Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS progress ticket . Note The available version numbers for this script can be found here (MetOffice github access required) If you wish to deactivate the CDDS environment then you can use the command conda deactivate . Create the request configuration file Info Please, also the documentation of the request configuration . A request configuration file contains a number of fields that guide what CDDS processing does and can be viewed as a \"control\" file with a reasonable number of arguments. The simplest approach is to copy an existing file and edit certain fields. Examples: GCModelDev HadGEM3-GC31-LL GCModelDev HadGEM3-GC31-LL using ens class GCModelDev HadGEM3-GC31-MM GCModelDev UKESM1-0-LL These request files should be suitable for use both within the Met Office and on JASMIN. If you are working with a particular model then to set up a new CDDS processing \"package\", the user would need to alter the experiment_id and/or variant_label fields, possibly the mip, and the workflow_id along with a set of streams Important Check that the mode in the common section of the request configuration file is set to relaxed . In relaxed mode CDDS will allow you to use any value for the mip (activity id) and experiment_id . Check that the mip_era in the metadata section of the request configuration file is set to GCModelDev . You also need to set following values manually: Value Description root_proc_dir Path to the CDDS proc directory root_data_dir Path to the CDDS data directory output_mass_root Path to the moose location where the data should be archived starts with moose: output_mass_suffix Sub-directory in MASS to used when moving data. Info The CDDS data directory is the directory where the model output files are written to. The CDDS proc directory is the directory where all the non-data outputs are written to, like the log files. Note Please check the other values, as well and do adjustments as needed. For any help, please contact the CDDS Team . Prepare a list of variables to process Create a text file with the list of variables or copy and modify an existing list. Each line in the file should have the form <mip table>/<variable name>:<stream> Example For example process the variable tas for the MIP table Amon when processing the ap5 stream: Amon/tas:ap5 Set the value variable_list_file in the request configuration to the path of the created variable file. Checkout and configure the CDDS workflow Run the following command after replacing values within <> : checkout_processing_workflow <name for processing workflow> \\ <path to request configuration> \\ --workflow_destination . Example Checkout the CDDS processing workflow with the name my-cdds-test and the request file location /home/foo/cdds-example-1/request.cfg : checkout_processing_workflow my-cdds-test \\ /home/foo/cdds-example-1/request.cfg \\ --workflow_destination . Info A directory containing a rose workflow will be placed in a subdirectory under the location specified in --workflow_destination . If this is not specified it will be checked out under ~/roses/ This step is optional: Set some useful environmental variables to access the CDDS directories: export CDDS_PROC_DIR = /<root_proc_dir>/<mip_era>/<mip>/<model_id>_<experiment_id>_<variant_label>/<package>/ export CDDS_DATA_DIR = /<root_data_dir>/<mip_era>/<mip>/<model_id>/<experiment_id>/<variant_label>/<package>/ ls $CDDS_PROC_DIR ls $CDDS_DATA_DIR where you must replace all values within <> . The root_proc_dir and root_data_dir are the values that has been specified in the request configuration. Run the workflow: cd <name for processing workflow> cylc vip . Info Cylc 8 is used for running the processing workflow. If your default version of cylc is not cylc 8 (run cylc --version to check) you will need to run the following command before running the workflow: export CYLC_VERSION = 8 Monitor conversion workflows For each stream a CDDS Convert workflow will be triggered by the processing workflow. Each of the workflows launched by CDDS Convert requires monitoring. This can be done using the command line tool cylc gui to obtain a window with an updating summary of workflows progress or equivalently the Cylc Review online tools. Conversion workflows will usually be named cdds_<model id>_<experiment id>_<variant_label>_<stream> and each stream will run completely independently. If a workflow has issues, due to task failure, it will stall, and you will receive an e-mail. If you hit issues or are unsure how to proceed update the CDDS progress ticket for your package with anything you believe is relevant (include the location of your working directory) and contact the CDDS Team for advice. The conversion workflows run the following steps run_extract_<stream> Extract Run CDDS Extract for this stream. Runs in long queue with a wall time of 2 days. If there are any issues with extracting data they will be reported in the job.err log file in the workflow and the $CDDS_PROC_DIR/extract/log/cdds_extract_<stream>_<date stamp>.log log file and the task will fail. The extraction task will automatically resubmit 4 times if it fails and manual intervention is required to proceed. Most issues are related to either MASS (i.e. moo commands failing), file system anomalies (failure to create files /directories) or running out of time. Identify issues either by searching for \"CRITICAL\" in the job.err logs in Cylc Review or by using grep CRITICAL $CDDS_PROC_DIR /extract/log/cdds_extract_<stream>_<date stamp>.log If the issue appears to be due to MASS issues you can re-run the failed CDDS Extract job by re-triggering the run_extract_<stream> task via the cylc gui or via the cylc command line tools: cylc trigger cdds_<model id>_<experiment id>_<variant label>_<stream> run_extract_<stream>:failed If in doubt update your CDDS progress ticket and contact CDDS Team for advice. validate_extract_<stream> Extract Validation Validation of the output is now performed as a separate task from extracting it. This task will report missing or unexpected files and unreadable netcdf files. setup_output_dir_<stream> Setup Output Directory This task will create output directories for conversion output. mip_convert_<stream>_<grid group> MIP Convert Run MIP Convert to produce output files for a small time window for this simulation. Will retry up to 3 times before workflow stalls. CRITICAL issues are appended to $CDDS_PROC_DIR/convert/log/critical_issues.log . These will likely need user action to correct for. So, update your CDDS progress ticket and contact CDDS Team for advice. The CRITICAL log file will not exist if there are no critical issues. A variant named mip_convert_first_<stream>_<grid group> may be launched to align the cycling dates with the concatenation processing. finaliser_<stream> MIP Convert Finaliser This ensures that concatenation tasks are launched once all MIP Convert tasks have been successfully performed for a particular time range. This step should never fail. organise_files_<stream> Organise Files Re-arranges the output files on disk from a directory structure created by the MIP Convert tasks of the form $CDDS_DATA_DIR /output/<stream>_mip_convert/<YYYY-MM-DD>/<grid>/<files> to $CDDS_DATA_DIR /output/<stream>_concat/<MIP table>/<variable name>/<files> Ready for concatenation. A variation named organise_files_final_<stream> does the same thing but at the end of the conversion process. mip_concatenate_setup_<stream> MIP Concatenate Setup This step constructs a list of concatenation jobs that must be performed mip_concatenate_batch_<stream> MIP Concatenate Batch Perform the concatenation commands ( ncrcat ) required to join small files together. Runs in long queue with a wall time of 2 days and can retry up to 3 times before workflow stalls (failures are usually due to running out of time while performing a concatenation). Only one mip_concatenate_batch_<stream> task can run at one time. Issues can be identified using: grep CRITICAL $CDDS_PROC_DIR /convert/log/mip_concatenate_*.log If any critical issues arise or tasks fail update your CDDS progress ticket and contact the CDDS Team for advice. Output data is written to $CDDS_DATA_DIR /output/<stream>/<MIP table>/<variable name>/<files> run_qc_<stream> Quality Check (QC) Run the QC process on output data for this stream Produces a report at: $CDDS_PROC_DIR /qualitycheck/report_<stream>_<datestamp>.json and a list of variables which pass the quality checks at: $CDDS_PROC_DIR /qualitycheck/approved_variables_<stream>_<datestamp>.txt and a log file at: $CDDS_PROC_DIR /qualitycheck/log/qc_run_and_report_<stream>_<datestamp>.log The approved variables file will have one line per successfully produced dataset of the form: <MIP table>/<variable name> ; <Directory containing files> This task will fail if any QC issues are found and will not resubmit. If this occurs please update your CDDS progress ticket and contact the CDDS Team for advice. run_transfer_<stream> Transfer Archive data for variables that are marked active in the requested variables file produced by CDDS Prepare and have successfully passed the QC checks, i.e. are listed in the approved variables file. Will not automatically retry, even if failure was due to MASS/MOOSE issues. The location in MASS to which these data are archived is determined by the output_mass_suffix argument specified in the request configuration file. Task will fail if There are MASS issues: For example if the following command returns anything there has been a MASS outage and you can re-trigger the task: grep SSC_STORAGE_SYSTEM_UNAVAILABLE $CDDS_PROC_DIR /archive/log/cdds_store_<stream>_<date stamp>.log An attempt is made to archive data that already exists in MASS. If this occurs please update your CDDS progress ticket and contact the CDDS Team for advice. VERY IMPORTANT Do not delete data from MASS without consultation with Matt Mizielinski . completion_<stream> Completion This is a dummy task that is the last thing to run in the workflow -- this is to allow inter workflow dependencies by allowing the CDDS workflow to monitor whether each per stream workflow has completed. If all goes well the workflow will complete, and you will receive an email confirming that the workflow has shutdown containing content of the form: Message: AUTOMATIC See: http://fcm1/cylc-review/taskjobs/<user id>/<workflow name>","title":"GCModelDev"},{"location":"operational_procedure/gcmodeldev/#generating-cmorised-data-with-cdds-for-gcmodeldev-simulations-using-the-cdds-workflow","text":"See also guidance for CMIP6 / CMIP6 Plus generation of CMORised data . Tip Use <script> -h or <script> --help to print information about the script, including available parameters. Example A simulation for the pre-industrial control from HadGEM3 will be used as an example in these instructions. Note The procedure below assumes that you are keeping track of progress using a CDDS progress ticket . For exercises such as CMIP6 this was managed centrally, but as GCModelDev is intended to provide support for ad-hoc processing we recommend you have some form of progress note, you are welcome to use the CDDS Trac for this purpose if you wish.","title":"Generating CMORised data with CDDS for GCModelDev simulations using the CDDS Workflow"},{"location":"operational_procedure/gcmodeldev/#prerequisites","text":"Before running the CDDS Operational Procedure, please ensure that: you have a project framework to work within (see next section) you use a bash shell. CDDS uses Conda which can experience problems running in a shell other than bash. Tip You can check which shell you use by following command: echo $SHELL If the result is not /bin/bash , you can switch to a bash shell by running: /bin/bash If any of the above are not true please contact the CDDS Team for guidance.","title":"Prerequisites"},{"location":"operational_procedure/gcmodeldev/#what-to-do-when-things-go-wrong","text":"On occasion issues will arise with tasks performed by users of CDDS and these will trigger CRITICAL error messages in the logs and usually require user intervention. Many simple issues (MASS/MOOSE or file system problems) can be resolved by re-triggering tasks. Support is available via the CDDS Team .","title":"What to do when things go wrong"},{"location":"operational_procedure/gcmodeldev/#the-project-framework","text":"The tools we have were written primarily for CMIP6 to CMORise HadGEM3 and UKESM1 model output, but can be applied to other projects provided an appropriate set of variable and metadata definitions are available. Defining a new project, e.g. CMIP6, requires a reasonable amount of information as does adding an entirely new model configuration and the CDDS team should be involved in discussions to do this if you need it. However, it is straightforward to use an existing project and include new activities and experiments. When run in relaxed mode CDDS will allow you to use any value for the mip (activity id) and experiment_id . We have a general purpose project for users interested in CMORising data for adhoc use called GCModelDev which takes the CMIP6 variable definitions and standards, to which we can add new variables as required. This is not intended for preparing data for immediate publication to locations such as ESGF, but can be used for analysis alongside CMIP6 data and for feeding in to tools that base themselves on the same data structure/standards.","title":"The project framework"},{"location":"operational_procedure/gcmodeldev/#activate-the-cdds-install","text":"MOHC JASMIN Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS progress ticket . Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS progress ticket . Note The available version numbers for this script can be found here (MetOffice github access required) If you wish to deactivate the CDDS environment then you can use the command conda deactivate .","title":"Activate the CDDS install"},{"location":"operational_procedure/gcmodeldev/#create-the-request-configuration-file","text":"Info Please, also the documentation of the request configuration . A request configuration file contains a number of fields that guide what CDDS processing does and can be viewed as a \"control\" file with a reasonable number of arguments. The simplest approach is to copy an existing file and edit certain fields. Examples: GCModelDev HadGEM3-GC31-LL GCModelDev HadGEM3-GC31-LL using ens class GCModelDev HadGEM3-GC31-MM GCModelDev UKESM1-0-LL These request files should be suitable for use both within the Met Office and on JASMIN. If you are working with a particular model then to set up a new CDDS processing \"package\", the user would need to alter the experiment_id and/or variant_label fields, possibly the mip, and the workflow_id along with a set of streams Important Check that the mode in the common section of the request configuration file is set to relaxed . In relaxed mode CDDS will allow you to use any value for the mip (activity id) and experiment_id . Check that the mip_era in the metadata section of the request configuration file is set to GCModelDev . You also need to set following values manually: Value Description root_proc_dir Path to the CDDS proc directory root_data_dir Path to the CDDS data directory output_mass_root Path to the moose location where the data should be archived starts with moose: output_mass_suffix Sub-directory in MASS to used when moving data. Info The CDDS data directory is the directory where the model output files are written to. The CDDS proc directory is the directory where all the non-data outputs are written to, like the log files. Note Please check the other values, as well and do adjustments as needed. For any help, please contact the CDDS Team .","title":"Create the request configuration file"},{"location":"operational_procedure/gcmodeldev/#prepare-a-list-of-variables-to-process","text":"Create a text file with the list of variables or copy and modify an existing list. Each line in the file should have the form <mip table>/<variable name>:<stream> Example For example process the variable tas for the MIP table Amon when processing the ap5 stream: Amon/tas:ap5 Set the value variable_list_file in the request configuration to the path of the created variable file.","title":"Prepare a list of variables to process"},{"location":"operational_procedure/gcmodeldev/#checkout-and-configure-the-cdds-workflow","text":"Run the following command after replacing values within <> : checkout_processing_workflow <name for processing workflow> \\ <path to request configuration> \\ --workflow_destination . Example Checkout the CDDS processing workflow with the name my-cdds-test and the request file location /home/foo/cdds-example-1/request.cfg : checkout_processing_workflow my-cdds-test \\ /home/foo/cdds-example-1/request.cfg \\ --workflow_destination . Info A directory containing a rose workflow will be placed in a subdirectory under the location specified in --workflow_destination . If this is not specified it will be checked out under ~/roses/ This step is optional: Set some useful environmental variables to access the CDDS directories: export CDDS_PROC_DIR = /<root_proc_dir>/<mip_era>/<mip>/<model_id>_<experiment_id>_<variant_label>/<package>/ export CDDS_DATA_DIR = /<root_data_dir>/<mip_era>/<mip>/<model_id>/<experiment_id>/<variant_label>/<package>/ ls $CDDS_PROC_DIR ls $CDDS_DATA_DIR where you must replace all values within <> . The root_proc_dir and root_data_dir are the values that has been specified in the request configuration. Run the workflow: cd <name for processing workflow> cylc vip . Info Cylc 8 is used for running the processing workflow. If your default version of cylc is not cylc 8 (run cylc --version to check) you will need to run the following command before running the workflow: export CYLC_VERSION = 8","title":"Checkout and configure the CDDS workflow"},{"location":"operational_procedure/gcmodeldev/#monitor-conversion-workflows","text":"For each stream a CDDS Convert workflow will be triggered by the processing workflow. Each of the workflows launched by CDDS Convert requires monitoring. This can be done using the command line tool cylc gui to obtain a window with an updating summary of workflows progress or equivalently the Cylc Review online tools. Conversion workflows will usually be named cdds_<model id>_<experiment id>_<variant_label>_<stream> and each stream will run completely independently. If a workflow has issues, due to task failure, it will stall, and you will receive an e-mail. If you hit issues or are unsure how to proceed update the CDDS progress ticket for your package with anything you believe is relevant (include the location of your working directory) and contact the CDDS Team for advice. The conversion workflows run the following steps run_extract_<stream> Extract Run CDDS Extract for this stream. Runs in long queue with a wall time of 2 days. If there are any issues with extracting data they will be reported in the job.err log file in the workflow and the $CDDS_PROC_DIR/extract/log/cdds_extract_<stream>_<date stamp>.log log file and the task will fail. The extraction task will automatically resubmit 4 times if it fails and manual intervention is required to proceed. Most issues are related to either MASS (i.e. moo commands failing), file system anomalies (failure to create files /directories) or running out of time. Identify issues either by searching for \"CRITICAL\" in the job.err logs in Cylc Review or by using grep CRITICAL $CDDS_PROC_DIR /extract/log/cdds_extract_<stream>_<date stamp>.log If the issue appears to be due to MASS issues you can re-run the failed CDDS Extract job by re-triggering the run_extract_<stream> task via the cylc gui or via the cylc command line tools: cylc trigger cdds_<model id>_<experiment id>_<variant label>_<stream> run_extract_<stream>:failed If in doubt update your CDDS progress ticket and contact CDDS Team for advice. validate_extract_<stream> Extract Validation Validation of the output is now performed as a separate task from extracting it. This task will report missing or unexpected files and unreadable netcdf files. setup_output_dir_<stream> Setup Output Directory This task will create output directories for conversion output. mip_convert_<stream>_<grid group> MIP Convert Run MIP Convert to produce output files for a small time window for this simulation. Will retry up to 3 times before workflow stalls. CRITICAL issues are appended to $CDDS_PROC_DIR/convert/log/critical_issues.log . These will likely need user action to correct for. So, update your CDDS progress ticket and contact CDDS Team for advice. The CRITICAL log file will not exist if there are no critical issues. A variant named mip_convert_first_<stream>_<grid group> may be launched to align the cycling dates with the concatenation processing. finaliser_<stream> MIP Convert Finaliser This ensures that concatenation tasks are launched once all MIP Convert tasks have been successfully performed for a particular time range. This step should never fail. organise_files_<stream> Organise Files Re-arranges the output files on disk from a directory structure created by the MIP Convert tasks of the form $CDDS_DATA_DIR /output/<stream>_mip_convert/<YYYY-MM-DD>/<grid>/<files> to $CDDS_DATA_DIR /output/<stream>_concat/<MIP table>/<variable name>/<files> Ready for concatenation. A variation named organise_files_final_<stream> does the same thing but at the end of the conversion process. mip_concatenate_setup_<stream> MIP Concatenate Setup This step constructs a list of concatenation jobs that must be performed mip_concatenate_batch_<stream> MIP Concatenate Batch Perform the concatenation commands ( ncrcat ) required to join small files together. Runs in long queue with a wall time of 2 days and can retry up to 3 times before workflow stalls (failures are usually due to running out of time while performing a concatenation). Only one mip_concatenate_batch_<stream> task can run at one time. Issues can be identified using: grep CRITICAL $CDDS_PROC_DIR /convert/log/mip_concatenate_*.log If any critical issues arise or tasks fail update your CDDS progress ticket and contact the CDDS Team for advice. Output data is written to $CDDS_DATA_DIR /output/<stream>/<MIP table>/<variable name>/<files> run_qc_<stream> Quality Check (QC) Run the QC process on output data for this stream Produces a report at: $CDDS_PROC_DIR /qualitycheck/report_<stream>_<datestamp>.json and a list of variables which pass the quality checks at: $CDDS_PROC_DIR /qualitycheck/approved_variables_<stream>_<datestamp>.txt and a log file at: $CDDS_PROC_DIR /qualitycheck/log/qc_run_and_report_<stream>_<datestamp>.log The approved variables file will have one line per successfully produced dataset of the form: <MIP table>/<variable name> ; <Directory containing files> This task will fail if any QC issues are found and will not resubmit. If this occurs please update your CDDS progress ticket and contact the CDDS Team for advice. run_transfer_<stream> Transfer Archive data for variables that are marked active in the requested variables file produced by CDDS Prepare and have successfully passed the QC checks, i.e. are listed in the approved variables file. Will not automatically retry, even if failure was due to MASS/MOOSE issues. The location in MASS to which these data are archived is determined by the output_mass_suffix argument specified in the request configuration file. Task will fail if There are MASS issues: For example if the following command returns anything there has been a MASS outage and you can re-trigger the task: grep SSC_STORAGE_SYSTEM_UNAVAILABLE $CDDS_PROC_DIR /archive/log/cdds_store_<stream>_<date stamp>.log An attempt is made to archive data that already exists in MASS. If this occurs please update your CDDS progress ticket and contact the CDDS Team for advice. VERY IMPORTANT Do not delete data from MASS without consultation with Matt Mizielinski . completion_<stream> Completion This is a dummy task that is the last thing to run in the workflow -- this is to allow inter workflow dependencies by allowing the CDDS workflow to monitor whether each per stream workflow has completed. If all goes well the workflow will complete, and you will receive an email confirming that the workflow has shutdown containing content of the form: Message: AUTOMATIC See: http://fcm1/cylc-review/taskjobs/<user id>/<workflow name>","title":"Monitor conversion workflows"},{"location":"operational_procedure/sim_review/","text":"Simulation Ticket Review Procedure After a package is processed by following the operational procedure , the user assigns the ticket to a member of the CDDS team to review the processing and then perform the final step to submit the data for publication. This document describes the steps that need to be performed in the review. The check before publishing steps should also be done by the user who processed the data to check that the processing has been successful. The final publication steps can only be performed by a member of the CDDS teams with the required permission. Checklist before publication Info All the following checks can be combined in a single command by running: . ~cdds/bin/setup_env_for_cdds <version> cdds_sim_review <location of request.cfg> Check the critical issues log in the critical_issues.log for each component. See the operational procedure for more details. Info You can use following command to check for critical logs: grep -irn \"critical\" $CDDS_PROC_DIR --exclude \"*.py\" --exclude \"*.svn*\" Check the QC report in $CDDS_PROC_DIR/qualitycheck/report_<date-time>.json . The aggregated_summary and details fields should be empty. Check approved variables list that expected variables have been added. Check that variables with recently reported issues are not in the approved variables list. Check permissions for proc/archive/log . It should have write permissions for everyone, so move_in_mass can work. Info You can check the premissions for proc/archive/log by running following command: ls -l $CDDS_PROC_DIR /transfer/ | grep log The permission should be drwxrwxrwx for the log directory. Look for any partial files left over from the concatenation process using the find command: find $CDDS_DATA_DIR /output/*_* -type f Submission for publication In order to complete the submission, you will need access to the els055 server. Only members of the CDDS team are expected to do this part. After completing the pre-publication checks, add a message to the ticket to confirm that the processing was successful. Log on to els055 . Info If you have not logged on before, please see the section below on setting up els055 . Run the move_in_mass command that was generated when running cdds_sim_review . Example move_in_mass request.json -p --original_state = embargoed --new_state = available --mass_location = production --variables_list_file = approved_variables_YYYY-MM-DDThhmmss.txt When move_in_mass has completed, check for errors in the log file: $CDDS_PROC_DIR/archive/log/move_in_mass_<date-time>.log Info Use grep to search for any CRITICAL messages in the log file: grep \"CRITICAL\" $CDDS_PROC_DIR /archive/log/move_in_mass_<date-time>.log The <date-time> must be replaced to the time stamp of the log file. Confirm that the last line includes the phrase Moving complete . Check the message queue to confirm that messages are waiting for processing. Info For CMIP6, you can use following command to confirm that shows the messages are waiting for processing: list_queue CMIP6_available For CMIP6Plus, you can use following command to confirm that shows the messages are waiting for processing: list_queue CMIP6Plus_available Once you are happy that the move_in_mass command has executed successfully, add a message stating this to the ticket, change the status of the ticket to approved and reassign to the original user for teardown and closing. Setting up els055 Create a new file ~/.cdds_credentials . Contact Matt Mizielinski for the required settings.","title":"Simulation Review Procedure"},{"location":"operational_procedure/sim_review/#simulation-ticket-review-procedure","text":"After a package is processed by following the operational procedure , the user assigns the ticket to a member of the CDDS team to review the processing and then perform the final step to submit the data for publication. This document describes the steps that need to be performed in the review. The check before publishing steps should also be done by the user who processed the data to check that the processing has been successful. The final publication steps can only be performed by a member of the CDDS teams with the required permission.","title":"Simulation Ticket Review Procedure"},{"location":"operational_procedure/sim_review/#checklist-before-publication","text":"Info All the following checks can be combined in a single command by running: . ~cdds/bin/setup_env_for_cdds <version> cdds_sim_review <location of request.cfg> Check the critical issues log in the critical_issues.log for each component. See the operational procedure for more details. Info You can use following command to check for critical logs: grep -irn \"critical\" $CDDS_PROC_DIR --exclude \"*.py\" --exclude \"*.svn*\" Check the QC report in $CDDS_PROC_DIR/qualitycheck/report_<date-time>.json . The aggregated_summary and details fields should be empty. Check approved variables list that expected variables have been added. Check that variables with recently reported issues are not in the approved variables list. Check permissions for proc/archive/log . It should have write permissions for everyone, so move_in_mass can work. Info You can check the premissions for proc/archive/log by running following command: ls -l $CDDS_PROC_DIR /transfer/ | grep log The permission should be drwxrwxrwx for the log directory. Look for any partial files left over from the concatenation process using the find command: find $CDDS_DATA_DIR /output/*_* -type f","title":"Checklist before publication"},{"location":"operational_procedure/sim_review/#submission-for-publication","text":"In order to complete the submission, you will need access to the els055 server. Only members of the CDDS team are expected to do this part. After completing the pre-publication checks, add a message to the ticket to confirm that the processing was successful. Log on to els055 . Info If you have not logged on before, please see the section below on setting up els055 . Run the move_in_mass command that was generated when running cdds_sim_review . Example move_in_mass request.json -p --original_state = embargoed --new_state = available --mass_location = production --variables_list_file = approved_variables_YYYY-MM-DDThhmmss.txt When move_in_mass has completed, check for errors in the log file: $CDDS_PROC_DIR/archive/log/move_in_mass_<date-time>.log Info Use grep to search for any CRITICAL messages in the log file: grep \"CRITICAL\" $CDDS_PROC_DIR /archive/log/move_in_mass_<date-time>.log The <date-time> must be replaced to the time stamp of the log file. Confirm that the last line includes the phrase Moving complete . Check the message queue to confirm that messages are waiting for processing. Info For CMIP6, you can use following command to confirm that shows the messages are waiting for processing: list_queue CMIP6_available For CMIP6Plus, you can use following command to confirm that shows the messages are waiting for processing: list_queue CMIP6Plus_available Once you are happy that the move_in_mass command has executed successfully, add a message stating this to the ticket, change the status of the ticket to approved and reassign to the original user for teardown and closing.","title":"Submission for publication"},{"location":"operational_procedure/sim_review/#setting-up-els055","text":"Create a new file ~/.cdds_credentials . Contact Matt Mizielinski for the required settings.","title":"Setting up els055"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL/","text":"GCModelDev HadGEM3 GC31 LL request file example CDDS v3.0.x In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-experiment-id institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = HadGEM3-GC31-LL model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bg466 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"GCModelDev HadGEM3-GC31-LL Request"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL/#gcmodeldev-hadgem3-gc31-ll-request-file-example","text":"","title":"GCModelDev HadGEM3 GC31 LL request file example"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL/#cdds-v30x","text":"In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-experiment-id institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = HadGEM3-GC31-LL model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bg466 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"CDDS v3.0.x"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL_envs/","text":"GCModelDev HadGEM3 GC31 LL request file example (ens MASS data class) CDDS v3.0.x In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-1pctCO2 institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r21i1p1f3 model_id = HadGEM3-GC31-LL model_type = AOGCM AER [netcdf_global_attributes] mass_ensemble_member = r011i1p1f1 [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = ens mass_ensemble_member = r011i1p1f1 start_date = 1950-01-01T00:00:00Z model_workflow_id = u-cp832 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"GCModelDev HadGEM3-GC31-LL ensemble class Request"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL_envs/#gcmodeldev-hadgem3-gc31-ll-request-file-example-ens-mass-data-class","text":"","title":"GCModelDev HadGEM3 GC31 LL request file example (ens MASS data class)"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL_envs/#cdds-v30x","text":"In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-1pctCO2 institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r21i1p1f3 model_id = HadGEM3-GC31-LL model_type = AOGCM AER [netcdf_global_attributes] mass_ensemble_member = r011i1p1f1 [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = ens mass_ensemble_member = r011i1p1f1 start_date = 1950-01-01T00:00:00Z model_workflow_id = u-cp832 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"CDDS v3.0.x"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-MM/","text":"GCModelDev HadGEM3 GC31 MM request file example CDDS v3.0.x In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-1pctCO2 institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-MM parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = HadGEM3-GC31-MM model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bg618 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = esting # This is appended to output_mass_root [misc] atmos_timestep = 900 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"GCModelDev HadGEM3-GC31-MM Request"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-MM/#gcmodeldev-hadgem3-gc31-mm-request-file-example","text":"","title":"GCModelDev HadGEM3 GC31 MM request file example"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-MM/#cdds-v30x","text":"In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-1pctCO2 institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-MM parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = HadGEM3-GC31-MM model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bg618 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = esting # This is appended to output_mass_root [misc] atmos_timestep = 900 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"CDDS v3.0.x"},{"location":"operational_procedure/gcmodeldev_examples/UKESM1-0-LL/","text":"GCModelDev UKESM1 0 LL request file example CDDS v3.0.x In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-experiment-id institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = UKESM1-0-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = UKESM1-0-LL model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bc179 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"GCModelDev UKESM1-0-LL Request"},{"location":"operational_procedure/gcmodeldev_examples/UKESM1-0-LL/#gcmodeldev-ukesm1-0-ll-request-file-example","text":"","title":"GCModelDev UKESM1 0 LL request file example"},{"location":"operational_procedure/gcmodeldev_examples/UKESM1-0-LL/#cdds-v30x","text":"In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-experiment-id institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = UKESM1-0-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = UKESM1-0-LL model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bc179 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"CDDS v3.0.x"},{"location":"tutorials/add_mapping/","text":"Variable Mappings for CDDS The following pages are constructed from the Mappings held in MIP convert: UKESM1 Mappings HadGEM3 Mappings Mapping Hierarchy The mappings within MIP Convert are arranged in a hierarchy with names in the following forms common_mappings.cfg <mip_table>_mappings.cfg <base_model>_mappings.cfg <base_model>_<mip_table_id>_mappings.cfg <model_id>_mappings.cfg <model_id>_<mip_table_id>_mappings.cfg where <base_model> refers to the section of the model id before the first dash - , e.g. \"UKESM1\" or \"HadGEM3\" . Mappings in files furthest down this list overload those specified in files at the top. Model to MIP Mapping Configuration Files Each of the above model to MIP mapping configuration files contains the following sections. [DEFAULT] [COMMON] [variable name] The [DEFAULT] Section The [DEFAULT] section contains options that are propagated to all other sections. This is useful for setting a default value for a option for all sections. A value provided for the same option in any other section will be used for that section over the default value defined in the [DEFAULT] section. The [COMMON] Section The [COMMON] section contains options that may be used by other sections by using the syntax ${COMMON:<option>} . This is useful for setting values for comments or notes that would otherwise be repeated multiple times in the model to MIP mapping configuration files. The [variable name] Section(s) The [variable name] sections provide the model to MIP mapping corresponding to the specified MIP requested variable and contains the following required options: Required Options Description Notes dimension The dimensions of the MIP requested variable. expression The expression required to convert the input variable / input variables to the MIP requested variable. mip_table_id A space-separated list of MIP table identifiers that the model to MIP mapping is valid for. positive The direction of a vertical energy (heat) flux or surface momentum flux (stress) input; use 'up' or 'down' depending on whether the direction is positive when it is directed upward or downward, respectively. This argument is required for vertical energy and salt fluxes, for flux correction fields, and for surface stress. [1] status The status of the MIP requested variable. Valid values are ok and embargoed . 2 units The units of the data of the MIP requested variable i.e., after the expression has been applied. Notes This information is used by CMOR to determine whether a sign change is necessary to make the data consistent with the MIP requirements. For more information, please see the cmor_variable section in the CMOR Documentation. The MIP requested variables are reviewed to ensure they have been produced correctly, MIP requested variables that have not passed review will not be submitted to ESGF and so will not be available for other institutes to use. This is used by CDDS, but does not affect MIP Convert behaviour The following options are optional: Optional Options Description Notes comment The details relating to the model to MIP mapping that should be written to the output netCDF file, e.g., to qualify the details of the model to MIP mapping, add health warnings,etc. notes Any details relating to the model to MIP mapping that should not be written to the output netCDF file, e.g., who added the model to MIP mapping, why, reasons for using this model to MIP mapping over another in certain cases, any other special cases notes, etc. component A space-separated list of components that the model to MIP mapping is valid for. valid_min The minimum valid value for the data of the MIP requested variable; values in the data lower than this value are replaced with zero. Constructing an Expression Each input variable in an expression must contain one of the following: Expression Items File Type Description Notes stash PP LBUSER(4), STASH Code, see Chapter 4 (page 25) of UMDP F03 variable_name netCDF The name of the data variable in the model output files that is used to create the input variable. Example 1. One to One Mapping expression = m01s03i236 expression = sitemptop Example 2a. Constants and Arithmetic Expressions can use numerical values and constants (which must be written using upper case letters; constants are available in :mod: mip_convert.process.constants ): expression = rain_ai * 100. * SECONDS_IN_DAY Example 2b. Constants and Arithmetic For atmospheric tendency diagnostics, the atmospheric model timestep must be specified (the value of the atmospheric model timestep is obtained from the user configuration file, please see the request_section in the user_guide ): expression = m01s30i181 / ATMOS_TIMESTEP Example 3a. Constraints To specify additional constraints, use square brackets: expression = m01s08i223[blev=0.05] expression = pbo[cell_methods=time: mean (interval: 120 s)] Example 3b. Constraints Multiple values for a single constraint should be separated by spaces: expression = m01s30i201[blev=850.0 500.0 250.0] Example 3c. Constraints Multiple constraints within the square brackets should be separated by commas: expression = m01s02i204[lbplev=4, lbtim=122] Example 4. Processor In cases where it is not possible to describe the conversion of the input variable / input variables to the MIP requested variable using a basic expression like the ones above, a function can be specified: expression = my_function_name(m01s03i236) The values of the arguments of the function must follow the same syntax as the basic expression. The following constraints can currently be used in an expression : Expression Items File Type Description Notes blev PP BLEV, level, see Chapter 4 (page 26) of UMDP F03 cell_methods netCDF The cell methods depth netCDF Value of the depth coordinate lbplev PP LBUSER(5), pseudo level, see Chapter 4 (page 25) of UMDP F03 lbproc PP LBPROC, processing code, see Chapter 4 (page 21) of UMDP F03 lbtim PP LBTIM, time indicator, see Chapter 4 (page 17) of UMDP F03 lbtim_ia PP IA component of LBTIM (sampling frequency) How to Add a New Model to MIP mapping Note If you want to contribute your changes to CDDS then you must follow the development workflow. See the Developer Documentation pages for guidance. Requried Information the MIP requested variable name and the MIP table identifier. the constraints, i.e., the data to be read from the model output files to create the input variables. an expression describing how to process the input variables to produce a MIP output variable. the units of the MIP output variable after the expression has been applied. It is not necessary to include the units in model to MIP mappings used for netCDF model output files if the expression consists of a single constraint. the component, which is the domain as described \u200bhere. Determine which configuration file the model to MIP mapping should be added to Model to MIP mapping configuration files are located in the mip_convert/process sub package (their names end in _mappings.cfg ). if there is currently no entry for the MIP requested variable name in the model to MIP mapping configuration files (use, e.g. grep <mip_requested_variable_name> <branch>/mip_convert/mip_convert/process/*_mappings.cfg ), add the model to MIP mapping to common_mappings.cfg. if there is already an entry in common_mappings.cfg , add the model to MIP mapping to the appropriate ` _mappings.cfg`` configuration file. for multiple model to MIP mappings for the same MIP requested variable name, any expressions containing lbproc=128 should be added to common_mappings.cfg , while others should be added to the appropriate ` _mappings.cfg`` configuration file. if there are any issues, please ask Matthew Mizielinski. Add the model to MIP mapping to the appropriate configuration file. The sections in the model to MIP mapping configuration files are the MIP requested variable names and are listed in alphabetical order. If the expression continues beyond the 120 character line limit, add a new line before a binary operator, see \u200bPEP8. If the expression contains a function and an appropriate function does not already exist, add the function to mip_convert/process/processors.py \u200b. The function: can have any number of arguments, each corresponding to a cube. should work directly on the cube(s) (i.e., do not make a copy of the cube(s) or the data). should not update the standard_name or long_name of the cube(s) (the \u200bMIP table contains this information and will be automatically added to the output netCDF files by CMOR). must return a single cube. Test the Model to MIP Mapping Test and verify all mappings before submitting a pull request.","title":"Add a Variable Mapping"},{"location":"tutorials/add_mapping/#variable-mappings-for-cdds","text":"The following pages are constructed from the Mappings held in MIP convert: UKESM1 Mappings HadGEM3 Mappings","title":"Variable Mappings for CDDS"},{"location":"tutorials/add_mapping/#mapping-hierarchy","text":"The mappings within MIP Convert are arranged in a hierarchy with names in the following forms common_mappings.cfg <mip_table>_mappings.cfg <base_model>_mappings.cfg <base_model>_<mip_table_id>_mappings.cfg <model_id>_mappings.cfg <model_id>_<mip_table_id>_mappings.cfg where <base_model> refers to the section of the model id before the first dash - , e.g. \"UKESM1\" or \"HadGEM3\" . Mappings in files furthest down this list overload those specified in files at the top.","title":"Mapping Hierarchy"},{"location":"tutorials/add_mapping/#model-to-mip-mapping-configuration-files","text":"Each of the above model to MIP mapping configuration files contains the following sections. [DEFAULT] [COMMON] [variable name]","title":"Model to MIP Mapping Configuration Files"},{"location":"tutorials/add_mapping/#the-default-section","text":"The [DEFAULT] section contains options that are propagated to all other sections. This is useful for setting a default value for a option for all sections. A value provided for the same option in any other section will be used for that section over the default value defined in the [DEFAULT] section.","title":"The [DEFAULT] Section"},{"location":"tutorials/add_mapping/#the-common-section","text":"The [COMMON] section contains options that may be used by other sections by using the syntax ${COMMON:<option>} . This is useful for setting values for comments or notes that would otherwise be repeated multiple times in the model to MIP mapping configuration files.","title":"The [COMMON] Section"},{"location":"tutorials/add_mapping/#the-variable-name-sections","text":"The [variable name] sections provide the model to MIP mapping corresponding to the specified MIP requested variable and contains the following required options: Required Options Description Notes dimension The dimensions of the MIP requested variable. expression The expression required to convert the input variable / input variables to the MIP requested variable. mip_table_id A space-separated list of MIP table identifiers that the model to MIP mapping is valid for. positive The direction of a vertical energy (heat) flux or surface momentum flux (stress) input; use 'up' or 'down' depending on whether the direction is positive when it is directed upward or downward, respectively. This argument is required for vertical energy and salt fluxes, for flux correction fields, and for surface stress. [1] status The status of the MIP requested variable. Valid values are ok and embargoed . 2 units The units of the data of the MIP requested variable i.e., after the expression has been applied. Notes This information is used by CMOR to determine whether a sign change is necessary to make the data consistent with the MIP requirements. For more information, please see the cmor_variable section in the CMOR Documentation. The MIP requested variables are reviewed to ensure they have been produced correctly, MIP requested variables that have not passed review will not be submitted to ESGF and so will not be available for other institutes to use. This is used by CDDS, but does not affect MIP Convert behaviour The following options are optional: Optional Options Description Notes comment The details relating to the model to MIP mapping that should be written to the output netCDF file, e.g., to qualify the details of the model to MIP mapping, add health warnings,etc. notes Any details relating to the model to MIP mapping that should not be written to the output netCDF file, e.g., who added the model to MIP mapping, why, reasons for using this model to MIP mapping over another in certain cases, any other special cases notes, etc. component A space-separated list of components that the model to MIP mapping is valid for. valid_min The minimum valid value for the data of the MIP requested variable; values in the data lower than this value are replaced with zero.","title":"The [variable name] Section(s)"},{"location":"tutorials/add_mapping/#constructing-an-expression","text":"Each input variable in an expression must contain one of the following: Expression Items File Type Description Notes stash PP LBUSER(4), STASH Code, see Chapter 4 (page 25) of UMDP F03 variable_name netCDF The name of the data variable in the model output files that is used to create the input variable. Example 1. One to One Mapping expression = m01s03i236 expression = sitemptop Example 2a. Constants and Arithmetic Expressions can use numerical values and constants (which must be written using upper case letters; constants are available in :mod: mip_convert.process.constants ): expression = rain_ai * 100. * SECONDS_IN_DAY Example 2b. Constants and Arithmetic For atmospheric tendency diagnostics, the atmospheric model timestep must be specified (the value of the atmospheric model timestep is obtained from the user configuration file, please see the request_section in the user_guide ): expression = m01s30i181 / ATMOS_TIMESTEP Example 3a. Constraints To specify additional constraints, use square brackets: expression = m01s08i223[blev=0.05] expression = pbo[cell_methods=time: mean (interval: 120 s)] Example 3b. Constraints Multiple values for a single constraint should be separated by spaces: expression = m01s30i201[blev=850.0 500.0 250.0] Example 3c. Constraints Multiple constraints within the square brackets should be separated by commas: expression = m01s02i204[lbplev=4, lbtim=122] Example 4. Processor In cases where it is not possible to describe the conversion of the input variable / input variables to the MIP requested variable using a basic expression like the ones above, a function can be specified: expression = my_function_name(m01s03i236) The values of the arguments of the function must follow the same syntax as the basic expression. The following constraints can currently be used in an expression : Expression Items File Type Description Notes blev PP BLEV, level, see Chapter 4 (page 26) of UMDP F03 cell_methods netCDF The cell methods depth netCDF Value of the depth coordinate lbplev PP LBUSER(5), pseudo level, see Chapter 4 (page 25) of UMDP F03 lbproc PP LBPROC, processing code, see Chapter 4 (page 21) of UMDP F03 lbtim PP LBTIM, time indicator, see Chapter 4 (page 17) of UMDP F03 lbtim_ia PP IA component of LBTIM (sampling frequency)","title":"Constructing an Expression"},{"location":"tutorials/add_mapping/#how-to-add-a-new-model-to-mip-mapping","text":"Note If you want to contribute your changes to CDDS then you must follow the development workflow. See the Developer Documentation pages for guidance.","title":"How to Add a New Model to MIP mapping"},{"location":"tutorials/add_mapping/#requried-information","text":"the MIP requested variable name and the MIP table identifier. the constraints, i.e., the data to be read from the model output files to create the input variables. an expression describing how to process the input variables to produce a MIP output variable. the units of the MIP output variable after the expression has been applied. It is not necessary to include the units in model to MIP mappings used for netCDF model output files if the expression consists of a single constraint. the component, which is the domain as described \u200bhere.","title":"Requried Information"},{"location":"tutorials/add_mapping/#determine-which-configuration-file-the-model-to-mip-mapping-should-be-added-to","text":"Model to MIP mapping configuration files are located in the mip_convert/process sub package (their names end in _mappings.cfg ). if there is currently no entry for the MIP requested variable name in the model to MIP mapping configuration files (use, e.g. grep <mip_requested_variable_name> <branch>/mip_convert/mip_convert/process/*_mappings.cfg ), add the model to MIP mapping to common_mappings.cfg. if there is already an entry in common_mappings.cfg , add the model to MIP mapping to the appropriate ` _mappings.cfg`` configuration file. for multiple model to MIP mappings for the same MIP requested variable name, any expressions containing lbproc=128 should be added to common_mappings.cfg , while others should be added to the appropriate ` _mappings.cfg`` configuration file. if there are any issues, please ask Matthew Mizielinski.","title":"Determine which configuration file the model to MIP mapping should be added to"},{"location":"tutorials/add_mapping/#add-the-model-to-mip-mapping-to-the-appropriate-configuration-file","text":"The sections in the model to MIP mapping configuration files are the MIP requested variable names and are listed in alphabetical order. If the expression continues beyond the 120 character line limit, add a new line before a binary operator, see \u200bPEP8. If the expression contains a function and an appropriate function does not already exist, add the function to mip_convert/process/processors.py \u200b. The function: can have any number of arguments, each corresponding to a cube. should work directly on the cube(s) (i.e., do not make a copy of the cube(s) or the data). should not update the standard_name or long_name of the cube(s) (the \u200bMIP table contains this information and will be automatically added to the output netCDF files by CMOR). must return a single cube.","title":"Add the model to MIP mapping to the appropriate configuration file."},{"location":"tutorials/add_mapping/#test-the-model-to-mip-mapping","text":"Test and verify all mappings before submitting a pull request.","title":"Test the Model to MIP Mapping"},{"location":"tutorials/add_plugin/","text":"How to build your own plugin Warning The interfaces for the plugin can slightly change from version to version. Info There is a demo project that can use as reference: ~kschmatz/workspace/cdds-test-plugins Steps Create a plugin that extends from the CddsPlugin class Create classes that extends from the ModelParameters class Create classes for atmosphere grid and ocean grid that both extend from the GridInfo class Use your grid classes in your model parameters class Use your model parameters classes in your plugin Create your own grid labels Use your grid labels in your plugin Use your plugin with CDDS Preconditions Check that you, add the CDDS project (at least the cdds_common package) is in your Python path. Example To make sure that the CDDS project is in your Python path by using a script load_conda () { # load conda environment . ~cdds/software/miniconda3/bin/activate <cdds_conda_version> } setup_cdds_package () { # add cdds_common containing plugins implementation to PATH # Setup CDDS project local cdds_dir = <path-to-cdds-project> local cdds_package = \"cdds\" # Update PATH: if [ -d $cdds_dir / $cdds_package /bin ] ; then export PATH = $cdds_dir / $cdds_package /bin: $PATH fi # Update PYTHONPATH: if [ -d $cdds_dir / $cdds_package ] ; then export PYTHONPATH = $cdds_dir / $cdds_package : $PYTHONPATH fi } setup_project () { <setup-your-project> } load_conda setup_cdds_package setup_project Set <cdds_conda_version> to the conda version of the CDDS project you want to use (e.g. cdds-3.0_dev-0 ) Set the <path-to-cdds-project> to the path to the CDDS project. Replace <setup-your-project> with the commands you need to setup your project. Step 1: Create a plugin class Create a module <my>_plugin.py where <my> can be replaced with any name you like. A good choice would be the MIP era that the plugin should support. Import CddsPlugin from CDDS project: from cdds.common.plugins.plugins import CddsPlugin Create a plugin class that extends from the CddsPlugin class: class MyPlugin ( CddsPlugin ): Info If you use Pycharm, you have the benefit that you can move your cursor to the class name (here: MyPlugin ) and press ALT + Enter . You should now see an option to Implement abstract methods , select it. The methods you have to implement will be automatically add to your class. The same is with the imports that are needed. Add the __init__ method. This method must call the __init__ method of the super class with the parameter mip_era. The mip_era is the one that your plugin should support. To implement all methods you need a ModelParameters class, a GridLabel and a StreamInfo class. How to create this classes, will be explained in next sections. Example from typing import Type from cdds_common.cdds_plugins.grid import GridLabel from cdds_common.cdds_plugins.models import ModelParameters from cdds_common.cdds_plugins.plugins import CddsPlugin from cdds_common.cdds_plugins.streams import StreamInfo class MyPlugin ( CddsPlugin ): def __init__ ( self ): super ( MyPlugin , self ) . __init__ ( 'my_mip_era' ) def models_parameters ( self , model_id : str ) -> ModelParameters : pass def overload_models_parameters ( self , source_dir : str ) -> None : pass def grid_labels ( self ) -> Type [ GridLabel ]: pass def stream_info ( self ) -> StreamInfo : pass For the moment, we leave the plugin class like it is and take a look at the ModelParameters class and GridLabel class and StreamInfo class. We will come back to the plugin class later again. Step 2: Create the model parameters class You should create one model parameter class for each model you want to support. For demonstration, here we only show you to create one model parameter class. To create more classes, simple repeat the steps. Note You can use the same module or another one. In my demonstration project, I used another module to implement everything that is related to the model parameters. Import the ModelParameters class from the CDDS project: from cdds.common.plugins.models import ModelParameters Create a class that extends from the ModelParameters Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add default __init__ method. Example from typing import List , Dict from cdds_common.cdds_plugins.grid import GridType , GridInfo from cdds_common.cdds_plugins.models import ModelParameters class MyModelParams ( ModelParameters ): def __init__ ( self ): super ( MyModelParams , self ) . __init__ () @property def model_version ( self ) -> str : pass @property def data_request_version ( self ) -> str : pass @property def um_version ( self ) -> str : pass def grid_info ( self , grid_type : GridType ) -> GridInfo : pass def temp_space ( self , stream_id : str ) -> int : pass def memory ( self , stream_id : str ) -> str : pass def cycle_length ( self , stream_id : str ) -> str : pass def sizing_info ( self , frequency : str , shape : str ) -> float : pass def full_sizing_info ( self ) -> Dict [ str , Dict [ str , float ]]: pass def is_model ( self , model_id : str ) -> bool : pass def subdaily_streams ( self ) -> List [ str ]: pass Implement all method except the grid_info method. Next step is to implement the GridInfo classes. Step 3: Implement grid information classes Note You can use the same module or another one. In my demonstration project, I used another module to implement everything that is related to the grid information. Import the GridInfo class and GridType enum from the CDDS project: from cdds.common.plugins.grid import GridType , GridInfo For each grid type, there should be an own grid information class. So, create two classes that extends from the GridInfo - one for the atmosphere grid and one for the ocean grid. Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add the __init__ methods that call the super __init__ method and has a parameter the supported grid type of your grid information class. Implement the methods. Note Often the masks method is not implemented (simple: return None ) for atmosphere grids. Example class for atmosphere grid from typing import List , Dict from cdds.common.plugins.grid import GridType , GridInfo , OceanGridPolarMask class MyAtmosGridInfo ( GridInfo ): def __init__ ( self ): super ( MyAtmosGridInfo , self ) . __init__ ( GridType . ATMOS ) @property def model_info ( self ) -> str : pass @property def nominal_resolution ( self ) -> str : pass @property def longitude ( self ) -> int : pass @property def latitude ( self ) -> int : pass @property def v_latitude ( self ) -> int : pass @property def levels ( self ) -> int : pass @property def masks ( self ) -> Dict [ str , OceanGridPolarMask ]: pass @property def replacement_coordinates_file ( self ) -> str : pass def ancil_filenames ( self ) -> List [ str ]: pass def hybrid_heights_files ( self ) -> List [ str ]: pass Example class for ocean grid from typing import List , Dict from cdds.common.plugins.grid import GridType , GridInfo , OceanGridPolarMask class MyOceanGridInfo ( GridInfo ): def __init__ ( self ): super ( MyOceanGridInfo , self ) . __init__ ( GridType . OCEAN ) @property def model_info ( self ) -> str : pass @property def nominal_resolution ( self ) -> str : pass @property def longitude ( self ) -> int : pass @property def latitude ( self ) -> int : pass @property def v_latitude ( self ) -> int : pass @property def levels ( self ) -> int : pass @property def masks ( self ) -> Dict [ str , OceanGridPolarMask ]: pass @property def replacement_coordinates_file ( self ) -> str : pass def ancil_filenames ( self ) -> List [ str ]: pass def hybrid_heights_files ( self ) -> List [ str ]: git pass The next step is to use your grid information classes in your model parameters class. Step 4: Use your grid information classes in your model parameters class Implement the grid_info method in your model parameters class such that it returns the right grid information class according the grid type. Example def grid_info ( self , grid_type : GridType ) -> GridInfo : if grid_type == GridType . ATMOS : return MyAtmosGridInfo () elif grid_type == GridType . OCEAN : return MyOceanGridInfo () else : raise ValueError ( \"Unsupported grid type: {} \" . format ( GridType )) Step 5: Use your model parameters class in your plugin Go to your plugin class and implement the model_parameters method. This method should return the corresponding model parameters class according the model ID. Example def models_parameters ( self , model_id : str ) -> ModelParameters : model_params = { 'MyModel' : MyModelParams () } return model_params [ model_id ] If you allow overloading the defined parameters in your ModelParameters classes, then implement the overload_models_parameters method. The last to do for finishing the plugin is to implement the GridLabel Step 6: Implement the grid labels Import the GridLabel class from CDDS: from cdds.common.plugins.grid import GridLabel Create a class your own grid label enum class that extend from GridLabel with a __init__ method that as the arguments grid_name , label and extra_info Example class MyGridLabel ( GridLabel ): def __init__ ( self , grid_name : str , label : str , extra_info : bool ) -> None : self . grid_name = grid_name self . label = label self . extra_info = extra_info @classmethod def from_name ( cls , name : str ) -> 'GridLabel' : pass Add the grid label you want to support. Example Here, MyGridLabel should support NATIVE or NATIVE_ZONAL . class MyGridLabel ( GridLabel ): def __init__ ( self , grid_name : str , label : str , extra_info : bool ) -> None : self . grid_name = grid_name self . label = label self . extra_info = extra_info @classmethod def from_name ( cls , name : str ) -> 'GridLabel' : for grid_label in MyGridLabel : if grid_label . grid_name == name . lower (): return grid_label raise KeyError ( 'Not supported grid labels for {} ' . format ( name )) NATIVE = 'native' , 'gn' , False NATIVE_ZONAL = 'native-zonal' , 'gnz' , False Step 7: Use your grid labels in your plugin Implement the grid_labels in your plugin by simply returning your grid label enum class Example def grid_labels ( self ) -> Type [ GridLabel ]: return MyGridLabel Step 8: Implement the stream info class You should create a stream info class for all information around streams. Import the StreamInfo class from the CDDS project: from cdds.common.plugins.streams import StreamInfo Create a class that extends from the StreamInfo Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add __init__ method Implement all necesary methods Example class MyStreamInfo ( StreamInfo ): def __init__ ( self , config_path : str = '' ) -> None : super ( Cmip6StreamInfo , self ) . __init__ ( config_path ) def _load_streams ( self , configuration : Dict [ str , Any ]) -> None : pass def retrieve_stream_id ( self , variable : str , mip_table : str ) -> Tuple [ str , str ]: pass Step 9: Use your stream info in your plugin Use the stream_info in your plugin by simply returning your stream info Example def stream_info ( self ) -> StreamInfo : return MyStreamInfo () Congratulations, you finished to implement the plugin. Now, you can use it with CDDS. Step 10: Use your plugin with CDDS Add your implementation to the PYTHONPATH (and PATH ). Otherwise, CDDS will not be able to find it. Change the value of the external_plugin_location in the request.cfg file to the path to your plugin implementation Change the value of the external_plugin in the request.cfg file to the module path to your plugin Example: Change values for the ARISE plugin external_plugin_location = /project/cdds/arise external_plugin = arise.plugin","title":"Add a Plugin"},{"location":"tutorials/add_plugin/#how-to-build-your-own-plugin","text":"Warning The interfaces for the plugin can slightly change from version to version. Info There is a demo project that can use as reference: ~kschmatz/workspace/cdds-test-plugins","title":"How to build your own plugin"},{"location":"tutorials/add_plugin/#steps","text":"Create a plugin that extends from the CddsPlugin class Create classes that extends from the ModelParameters class Create classes for atmosphere grid and ocean grid that both extend from the GridInfo class Use your grid classes in your model parameters class Use your model parameters classes in your plugin Create your own grid labels Use your grid labels in your plugin Use your plugin with CDDS","title":"Steps"},{"location":"tutorials/add_plugin/#preconditions","text":"Check that you, add the CDDS project (at least the cdds_common package) is in your Python path. Example To make sure that the CDDS project is in your Python path by using a script load_conda () { # load conda environment . ~cdds/software/miniconda3/bin/activate <cdds_conda_version> } setup_cdds_package () { # add cdds_common containing plugins implementation to PATH # Setup CDDS project local cdds_dir = <path-to-cdds-project> local cdds_package = \"cdds\" # Update PATH: if [ -d $cdds_dir / $cdds_package /bin ] ; then export PATH = $cdds_dir / $cdds_package /bin: $PATH fi # Update PYTHONPATH: if [ -d $cdds_dir / $cdds_package ] ; then export PYTHONPATH = $cdds_dir / $cdds_package : $PYTHONPATH fi } setup_project () { <setup-your-project> } load_conda setup_cdds_package setup_project Set <cdds_conda_version> to the conda version of the CDDS project you want to use (e.g. cdds-3.0_dev-0 ) Set the <path-to-cdds-project> to the path to the CDDS project. Replace <setup-your-project> with the commands you need to setup your project.","title":"Preconditions"},{"location":"tutorials/add_plugin/#step-1-create-a-plugin-class","text":"Create a module <my>_plugin.py where <my> can be replaced with any name you like. A good choice would be the MIP era that the plugin should support. Import CddsPlugin from CDDS project: from cdds.common.plugins.plugins import CddsPlugin Create a plugin class that extends from the CddsPlugin class: class MyPlugin ( CddsPlugin ): Info If you use Pycharm, you have the benefit that you can move your cursor to the class name (here: MyPlugin ) and press ALT + Enter . You should now see an option to Implement abstract methods , select it. The methods you have to implement will be automatically add to your class. The same is with the imports that are needed. Add the __init__ method. This method must call the __init__ method of the super class with the parameter mip_era. The mip_era is the one that your plugin should support. To implement all methods you need a ModelParameters class, a GridLabel and a StreamInfo class. How to create this classes, will be explained in next sections. Example from typing import Type from cdds_common.cdds_plugins.grid import GridLabel from cdds_common.cdds_plugins.models import ModelParameters from cdds_common.cdds_plugins.plugins import CddsPlugin from cdds_common.cdds_plugins.streams import StreamInfo class MyPlugin ( CddsPlugin ): def __init__ ( self ): super ( MyPlugin , self ) . __init__ ( 'my_mip_era' ) def models_parameters ( self , model_id : str ) -> ModelParameters : pass def overload_models_parameters ( self , source_dir : str ) -> None : pass def grid_labels ( self ) -> Type [ GridLabel ]: pass def stream_info ( self ) -> StreamInfo : pass For the moment, we leave the plugin class like it is and take a look at the ModelParameters class and GridLabel class and StreamInfo class. We will come back to the plugin class later again.","title":"Step 1: Create a plugin class"},{"location":"tutorials/add_plugin/#step-2-create-the-model-parameters-class","text":"You should create one model parameter class for each model you want to support. For demonstration, here we only show you to create one model parameter class. To create more classes, simple repeat the steps. Note You can use the same module or another one. In my demonstration project, I used another module to implement everything that is related to the model parameters. Import the ModelParameters class from the CDDS project: from cdds.common.plugins.models import ModelParameters Create a class that extends from the ModelParameters Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add default __init__ method. Example from typing import List , Dict from cdds_common.cdds_plugins.grid import GridType , GridInfo from cdds_common.cdds_plugins.models import ModelParameters class MyModelParams ( ModelParameters ): def __init__ ( self ): super ( MyModelParams , self ) . __init__ () @property def model_version ( self ) -> str : pass @property def data_request_version ( self ) -> str : pass @property def um_version ( self ) -> str : pass def grid_info ( self , grid_type : GridType ) -> GridInfo : pass def temp_space ( self , stream_id : str ) -> int : pass def memory ( self , stream_id : str ) -> str : pass def cycle_length ( self , stream_id : str ) -> str : pass def sizing_info ( self , frequency : str , shape : str ) -> float : pass def full_sizing_info ( self ) -> Dict [ str , Dict [ str , float ]]: pass def is_model ( self , model_id : str ) -> bool : pass def subdaily_streams ( self ) -> List [ str ]: pass Implement all method except the grid_info method. Next step is to implement the GridInfo classes.","title":"Step 2: Create the model parameters class"},{"location":"tutorials/add_plugin/#step-3-implement-grid-information-classes","text":"Note You can use the same module or another one. In my demonstration project, I used another module to implement everything that is related to the grid information. Import the GridInfo class and GridType enum from the CDDS project: from cdds.common.plugins.grid import GridType , GridInfo For each grid type, there should be an own grid information class. So, create two classes that extends from the GridInfo - one for the atmosphere grid and one for the ocean grid. Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add the __init__ methods that call the super __init__ method and has a parameter the supported grid type of your grid information class. Implement the methods. Note Often the masks method is not implemented (simple: return None ) for atmosphere grids. Example class for atmosphere grid from typing import List , Dict from cdds.common.plugins.grid import GridType , GridInfo , OceanGridPolarMask class MyAtmosGridInfo ( GridInfo ): def __init__ ( self ): super ( MyAtmosGridInfo , self ) . __init__ ( GridType . ATMOS ) @property def model_info ( self ) -> str : pass @property def nominal_resolution ( self ) -> str : pass @property def longitude ( self ) -> int : pass @property def latitude ( self ) -> int : pass @property def v_latitude ( self ) -> int : pass @property def levels ( self ) -> int : pass @property def masks ( self ) -> Dict [ str , OceanGridPolarMask ]: pass @property def replacement_coordinates_file ( self ) -> str : pass def ancil_filenames ( self ) -> List [ str ]: pass def hybrid_heights_files ( self ) -> List [ str ]: pass Example class for ocean grid from typing import List , Dict from cdds.common.plugins.grid import GridType , GridInfo , OceanGridPolarMask class MyOceanGridInfo ( GridInfo ): def __init__ ( self ): super ( MyOceanGridInfo , self ) . __init__ ( GridType . OCEAN ) @property def model_info ( self ) -> str : pass @property def nominal_resolution ( self ) -> str : pass @property def longitude ( self ) -> int : pass @property def latitude ( self ) -> int : pass @property def v_latitude ( self ) -> int : pass @property def levels ( self ) -> int : pass @property def masks ( self ) -> Dict [ str , OceanGridPolarMask ]: pass @property def replacement_coordinates_file ( self ) -> str : pass def ancil_filenames ( self ) -> List [ str ]: pass def hybrid_heights_files ( self ) -> List [ str ]: git pass The next step is to use your grid information classes in your model parameters class.","title":"Step 3: Implement grid information classes"},{"location":"tutorials/add_plugin/#step-4-use-your-grid-information-classes-in-your-model-parameters-class","text":"Implement the grid_info method in your model parameters class such that it returns the right grid information class according the grid type. Example def grid_info ( self , grid_type : GridType ) -> GridInfo : if grid_type == GridType . ATMOS : return MyAtmosGridInfo () elif grid_type == GridType . OCEAN : return MyOceanGridInfo () else : raise ValueError ( \"Unsupported grid type: {} \" . format ( GridType ))","title":"Step 4: Use your grid information classes in your model parameters class"},{"location":"tutorials/add_plugin/#step-5-use-your-model-parameters-class-in-your-plugin","text":"Go to your plugin class and implement the model_parameters method. This method should return the corresponding model parameters class according the model ID. Example def models_parameters ( self , model_id : str ) -> ModelParameters : model_params = { 'MyModel' : MyModelParams () } return model_params [ model_id ] If you allow overloading the defined parameters in your ModelParameters classes, then implement the overload_models_parameters method. The last to do for finishing the plugin is to implement the GridLabel","title":"Step 5: Use your model parameters class in your plugin"},{"location":"tutorials/add_plugin/#step-6-implement-the-grid-labels","text":"Import the GridLabel class from CDDS: from cdds.common.plugins.grid import GridLabel Create a class your own grid label enum class that extend from GridLabel with a __init__ method that as the arguments grid_name , label and extra_info Example class MyGridLabel ( GridLabel ): def __init__ ( self , grid_name : str , label : str , extra_info : bool ) -> None : self . grid_name = grid_name self . label = label self . extra_info = extra_info @classmethod def from_name ( cls , name : str ) -> 'GridLabel' : pass Add the grid label you want to support. Example Here, MyGridLabel should support NATIVE or NATIVE_ZONAL . class MyGridLabel ( GridLabel ): def __init__ ( self , grid_name : str , label : str , extra_info : bool ) -> None : self . grid_name = grid_name self . label = label self . extra_info = extra_info @classmethod def from_name ( cls , name : str ) -> 'GridLabel' : for grid_label in MyGridLabel : if grid_label . grid_name == name . lower (): return grid_label raise KeyError ( 'Not supported grid labels for {} ' . format ( name )) NATIVE = 'native' , 'gn' , False NATIVE_ZONAL = 'native-zonal' , 'gnz' , False","title":"Step 6: Implement the grid labels"},{"location":"tutorials/add_plugin/#step-7-use-your-grid-labels-in-your-plugin","text":"Implement the grid_labels in your plugin by simply returning your grid label enum class Example def grid_labels ( self ) -> Type [ GridLabel ]: return MyGridLabel","title":"Step 7: Use your grid labels in your plugin"},{"location":"tutorials/add_plugin/#step-8-implement-the-stream-info-class","text":"You should create a stream info class for all information around streams. Import the StreamInfo class from the CDDS project: from cdds.common.plugins.streams import StreamInfo Create a class that extends from the StreamInfo Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add __init__ method Implement all necesary methods Example class MyStreamInfo ( StreamInfo ): def __init__ ( self , config_path : str = '' ) -> None : super ( Cmip6StreamInfo , self ) . __init__ ( config_path ) def _load_streams ( self , configuration : Dict [ str , Any ]) -> None : pass def retrieve_stream_id ( self , variable : str , mip_table : str ) -> Tuple [ str , str ]: pass","title":"Step 8: Implement the stream info class"},{"location":"tutorials/add_plugin/#step-9-use-your-stream-info-in-your-plugin","text":"Use the stream_info in your plugin by simply returning your stream info Example def stream_info ( self ) -> StreamInfo : return MyStreamInfo () Congratulations, you finished to implement the plugin. Now, you can use it with CDDS.","title":"Step 9: Use your stream info in your plugin"},{"location":"tutorials/add_plugin/#step-10-use-your-plugin-with-cdds","text":"Add your implementation to the PYTHONPATH (and PATH ). Otherwise, CDDS will not be able to find it. Change the value of the external_plugin_location in the request.cfg file to the path to your plugin implementation Change the value of the external_plugin in the request.cfg file to the module path to your plugin Example: Change values for the ARISE plugin external_plugin_location = /project/cdds/arise external_plugin = arise.plugin","title":"Step 10: Use your plugin with CDDS"},{"location":"tutorials/json_model_parameter/","text":"Update Model Parameters JSON File Stream File Frequencies The stream_file_frequency section contains a key value pairs where the key the frequency is and the value the list of streams. Following frequencies are supported: hourly , daily , 10 day , quarterly , monthly Example Assuming ap4 and ap5 are monthly streams and ap6 is a daily stream: \"stream_file_frequency\" : { \"monthly\" : [ \"ap4\" , \"ap5\" ], \"daily\" : [ \"ap6\" ] } Cylc length / Memory / Temporary space There are following three sections that have an entry for each stream: cylc_length lengths of the suite cylc for each stream Example \"cycle_length\" : { \"ap4\" : \"P5Y\" , \"ap5\" : \"P5Y\" , \"ap6\" : \"P1Y\" } memory memory usages of each stream Note This is the starting point for memory and adaptive memory, storage and time limits are used. Example \"memory\" : { \"ap4\" : \"12G\" , \"ap5\" : \"8G\" , \"ap6\" : \"30G\" } temp_space temporary space of each stream Note This is the starting point for memory and adaptive memory, storage and time limits are used. Example \"temp_space\" : { \"ap4\" : 98304 , \"ap5\" : 40960 , \"ap6\" : 98304 } Sizing information The entry sizing_info represents the sizing information and has JSON objects represent the information as value. Each JSON object has as key the frequency and as value the shape with its coordinates. Note The frequency is the global attribute from the files being concatenated. Example \"sizing_info\" : { \"mon\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 }, \"monPt\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 } } Sub daily streams There are one entry for the subdaily streams subdaily_streams that contains an array of subdaily streams. Example \"subdaily_streams\" : [ \"ap6\" , \"ap7\" , \"ap8\" , \"ap9\" , \"apt\" ] Grid information There are two supported grids - atmosphere and ocean. The entries are atoms and ocean . Atmosphere Grid Information Following information should be provided: atmos_timestep the atmosphere timestep. The timestep is often different for different model resolutions. model_info a simple description of the grid, e.g. N96 grid . It is interpolated into grid attribute . nominal_resolution the nominal resolution. This needs to agree with the Controlled Vocabulary for the project. longitude the number of longitude points in the atmosphere grid latitude the number of latitude points in the atmosphere grid (T points and U points) v_latitude The number of latitude points on the atmosphere grid for data on V points levels the number of vertical levels ancil_filenames the ancillary file names. These are looked for in the directory <root_ancil_dir>/<source_id> as specified in the request config file hybrid_heights_files the hybrid heights files Example \"atmos\" : { \"atmos_timestep\" : 1200 , \"model_info\" : \"N96 grid\" , \"nominal_resolution\" : \"250 km\" , \"longitude\" : 192 , \"latitude\" : 144 , \"v_latitude\" : 145 , \"levels\" : 85 , \"ancil_filenames\" : [ \"qrparm.landfrac.pp\" , \"qrparm.soil.pp\" ], \"hybrid_heights_files\" : [ \"atmosphere_theta_levels_85.txt\" , \"atmosphere_rho_levels_86.txt\" ] } Ocean Grid Information Following information should be provided: model_info a simple description of the grid, e.g. N96 grid . It is interpolated into grid attribute . nominal_resolution the nominal resolution. This needs to agree with the Controlled Vocabulary for the project. longitude the number of longitude points in the ocean grid latitude the number of latitude points in the ocean grid (T points and U points) v_latitude the number of latitude points on the ocean grid for data on V points levels the number of vertical levels ancil_filenames the ancillary file names. These are looked for in the directory <root_ancil_dir>/<source_id> as specified in the request config file replacement_coordinates_file the replacement coordinates file for CICE model output hybrid_heights_files the hybrid heights files. Note This is not relevant for the ocean, but still needs to be present. bounds_coordinates (Optional, added in CDDS v3.1.0) This field is a dictionary containing the list of netcdf variables that correspond to the bounds variables for coordinates in the NEMO output files for each ocean substream. If this is not specified the built in defaults will be used. masked a json object of ocean grid polar masks for the grid. Each masked entry as a grid label and its masked value split by the slice_latitude and slice_longitude . These are used to mask duplicate cells along the polar rows. Example \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] } } halo_options the ncks options needed to move ocean halo rows and columns. Each option are given by grid label. They are used when extracting data from MASS. Example \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } Example \"ocean\" : { \"model_info\" : \"eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics\" , \"nominal_resolution\" : \"100 km\" , \"longitude\" : 360 , \"latitude\" : 330 , \"levels\" : 75 , \"replacement_coordinates_file\" : \"cice_eORCA1_coords.nc\" , \"ancil_filenames\" : [ \"ocean_constants.nc\" , \"ocean_byte_masks.nc\" ], \"hybrid_heights_files\" : [], \"bounds_coordinates\" : { \"onm-grid-T\" : [ \"bounds_nav_lon\" , \"bounds_nav_lat\" , \"time_centered_bounds\" , \"deptht_bounds\" ] }, \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] }, \"cice-U\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] } }, \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } } Example Example { \"stream_file_frequency\" : { \"monthly\" : [ \"ap4\" , \"ap5\" ], \"10 day\" : [ \"ap6\" ] }, \"cycle_length\" : { \"ap4\" : \"P5Y\" , \"ap5\" : \"P5Y\" , \"ap6\" : \"P1Y\" }, \"memory\" : { \"ap4\" : \"12G\" , \"ap5\" : \"8G\" , \"ap6\" : \"30G\" }, \"temp_space\" : { \"ap4\" : 98304 , \"ap5\" : 40960 , \"ap6\" : 98304 }, \"sizing_info\" : { \"mon\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 }, \"day\" : { \"default\" : 20 , \"144-192\" : 100 , \"19-144-192\" : 20 }, }, \"subdaily_streams\" : [ \"ap6\" ], \"grid_info\" : { \"atmos\" : { \"atmos_timestep\" : 1200 , \"model_info\" : \"N96 grid\" , \"nominal_resolution\" : \"250 km\" , \"longitude\" : 192 , \"latitude\" : 144 , \"v_latitude\" : 145 , \"levels\" : 85 , \"ancil_filenames\" : [ \"qrparm.landfrac.pp\" , \"qrparm.soil.pp\" ], \"hybrid_heights_files\" : [ \"atmosphere_theta_levels_85.txt\" , \"atmosphere_rho_levels_86.txt\" ] }, \"ocean\" : { \"model_info\" : \"eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics\" , \"nominal_resolution\" : \"100 km\" , \"longitude\" : 360 , \"latitude\" : 330 , \"levels\" : 75 , \"replacement_coordinates_file\" : \"cice_eORCA1_coords.nc\" , \"ancil_filenames\" : [ \"ocean_constants.nc\" , \"ocean_byte_masks.nc\" , \"ocean_basin.nc\" , \"diaptr_basin_masks.nc\" , \"ocean_zostoga.nc\" ], \"bounds_coordinates\" : { \"onm-grid-T\" : [ \"bounds_nav_lon\" , \"bounds_nav_lat\" , \"time_centered_bounds\" , \"deptht_bounds\" ] }, \"hybrid_heights_files\" : [], \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] }, \"cice-U\" : { \"slice_latitude\" : [ -1 , null , null ] } }, \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } } } Check Model Parameters JSON File There is a little tool, that checks the basic of your model parameters file: validate_model_params <request file> where the <request file> is the path to your request that uses your model parameters JSON file.","title":"Model Parameters JSON File"},{"location":"tutorials/json_model_parameter/#update-model-parameters-json-file","text":"","title":"Update Model Parameters JSON File"},{"location":"tutorials/json_model_parameter/#stream-file-frequencies","text":"The stream_file_frequency section contains a key value pairs where the key the frequency is and the value the list of streams. Following frequencies are supported: hourly , daily , 10 day , quarterly , monthly Example Assuming ap4 and ap5 are monthly streams and ap6 is a daily stream: \"stream_file_frequency\" : { \"monthly\" : [ \"ap4\" , \"ap5\" ], \"daily\" : [ \"ap6\" ] }","title":"Stream File Frequencies"},{"location":"tutorials/json_model_parameter/#cylc-length-memory-temporary-space","text":"There are following three sections that have an entry for each stream: cylc_length lengths of the suite cylc for each stream Example \"cycle_length\" : { \"ap4\" : \"P5Y\" , \"ap5\" : \"P5Y\" , \"ap6\" : \"P1Y\" } memory memory usages of each stream Note This is the starting point for memory and adaptive memory, storage and time limits are used. Example \"memory\" : { \"ap4\" : \"12G\" , \"ap5\" : \"8G\" , \"ap6\" : \"30G\" } temp_space temporary space of each stream Note This is the starting point for memory and adaptive memory, storage and time limits are used. Example \"temp_space\" : { \"ap4\" : 98304 , \"ap5\" : 40960 , \"ap6\" : 98304 }","title":"Cylc length / Memory / Temporary space"},{"location":"tutorials/json_model_parameter/#sizing-information","text":"The entry sizing_info represents the sizing information and has JSON objects represent the information as value. Each JSON object has as key the frequency and as value the shape with its coordinates. Note The frequency is the global attribute from the files being concatenated. Example \"sizing_info\" : { \"mon\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 }, \"monPt\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 } }","title":"Sizing information"},{"location":"tutorials/json_model_parameter/#sub-daily-streams","text":"There are one entry for the subdaily streams subdaily_streams that contains an array of subdaily streams. Example \"subdaily_streams\" : [ \"ap6\" , \"ap7\" , \"ap8\" , \"ap9\" , \"apt\" ]","title":"Sub daily streams"},{"location":"tutorials/json_model_parameter/#grid-information","text":"There are two supported grids - atmosphere and ocean. The entries are atoms and ocean .","title":"Grid information"},{"location":"tutorials/json_model_parameter/#atmosphere-grid-information","text":"Following information should be provided: atmos_timestep the atmosphere timestep. The timestep is often different for different model resolutions. model_info a simple description of the grid, e.g. N96 grid . It is interpolated into grid attribute . nominal_resolution the nominal resolution. This needs to agree with the Controlled Vocabulary for the project. longitude the number of longitude points in the atmosphere grid latitude the number of latitude points in the atmosphere grid (T points and U points) v_latitude The number of latitude points on the atmosphere grid for data on V points levels the number of vertical levels ancil_filenames the ancillary file names. These are looked for in the directory <root_ancil_dir>/<source_id> as specified in the request config file hybrid_heights_files the hybrid heights files Example \"atmos\" : { \"atmos_timestep\" : 1200 , \"model_info\" : \"N96 grid\" , \"nominal_resolution\" : \"250 km\" , \"longitude\" : 192 , \"latitude\" : 144 , \"v_latitude\" : 145 , \"levels\" : 85 , \"ancil_filenames\" : [ \"qrparm.landfrac.pp\" , \"qrparm.soil.pp\" ], \"hybrid_heights_files\" : [ \"atmosphere_theta_levels_85.txt\" , \"atmosphere_rho_levels_86.txt\" ] }","title":"Atmosphere Grid Information"},{"location":"tutorials/json_model_parameter/#ocean-grid-information","text":"Following information should be provided: model_info a simple description of the grid, e.g. N96 grid . It is interpolated into grid attribute . nominal_resolution the nominal resolution. This needs to agree with the Controlled Vocabulary for the project. longitude the number of longitude points in the ocean grid latitude the number of latitude points in the ocean grid (T points and U points) v_latitude the number of latitude points on the ocean grid for data on V points levels the number of vertical levels ancil_filenames the ancillary file names. These are looked for in the directory <root_ancil_dir>/<source_id> as specified in the request config file replacement_coordinates_file the replacement coordinates file for CICE model output hybrid_heights_files the hybrid heights files. Note This is not relevant for the ocean, but still needs to be present. bounds_coordinates (Optional, added in CDDS v3.1.0) This field is a dictionary containing the list of netcdf variables that correspond to the bounds variables for coordinates in the NEMO output files for each ocean substream. If this is not specified the built in defaults will be used. masked a json object of ocean grid polar masks for the grid. Each masked entry as a grid label and its masked value split by the slice_latitude and slice_longitude . These are used to mask duplicate cells along the polar rows. Example \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] } } halo_options the ncks options needed to move ocean halo rows and columns. Each option are given by grid label. They are used when extracting data from MASS. Example \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } Example \"ocean\" : { \"model_info\" : \"eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics\" , \"nominal_resolution\" : \"100 km\" , \"longitude\" : 360 , \"latitude\" : 330 , \"levels\" : 75 , \"replacement_coordinates_file\" : \"cice_eORCA1_coords.nc\" , \"ancil_filenames\" : [ \"ocean_constants.nc\" , \"ocean_byte_masks.nc\" ], \"hybrid_heights_files\" : [], \"bounds_coordinates\" : { \"onm-grid-T\" : [ \"bounds_nav_lon\" , \"bounds_nav_lat\" , \"time_centered_bounds\" , \"deptht_bounds\" ] }, \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] }, \"cice-U\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] } }, \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } }","title":"Ocean Grid Information"},{"location":"tutorials/json_model_parameter/#example","text":"Example { \"stream_file_frequency\" : { \"monthly\" : [ \"ap4\" , \"ap5\" ], \"10 day\" : [ \"ap6\" ] }, \"cycle_length\" : { \"ap4\" : \"P5Y\" , \"ap5\" : \"P5Y\" , \"ap6\" : \"P1Y\" }, \"memory\" : { \"ap4\" : \"12G\" , \"ap5\" : \"8G\" , \"ap6\" : \"30G\" }, \"temp_space\" : { \"ap4\" : 98304 , \"ap5\" : 40960 , \"ap6\" : 98304 }, \"sizing_info\" : { \"mon\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 }, \"day\" : { \"default\" : 20 , \"144-192\" : 100 , \"19-144-192\" : 20 }, }, \"subdaily_streams\" : [ \"ap6\" ], \"grid_info\" : { \"atmos\" : { \"atmos_timestep\" : 1200 , \"model_info\" : \"N96 grid\" , \"nominal_resolution\" : \"250 km\" , \"longitude\" : 192 , \"latitude\" : 144 , \"v_latitude\" : 145 , \"levels\" : 85 , \"ancil_filenames\" : [ \"qrparm.landfrac.pp\" , \"qrparm.soil.pp\" ], \"hybrid_heights_files\" : [ \"atmosphere_theta_levels_85.txt\" , \"atmosphere_rho_levels_86.txt\" ] }, \"ocean\" : { \"model_info\" : \"eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics\" , \"nominal_resolution\" : \"100 km\" , \"longitude\" : 360 , \"latitude\" : 330 , \"levels\" : 75 , \"replacement_coordinates_file\" : \"cice_eORCA1_coords.nc\" , \"ancil_filenames\" : [ \"ocean_constants.nc\" , \"ocean_byte_masks.nc\" , \"ocean_basin.nc\" , \"diaptr_basin_masks.nc\" , \"ocean_zostoga.nc\" ], \"bounds_coordinates\" : { \"onm-grid-T\" : [ \"bounds_nav_lon\" , \"bounds_nav_lat\" , \"time_centered_bounds\" , \"deptht_bounds\" ] }, \"hybrid_heights_files\" : [], \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] }, \"cice-U\" : { \"slice_latitude\" : [ -1 , null , null ] } }, \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } } }","title":"Example"},{"location":"tutorials/json_model_parameter/#check-model-parameters-json-file","text":"There is a little tool, that checks the basic of your model parameters file: validate_model_params <request file> where the <request file> is the path to your request that uses your model parameters JSON file.","title":"Check Model Parameters JSON File"},{"location":"tutorials/mip_convert/","text":"Overview The mip_convert package enables a user to produce the output netCDF files for a MIP using model output files. graph LR A[model output .pp data] --> C[MIP Convert + CMOR]; B[model output .nc data] --> C; C --> D[CF/CMIP6 Compliant .nc data]; The user makes requests for one or more MIP requested variables by providing specific information (including the appropriate MIP requested variable names) in the user configuration file. The information required to produce the MIP requested variables is gathered from the user configuration file, the model to MIP mapping configuration files and the appropriate MIP table, in that order. The following steps are then performed for each MIP requested variable name in the user configuration file to produce the output netCDF files: load the relevant data from the model output files into one or more input variables depending on whether there is a one-to-one / simple arithmetic relationship between the MIP output variable and the input variables or if the MIP output variable is based on an arithmetic combination of two or more input variables, respectively, using Iris and the information provided in the user configuration file and the model to MIP mapping configuration files. process the input variable / input variables to produce the MIP output variable using the information provided in the model to MIP mapping configuration files. save the MIP output variable to an output netCDF file using CMOR and the information provided in the user configuration file and the appropriate MIP table. Recommended Reading The Design Considerations and Overview section in the CMOR Documentation. Quick Start Guide Download the template user configuration file mip_convert.cfg . Make the appropriate edits to the template user configuration file using the information provided in the \"User Configuration File\" section and the specified sections in the CMOR Documentation . Source an environment with cdds and verify that mip_convert runs. mip_convert -h Produce the output netCDF files by running mip_convert and passing in the modified user configuration file as an argument. mip_convert mip_convert.cfg Check the exit code echo $? Exit Code Meaning 0 No errors were raised during processing. 1 An exception was raised and no MIP requested variables were produced. 2 One or more MIP requested variables were produced but not all variables were produced. See the CRITICAL messages in the log for further information about the MIP requested variables not produced. Check that the output netCDF files are as expected. For help or to report an issue, please see support . Selected MIP Convert Arguments Argument Description config_file The name of the user configuration file. For more information, please see the MIP Convert user guide -s or --stream_identifiers The stream identifiers to process. If all streams should be processed, do not specify this option. --relaxed-cmor If specified, CMIP6 style validation is not performed by CMOR. If the validation is run then the following fields are not checked; model_id ( source_id ), experiment_id , further_info_url , grid_label , parent_experiment_id , sub_experiment_id . --mip_era The MIP era (e.g. CMIP6). --external_plugin Module path to external CDDS plugin (e.g. arise.plugin ) --external_plugin_location Path to the external plugin implementation (e.g. /project/cdds/arise ) Example Usage Run for all streams with full checking of metadata mip_convert mip_convert.cfg Run for a single stream in relaxed mode mip_convert mip_convert.cfg -s ap4 --relaxed_cmor User Configuration File Reference The user configuration file provides the information required by MIP Convert to produce the output netCDF files. It contains the following sections, some of which are optional. Section Summary [COMMON] Convenience for setting up shared config values. [cmor_setup] Passed through to CMOR's cmor_setup() routine. [cmor_dataset] Passed through to CMOR's cmor_data_set_json() routine. [request] Configure mip_convert including input data. [stream_<stream_id>] The variables to produce from a particular <stream_id> . [masking] Apply polar row masking if needed. [halo_removal] Apply stripping of halo columns and rows if needed. [slicing_periods] Using the slicing period for a particular stream if given. [global_attributes] Add global attributes to the output netCDF. COMMON The optional [COMMON] section. cmor_setup The [cmor_setup] section contains the following options which are used by cmor_setup() . For a description of each option please see the documentation for cmor_setup() . Option Required by Used by CMOR Name mip_table_dir MIP Convert CMOR + MIP Convert inpath netcdf_file_action CMOR set_verbosity CMOR exit_control CMOR cmor_log_file CMOR log_file create_subdirectories CMOR Tip When configuring a user configuration file, the mip_table_dir is likely to be the only value that will need modification. cmor_dataset The required cmor_dataset section contains the following options used for cmor_data_set_json() Option Required by Used by Notes branch_method MIP Convert + CMOR MIP Convert + CMOR [1] calendar MIP Convert + CMOR MIP Convert + CMOR [2] comment CMOR 1 contact CMOR [1] experiment_id MIP Convert + CMOR MIP Convert + CMOR [1] grid CMOR CMOR [1] grid_label CMOR CMOR [1] institution_id MIP Convert + CMOR MIP Convert + CMOR [1] license CMOR CMOR [1] mip CMOR CMOR 1 mip_era MIP Convert + CMOR MIP Convert + CMOR [1] model_id MIP Convert + CMOR MIP Convert + CMOR 1 model_type CMOR CMOR 1 nominal_resolution CMOR CMOR [1] output_dir MIP Convert + CMOR MIP Convert + CMOR [7] output_file_template CMOR output_path_template CMOR references CMOR [1] sub_experiment_id MIP Convert + CMOR MIP Convert [1] variant_info CMOR [1] variant_label MIP Convert + CMOR MIP Convert + CMOR [1] Notes For a description of each option, please see the CMIP6 Global Attributes document _. See calendars for allowed values. It is recommended to use the comment to record any perturbed physicsdetails. See MIP. See model identifier. See model type. See outpath in the documentation for cmor_dataset_json _. MIP Convert determines: the experiment , institution , source , sub_experiment from the CV file using the experiment_id , institution_id , source_id and sub_experiment_id , respectively the forcing_index , initialization_index , physics_index and realization_index from the variant_label the further_info_url and tracking_prefix based on the information from the CV file the history Whenever a parent experiment exists the following options must also be specified. Option Used by Notes branch_date_in_child MIP Convert 1 [3] branch_date_in_parent MIP Convert 1 [3] parent_base_date MIP Convert 1 parent_experiment_id CMOR [3] parent_mip_era CMOR [3] parent_model_id CMOR 3 parent_time_units CMOR [3] parent_variant_label CMOR [3] Notes CMOR requires branch_time_in_child and branch_time_in_parent , which is determined from the options base_date (see the request <request_section> section) / parent_base_date (the base date of the child_experiment_id / parent_experiment_id ) and branch_date_in_child / branch_date_in_parent (the date in the child_experiment_id / parent_experiment_id from which the experiment branches) from the cmor_dataset <cmor_dataset_section> section in the |user configuration file| by taking the difference (in days) between the branch_date_in_child / branch_date_in_parent and the base_date / parent_base_date . If branch_date_in_child or branch_date_in_parent is N/A then branch_time_in_parent is set to 0. Dates should be provided in the form YYYY-MM-DDThh:mm:ssZ . For a description of each option, please see the CMIP6 Global Attributes document . See parent_source_id in the CMIP6 Global Attributes document . request The required request section contains the following options which are used only by MIP Convert. Option Required Description Notes ancil_files A space separated list of the full paths to any required ancillary files. atmos_timestep The atmospheric model timestep in integer seconds. [1] base_date Yes The date in the form YYYY-MM-DDThh:mm:ss . [2] deflate_level The deflation level when writing the output netCDF file from 0 (no compression) to 9 (maximum compression). force_coordinate_rotation If set to True , output data will be forced to include rotated coordinates and true lat-lon coordinates. hybrid_heights_file A space separated list of the full path to the files containing the information about the hybrid heights. [3] mask_slice Yes Optional slicing expression for masking data in the form of n:m,i:j , or no_mask 4 model_output_dir Yes The full path to the root directory containing the model output files. [5] reference_time Yes The reference time used to construct reftime and leadtime coordinates. Only used if these coordinates are specified corresponding variable entries in the MIP table replacement_coordinates_file The full path to the netCDF file containing area variables that refer to the horizontal coordinates that should be used to replace the corresponding values in the model output files. [6] run_bounds Yes The start and end time in the form <start_time> <end_time> , where <start_time> and <end_time> are in the form YYYY-MM-DDThh:mm:ss . shuffle Whether to shuffle when writing the output netCDF file. sites_file The full path to the file containing the information about the sites. [7] suite_id Yes The suite identifier of the model. Notes The atmos_timestep is required for atmospheric tendency diagnostics, which have model to MIP mappings that depend on the atmospheric model timestep, i.e., the expression contains ATMOS_TIMESTEP . The base_date is used to define the units of the time coordinate in the output netCDF file and is specified by the MIP. The file containing the information about the hybrid heights has the following columns; the model level number (int) the a_value (float) the a_lower_bound (float) the a_upper_bound (float) the b_value (float) the b_lower_bound (float) the b_upper_bound (float) If not specified, mip_convert will try to retrieve masking expressions from plugins (this is a default behaviour for CMIP6-like processing). Putting no_mask into configuration file allows mip_convert to process model output that does not require any masking; custom masks can be specified and passed to mip_convert without plugins dependencies. It is expected that the model output files are located in the directory <model_output_dir>/<suite_id>/<stream_id>/ , where the <suite_id> is the suite identifier and the <stream_id> is the |stream identifier|. Note that MIP Convert will load all the files in this directory and then use the run_bounds to select the required data; when selecting a short time period from a large number of |model output files| it is recommended to copy the relevant files to an empty directory to save time when loading. Currently, only CICE horizontal coordinates can be replaced. The file containing the information about the sites has the following columns; the site number (int) the longitude (float, from 0 to 360) [degrees] the latitude (float, from -90 to 90) [degrees] the orography (float) [metres] and a comment (string). This is usually only used to mask ocean/seaice data as part of a CDDS run. stream_<\\stream_id> The required [stream_<stream_id>] section, where the <stream_id> is the stream identifier, contains options equal to the name of the MIP table and values equal to a space-separated list of MIP requested variable names. Multiple [stream_<stream_id>] sections can be defined. Note All output netCDF files are created for a stream before moving onto the next stream. Example If we wanted to produce the following variables Amon/tas , Amon/pr , Emon/ps , Amon/tasmax , day/tasmin using the CMIP6 tables. We need two stream_stream_id sections as the list of MIP requested variables span streams ap5 and ap6 . Within each of these sections we specify the mip table in the form <MIP_ERA>_<MIP_TABLE>: Then the list of variables names as a space seperated list. [stream_ap5] CMIP6_Amon: tas pr CMIP6_Emon: ps [stream_ap6] CMIP6_Amon: tasmax CMIP6_day: tasmin masking The optional masking section is used if a particular stream needs to be masked. This is usually only used for polar row masking in NEMO & CICE output Example [masking] stream_inm_cice-T: -1:,180: halo_removal The optional halo removal section is used if for a particular stream haloes are to be removed. Example [halo_removal] stream_apa: 5:,:-10 stream_ap6: 20:-15 slicing_periods The optional slicing_periods section is used if for a particular stream a period for slicing is specified. For all streams that have no specified slicing period the default slicing period is year . Example [slicing_periods] stream_apa: month stream_ap6: year global_attributes The optional global_attributes section. Any information provided in the optional global_attributes <global_attributes> section will be written to the header of the output netCDF files.","title":"MIP Convert User Guide"},{"location":"tutorials/mip_convert/#overview","text":"The mip_convert package enables a user to produce the output netCDF files for a MIP using model output files. graph LR A[model output .pp data] --> C[MIP Convert + CMOR]; B[model output .nc data] --> C; C --> D[CF/CMIP6 Compliant .nc data]; The user makes requests for one or more MIP requested variables by providing specific information (including the appropriate MIP requested variable names) in the user configuration file. The information required to produce the MIP requested variables is gathered from the user configuration file, the model to MIP mapping configuration files and the appropriate MIP table, in that order. The following steps are then performed for each MIP requested variable name in the user configuration file to produce the output netCDF files: load the relevant data from the model output files into one or more input variables depending on whether there is a one-to-one / simple arithmetic relationship between the MIP output variable and the input variables or if the MIP output variable is based on an arithmetic combination of two or more input variables, respectively, using Iris and the information provided in the user configuration file and the model to MIP mapping configuration files. process the input variable / input variables to produce the MIP output variable using the information provided in the model to MIP mapping configuration files. save the MIP output variable to an output netCDF file using CMOR and the information provided in the user configuration file and the appropriate MIP table.","title":"Overview"},{"location":"tutorials/mip_convert/#recommended-reading","text":"The Design Considerations and Overview section in the CMOR Documentation.","title":"Recommended Reading"},{"location":"tutorials/mip_convert/#quick-start-guide","text":"Download the template user configuration file mip_convert.cfg . Make the appropriate edits to the template user configuration file using the information provided in the \"User Configuration File\" section and the specified sections in the CMOR Documentation . Source an environment with cdds and verify that mip_convert runs. mip_convert -h Produce the output netCDF files by running mip_convert and passing in the modified user configuration file as an argument. mip_convert mip_convert.cfg Check the exit code echo $? Exit Code Meaning 0 No errors were raised during processing. 1 An exception was raised and no MIP requested variables were produced. 2 One or more MIP requested variables were produced but not all variables were produced. See the CRITICAL messages in the log for further information about the MIP requested variables not produced. Check that the output netCDF files are as expected. For help or to report an issue, please see support .","title":"Quick Start Guide"},{"location":"tutorials/mip_convert/#selected-mip-convert-arguments","text":"Argument Description config_file The name of the user configuration file. For more information, please see the MIP Convert user guide -s or --stream_identifiers The stream identifiers to process. If all streams should be processed, do not specify this option. --relaxed-cmor If specified, CMIP6 style validation is not performed by CMOR. If the validation is run then the following fields are not checked; model_id ( source_id ), experiment_id , further_info_url , grid_label , parent_experiment_id , sub_experiment_id . --mip_era The MIP era (e.g. CMIP6). --external_plugin Module path to external CDDS plugin (e.g. arise.plugin ) --external_plugin_location Path to the external plugin implementation (e.g. /project/cdds/arise )","title":"Selected MIP Convert Arguments"},{"location":"tutorials/mip_convert/#example-usage","text":"Run for all streams with full checking of metadata mip_convert mip_convert.cfg Run for a single stream in relaxed mode mip_convert mip_convert.cfg -s ap4 --relaxed_cmor","title":"Example Usage"},{"location":"tutorials/mip_convert/#user-configuration-file-reference","text":"The user configuration file provides the information required by MIP Convert to produce the output netCDF files. It contains the following sections, some of which are optional. Section Summary [COMMON] Convenience for setting up shared config values. [cmor_setup] Passed through to CMOR's cmor_setup() routine. [cmor_dataset] Passed through to CMOR's cmor_data_set_json() routine. [request] Configure mip_convert including input data. [stream_<stream_id>] The variables to produce from a particular <stream_id> . [masking] Apply polar row masking if needed. [halo_removal] Apply stripping of halo columns and rows if needed. [slicing_periods] Using the slicing period for a particular stream if given. [global_attributes] Add global attributes to the output netCDF.","title":"User Configuration File Reference"},{"location":"tutorials/mip_convert/#common","text":"The optional [COMMON] section.","title":"COMMON"},{"location":"tutorials/mip_convert/#cmor_setup","text":"The [cmor_setup] section contains the following options which are used by cmor_setup() . For a description of each option please see the documentation for cmor_setup() . Option Required by Used by CMOR Name mip_table_dir MIP Convert CMOR + MIP Convert inpath netcdf_file_action CMOR set_verbosity CMOR exit_control CMOR cmor_log_file CMOR log_file create_subdirectories CMOR Tip When configuring a user configuration file, the mip_table_dir is likely to be the only value that will need modification.","title":"cmor_setup"},{"location":"tutorials/mip_convert/#cmor_dataset","text":"The required cmor_dataset section contains the following options used for cmor_data_set_json() Option Required by Used by Notes branch_method MIP Convert + CMOR MIP Convert + CMOR [1] calendar MIP Convert + CMOR MIP Convert + CMOR [2] comment CMOR 1 contact CMOR [1] experiment_id MIP Convert + CMOR MIP Convert + CMOR [1] grid CMOR CMOR [1] grid_label CMOR CMOR [1] institution_id MIP Convert + CMOR MIP Convert + CMOR [1] license CMOR CMOR [1] mip CMOR CMOR 1 mip_era MIP Convert + CMOR MIP Convert + CMOR [1] model_id MIP Convert + CMOR MIP Convert + CMOR 1 model_type CMOR CMOR 1 nominal_resolution CMOR CMOR [1] output_dir MIP Convert + CMOR MIP Convert + CMOR [7] output_file_template CMOR output_path_template CMOR references CMOR [1] sub_experiment_id MIP Convert + CMOR MIP Convert [1] variant_info CMOR [1] variant_label MIP Convert + CMOR MIP Convert + CMOR [1] Notes For a description of each option, please see the CMIP6 Global Attributes document _. See calendars for allowed values. It is recommended to use the comment to record any perturbed physicsdetails. See MIP. See model identifier. See model type. See outpath in the documentation for cmor_dataset_json _. MIP Convert determines: the experiment , institution , source , sub_experiment from the CV file using the experiment_id , institution_id , source_id and sub_experiment_id , respectively the forcing_index , initialization_index , physics_index and realization_index from the variant_label the further_info_url and tracking_prefix based on the information from the CV file the history Whenever a parent experiment exists the following options must also be specified. Option Used by Notes branch_date_in_child MIP Convert 1 [3] branch_date_in_parent MIP Convert 1 [3] parent_base_date MIP Convert 1 parent_experiment_id CMOR [3] parent_mip_era CMOR [3] parent_model_id CMOR 3 parent_time_units CMOR [3] parent_variant_label CMOR [3] Notes CMOR requires branch_time_in_child and branch_time_in_parent , which is determined from the options base_date (see the request <request_section> section) / parent_base_date (the base date of the child_experiment_id / parent_experiment_id ) and branch_date_in_child / branch_date_in_parent (the date in the child_experiment_id / parent_experiment_id from which the experiment branches) from the cmor_dataset <cmor_dataset_section> section in the |user configuration file| by taking the difference (in days) between the branch_date_in_child / branch_date_in_parent and the base_date / parent_base_date . If branch_date_in_child or branch_date_in_parent is N/A then branch_time_in_parent is set to 0. Dates should be provided in the form YYYY-MM-DDThh:mm:ssZ . For a description of each option, please see the CMIP6 Global Attributes document . See parent_source_id in the CMIP6 Global Attributes document .","title":"cmor_dataset"},{"location":"tutorials/mip_convert/#request","text":"The required request section contains the following options which are used only by MIP Convert. Option Required Description Notes ancil_files A space separated list of the full paths to any required ancillary files. atmos_timestep The atmospheric model timestep in integer seconds. [1] base_date Yes The date in the form YYYY-MM-DDThh:mm:ss . [2] deflate_level The deflation level when writing the output netCDF file from 0 (no compression) to 9 (maximum compression). force_coordinate_rotation If set to True , output data will be forced to include rotated coordinates and true lat-lon coordinates. hybrid_heights_file A space separated list of the full path to the files containing the information about the hybrid heights. [3] mask_slice Yes Optional slicing expression for masking data in the form of n:m,i:j , or no_mask 4 model_output_dir Yes The full path to the root directory containing the model output files. [5] reference_time Yes The reference time used to construct reftime and leadtime coordinates. Only used if these coordinates are specified corresponding variable entries in the MIP table replacement_coordinates_file The full path to the netCDF file containing area variables that refer to the horizontal coordinates that should be used to replace the corresponding values in the model output files. [6] run_bounds Yes The start and end time in the form <start_time> <end_time> , where <start_time> and <end_time> are in the form YYYY-MM-DDThh:mm:ss . shuffle Whether to shuffle when writing the output netCDF file. sites_file The full path to the file containing the information about the sites. [7] suite_id Yes The suite identifier of the model. Notes The atmos_timestep is required for atmospheric tendency diagnostics, which have model to MIP mappings that depend on the atmospheric model timestep, i.e., the expression contains ATMOS_TIMESTEP . The base_date is used to define the units of the time coordinate in the output netCDF file and is specified by the MIP. The file containing the information about the hybrid heights has the following columns; the model level number (int) the a_value (float) the a_lower_bound (float) the a_upper_bound (float) the b_value (float) the b_lower_bound (float) the b_upper_bound (float) If not specified, mip_convert will try to retrieve masking expressions from plugins (this is a default behaviour for CMIP6-like processing). Putting no_mask into configuration file allows mip_convert to process model output that does not require any masking; custom masks can be specified and passed to mip_convert without plugins dependencies. It is expected that the model output files are located in the directory <model_output_dir>/<suite_id>/<stream_id>/ , where the <suite_id> is the suite identifier and the <stream_id> is the |stream identifier|. Note that MIP Convert will load all the files in this directory and then use the run_bounds to select the required data; when selecting a short time period from a large number of |model output files| it is recommended to copy the relevant files to an empty directory to save time when loading. Currently, only CICE horizontal coordinates can be replaced. The file containing the information about the sites has the following columns; the site number (int) the longitude (float, from 0 to 360) [degrees] the latitude (float, from -90 to 90) [degrees] the orography (float) [metres] and a comment (string). This is usually only used to mask ocean/seaice data as part of a CDDS run.","title":"request"},{"location":"tutorials/mip_convert/#stream_stream_id","text":"The required [stream_<stream_id>] section, where the <stream_id> is the stream identifier, contains options equal to the name of the MIP table and values equal to a space-separated list of MIP requested variable names. Multiple [stream_<stream_id>] sections can be defined. Note All output netCDF files are created for a stream before moving onto the next stream. Example If we wanted to produce the following variables Amon/tas , Amon/pr , Emon/ps , Amon/tasmax , day/tasmin using the CMIP6 tables. We need two stream_stream_id sections as the list of MIP requested variables span streams ap5 and ap6 . Within each of these sections we specify the mip table in the form <MIP_ERA>_<MIP_TABLE>: Then the list of variables names as a space seperated list. [stream_ap5] CMIP6_Amon: tas pr CMIP6_Emon: ps [stream_ap6] CMIP6_Amon: tasmax CMIP6_day: tasmin","title":"stream_&lt;\\stream_id&gt;"},{"location":"tutorials/mip_convert/#masking","text":"The optional masking section is used if a particular stream needs to be masked. This is usually only used for polar row masking in NEMO & CICE output Example [masking] stream_inm_cice-T: -1:,180:","title":"masking"},{"location":"tutorials/mip_convert/#halo_removal","text":"The optional halo removal section is used if for a particular stream haloes are to be removed. Example [halo_removal] stream_apa: 5:,:-10 stream_ap6: 20:-15","title":"halo_removal"},{"location":"tutorials/mip_convert/#slicing_periods","text":"The optional slicing_periods section is used if for a particular stream a period for slicing is specified. For all streams that have no specified slicing period the default slicing period is year . Example [slicing_periods] stream_apa: month stream_ap6: year","title":"slicing_periods"},{"location":"tutorials/mip_convert/#global_attributes","text":"The optional global_attributes section. Any information provided in the optional global_attributes <global_attributes> section will be written to the header of the output netCDF files.","title":"global_attributes"},{"location":"tutorials/search_inventory/","text":"Script: search_inventory In release v1.6.0 we've added an inventory (updated each night) that is used by CDDS Prepare to avoid the need for combined \"approved_variables\" files to avoid duplicating previously provided data. The script allows the user to search the inventory using data set id patterns which can include wildcards instead of a particular \"facet\". The inventory database We generate a small sqlite3 database each night from the contents of MASS. As such this only contains data that we have produced within the Met Office and archived to MASS. Data set ids and \"facet\" structure The dataset ids used in CMIP6 are made up of a set of \"facets\", strings describing some aspect of what the data relates to. A facet may be the name of the MIP, experiment_id or variable name. The format for CMIP6 is CMIP6.<activity id (MIP)>.<institution id>.<source_id (model)>.<experiment id>.<variant label>.<MIP table>.<variable id>.<grid label> e.g. CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.historical.r1i1p1f2.Amon.tas.gn Search for datasets matching a facet pattern. The data set id used for the inventory is a set of 9 facets joined by dots. To search for all UKESM1 daily precipitation datasets you can run search_inventory CMIP6.*.*.UKESM1-0-LL.*.*.day.pr.* Which at the time of writing returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r1i1p1f2 day pr gn available v20190813 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r1i1p1f2.day.pr.gn CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r2i1p1f2 day pr gn available v20191104 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r2i1p1f2.day.pr.gn ... CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r4i1p1f2 day pr gn available v20190814 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r4i1p1f2.day.pr.gn CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r8i1p1f2 day pr gn available v20190906 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r8i1p1f2.day.pr.gn A total of 239 records were found. Locating data in MASS If you require the list of files in MASS for a particular dataset then the -s option will provide this for each data set matching the pattern in the inventory. For example search_inventory CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.*.pr.gn -s returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 Amon pr gn available v20190507 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.Amon.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_201501-204912.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_205001-210012.nc CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 day pr gn available v20190813 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.day.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20150101-20491230.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20500101-21001230.nc A total of 2 records were found. Note that in this case a moo ls command is run for each data set found.","title":"Search Inventory"},{"location":"tutorials/search_inventory/#script-search_inventory","text":"In release v1.6.0 we've added an inventory (updated each night) that is used by CDDS Prepare to avoid the need for combined \"approved_variables\" files to avoid duplicating previously provided data. The script allows the user to search the inventory using data set id patterns which can include wildcards instead of a particular \"facet\".","title":"Script: search_inventory"},{"location":"tutorials/search_inventory/#the-inventory-database","text":"We generate a small sqlite3 database each night from the contents of MASS. As such this only contains data that we have produced within the Met Office and archived to MASS.","title":"The inventory database"},{"location":"tutorials/search_inventory/#data-set-ids-and-facet-structure","text":"The dataset ids used in CMIP6 are made up of a set of \"facets\", strings describing some aspect of what the data relates to. A facet may be the name of the MIP, experiment_id or variable name. The format for CMIP6 is CMIP6.<activity id (MIP)>.<institution id>.<source_id (model)>.<experiment id>.<variant label>.<MIP table>.<variable id>.<grid label> e.g. CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.historical.r1i1p1f2.Amon.tas.gn","title":"Data set ids and \"facet\" structure"},{"location":"tutorials/search_inventory/#search-for-datasets-matching-a-facet-pattern","text":"The data set id used for the inventory is a set of 9 facets joined by dots. To search for all UKESM1 daily precipitation datasets you can run search_inventory CMIP6.*.*.UKESM1-0-LL.*.*.day.pr.* Which at the time of writing returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r1i1p1f2 day pr gn available v20190813 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r1i1p1f2.day.pr.gn CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r2i1p1f2 day pr gn available v20191104 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r2i1p1f2.day.pr.gn ... CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r4i1p1f2 day pr gn available v20190814 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r4i1p1f2.day.pr.gn CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r8i1p1f2 day pr gn available v20190906 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r8i1p1f2.day.pr.gn A total of 239 records were found.","title":"Search for datasets matching a facet pattern."},{"location":"tutorials/search_inventory/#locating-data-in-mass","text":"If you require the list of files in MASS for a particular dataset then the -s option will provide this for each data set matching the pattern in the inventory. For example search_inventory CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.*.pr.gn -s returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 Amon pr gn available v20190507 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.Amon.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_201501-204912.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_205001-210012.nc CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 day pr gn available v20190813 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.day.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20150101-20491230.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20500101-21001230.nc A total of 2 records were found. Note that in this case a moo ls command is run for each data set found.","title":"Locating data in MASS"},{"location":"tutorials/request/common/","text":"Common Section The common section in the request configuration contains common setting like the path to the root data folder or the path to the external plugin. Configuration Values external_plugin module path to external CDDS plugin, e.g. cdds_arise.arise_plugin external_plugin_location path to the external plugin implementation, e.g. /home/h03/cdds/arise mip_table_dir path to the directory containing the inforamtion about the MIP tables, e.g. /home/h03/cdds/etc/mip_tables/CMIP6/01.00.29/ Default: The path to the MIP table directoring specified in the corresponding plugin mode mode that should be used to CMORised the data - strict or relaxed . If mode is strict , CMOR will fail if there are warnings regarding compliance of CMOR standards. If mode is relaxed , CMOR will only fail if cubes present serve discrepancies with CMOR standards. Default: strict package package name used to distinguish different run throughs of CDDS, e.g. round-1 . workflow_basename name of the workflow name of the CDDS suite that will trigger during the CDDS processing suite Default : <model_id>_<experiment_id>_<variant_label> - all three values are defined in the metadata section root_proc_dir root path to the proc directory where the non-data outputs are written, e.g. log files. root_data_dir root path to the data directory where the output files of the model are written to. root_ancil_dir root path to the location of the ancillary files. The files should be located in a sub-directory of this path with the name of the model ID. Default: Path to the directory containing the ancillary files located in the CDDS home directory root_hybrid_heights_dir root path to the location of the hybrid heights files. Default:: Path to the directory containing hybrid heights files located in the CDDS home directory root_replacement_coordinates_dir root path to the location of the replacement coordinates files. Default: Path to the directory containing the replacement coordinates files located in the CDDS home directory sites_files path to the file containing the sites information. Default: Path to the file containing the site information located in the CDDS home directory standard_names_dir the directory containing the standard names that should be used. Default : Path to the standard names directory located in the CDDS home directory standard_names_version the version of the standard name directory that should be used. Default: latest simulation if set to True CDDS operation will be simulated log_level identify which log level should be used for logging - CRITICAL , INFO or DEBUG Default: INFO Examples Example [ common ] external_plugin = external_plugin_location = mip_table_dir = /home/h03/cdds/etc/mip_tables/CMIP6/01.00.29/ mode = strict package = round-1-part-1 workflow_basename = UKESM1-0-LL_historical_r1i1p1f2 root_proc_dir = /project/user/proc root_data_dir = /project/user/cdds_data root_ancil_dir = /home/h03/cdds/etc/ancil/ root_hybrid_heights_dir = /home/h03/cdds/etc/vertical_coordinates/ root_replacement_coordinates_dir = /home/h03/cdds/etc/horizontal_coordinates/ sites_file = /home/h03/cdds/etc/cfmip2/cfmip2-sites-orog.txt simulation = False log_level = INFO","title":"Common Section"},{"location":"tutorials/request/common/#common-section","text":"The common section in the request configuration contains common setting like the path to the root data folder or the path to the external plugin.","title":"Common Section"},{"location":"tutorials/request/common/#configuration-values","text":"external_plugin module path to external CDDS plugin, e.g. cdds_arise.arise_plugin external_plugin_location path to the external plugin implementation, e.g. /home/h03/cdds/arise mip_table_dir path to the directory containing the inforamtion about the MIP tables, e.g. /home/h03/cdds/etc/mip_tables/CMIP6/01.00.29/ Default: The path to the MIP table directoring specified in the corresponding plugin mode mode that should be used to CMORised the data - strict or relaxed . If mode is strict , CMOR will fail if there are warnings regarding compliance of CMOR standards. If mode is relaxed , CMOR will only fail if cubes present serve discrepancies with CMOR standards. Default: strict package package name used to distinguish different run throughs of CDDS, e.g. round-1 . workflow_basename name of the workflow name of the CDDS suite that will trigger during the CDDS processing suite Default : <model_id>_<experiment_id>_<variant_label> - all three values are defined in the metadata section root_proc_dir root path to the proc directory where the non-data outputs are written, e.g. log files. root_data_dir root path to the data directory where the output files of the model are written to. root_ancil_dir root path to the location of the ancillary files. The files should be located in a sub-directory of this path with the name of the model ID. Default: Path to the directory containing the ancillary files located in the CDDS home directory root_hybrid_heights_dir root path to the location of the hybrid heights files. Default:: Path to the directory containing hybrid heights files located in the CDDS home directory root_replacement_coordinates_dir root path to the location of the replacement coordinates files. Default: Path to the directory containing the replacement coordinates files located in the CDDS home directory sites_files path to the file containing the sites information. Default: Path to the file containing the site information located in the CDDS home directory standard_names_dir the directory containing the standard names that should be used. Default : Path to the standard names directory located in the CDDS home directory standard_names_version the version of the standard name directory that should be used. Default: latest simulation if set to True CDDS operation will be simulated log_level identify which log level should be used for logging - CRITICAL , INFO or DEBUG Default: INFO","title":"Configuration Values"},{"location":"tutorials/request/common/#examples","text":"Example [ common ] external_plugin = external_plugin_location = mip_table_dir = /home/h03/cdds/etc/mip_tables/CMIP6/01.00.29/ mode = strict package = round-1-part-1 workflow_basename = UKESM1-0-LL_historical_r1i1p1f2 root_proc_dir = /project/user/proc root_data_dir = /project/user/cdds_data root_ancil_dir = /home/h03/cdds/etc/ancil/ root_hybrid_heights_dir = /home/h03/cdds/etc/vertical_coordinates/ root_replacement_coordinates_dir = /home/h03/cdds/etc/horizontal_coordinates/ sites_file = /home/h03/cdds/etc/cfmip2/cfmip2-sites-orog.txt simulation = False log_level = INFO","title":"Examples"},{"location":"tutorials/request/config_request/","text":"Update Request Configuration The request configuration consists of following different sections: inheritance contains a setting that specify a template for the request. This section is optional. metadata contains all metadata settings about the experiment that should be processed, like the model ID or the MIP era. common contains common setting like the path to the root data folder or the path to the external plugin. data contains all settings that are used to archive the data in MASS. inventory contains all settings that are need to connect to and manage the inventory database. conversion contains settings that specify how CDDS is run, e.g., skip any steps when running CDDS. netcdf_global_attributes contains all attributes that will be set in the global attributes section of the CMOR file. misc contains any settings that do not fit in any other section.","title":"Request Configuration"},{"location":"tutorials/request/config_request/#update-request-configuration","text":"The request configuration consists of following different sections: inheritance contains a setting that specify a template for the request. This section is optional. metadata contains all metadata settings about the experiment that should be processed, like the model ID or the MIP era. common contains common setting like the path to the root data folder or the path to the external plugin. data contains all settings that are used to archive the data in MASS. inventory contains all settings that are need to connect to and manage the inventory database. conversion contains settings that specify how CDDS is run, e.g., skip any steps when running CDDS. netcdf_global_attributes contains all attributes that will be set in the global attributes section of the CMOR file. misc contains any settings that do not fit in any other section.","title":"Update Request Configuration"},{"location":"tutorials/request/conversion/","text":"Conversion Section The conversion in the request configuration contains settings that specify how CDDS is run. Configuration Values skip_extract skip the extract task at the start of the CDDS suite for each stream. Default : False , on Jasmin True skip_extract_validation skip validation at the end of the extract task. Default: False skip_configure skip the configure step of the CDDS suite for each stream. Default: False skip_qc skip the quality control task of the CDDS suite for each stream. Default: False skip_archive skip the archive task at the end of the CDDS suite for each stream. Default: False , on Jasmin True cylc_args arguments to be passed to cylc vip for the CDDS processing suite. Default: -v no_email_notifications no email notifications will be sent from the suites. Default: True scale_memory_limits memory scaling factor to be applied to all batch jobs. Please, contact the CDDS team for advice. override_cycling_frequency override default frequency for specified stream. Each stream should be specified along with the cycling frequency using the format <stream>=<frequency> , e.g. ap7=P3M ap8=P1M . slicing slicing period for specified stream. Each stream should be specified along with the slicing period using the format <stream>=<period> , e.g. ap7=year ap8=month . Only, year and month as period are supported. model_params_dir the path to the directory containing the model parameters that should be overloaded the model parameters in the plugin. continue_if_mip_convert_failed specify if the MIP convert task will fail if any errors occur or only if the data of all streams can not be processed. Default: False delete_preexisting_proc_dir specify if a pre-existing CDDS proc directory will be deleted when creating the CDDS directories. Default: False delete_preexisting_data_dir specify if a pre-existing CDDS data directory will be deleted when creating the CDDS directories. Default: False Examples Example [ conversion ] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = ap7=P3M model_params_dir = /home/user/model_params.json continue_if_mip_convert_failed = False delete_preexisting_proc_dir = False delete_preexisting_data_dir = False","title":"Conversion Section"},{"location":"tutorials/request/conversion/#conversion-section","text":"The conversion in the request configuration contains settings that specify how CDDS is run.","title":"Conversion Section"},{"location":"tutorials/request/conversion/#configuration-values","text":"skip_extract skip the extract task at the start of the CDDS suite for each stream. Default : False , on Jasmin True skip_extract_validation skip validation at the end of the extract task. Default: False skip_configure skip the configure step of the CDDS suite for each stream. Default: False skip_qc skip the quality control task of the CDDS suite for each stream. Default: False skip_archive skip the archive task at the end of the CDDS suite for each stream. Default: False , on Jasmin True cylc_args arguments to be passed to cylc vip for the CDDS processing suite. Default: -v no_email_notifications no email notifications will be sent from the suites. Default: True scale_memory_limits memory scaling factor to be applied to all batch jobs. Please, contact the CDDS team for advice. override_cycling_frequency override default frequency for specified stream. Each stream should be specified along with the cycling frequency using the format <stream>=<frequency> , e.g. ap7=P3M ap8=P1M . slicing slicing period for specified stream. Each stream should be specified along with the slicing period using the format <stream>=<period> , e.g. ap7=year ap8=month . Only, year and month as period are supported. model_params_dir the path to the directory containing the model parameters that should be overloaded the model parameters in the plugin. continue_if_mip_convert_failed specify if the MIP convert task will fail if any errors occur or only if the data of all streams can not be processed. Default: False delete_preexisting_proc_dir specify if a pre-existing CDDS proc directory will be deleted when creating the CDDS directories. Default: False delete_preexisting_data_dir specify if a pre-existing CDDS data directory will be deleted when creating the CDDS directories. Default: False","title":"Configuration Values"},{"location":"tutorials/request/conversion/#examples","text":"Example [ conversion ] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = ap7=P3M model_params_dir = /home/user/model_params.json continue_if_mip_convert_failed = False delete_preexisting_proc_dir = False delete_preexisting_data_dir = False","title":"Examples"},{"location":"tutorials/request/data/","text":"Data Section The data section in the request configuration contains all settings that are used to archive the data in MASS. Configuration Values data_version version of the data to archive. Format: v%Y%m%d Default: version of current date end_date end date for the processing. start_date start date for the processing. mass_data_class root of the location of input dataset on MASS, either crum or ens . Default: crum mass_ensemble_member identifier of the ensemble member for PPE simulations model_workflow_id workflow ID of the simulation model. max_file_size maximum netCDF file size in bytes Default: 20e9 (20GB) model_workflow_branch name of the workflow branch of the simulation model. Default: cdds model_workflow_revision workflow revision of the simulation model. Default: HEAD streams restrict only to these streams. If empty, all streams will be processed. variable_list_file path to the user created variables file. output_mass_root full path to the root MASS location to use for archiving the output data, e.g. moose:/adhoc/users/<user.name>/ output_mass_suffix sub-directory in MASS to use when moving data. This directory is appended to the root mass location defined in output_mass_root . Examples Example [ data ] data_version = v20240212 end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = max_file_size = 20e9 start_date = 1979-01-01T00:00:00Z model_workflow_id = u-bp880 model_workflow_branch = cdds model_workflow_revision = HEAD streams = ap4 ap5 ap6 variable_list_file = /home/h03/cdds/variables/variables.txt output_mass_root = moose:/adhoc/users/some.user/ output_mass_suffix = cdds","title":"Data Section"},{"location":"tutorials/request/data/#data-section","text":"The data section in the request configuration contains all settings that are used to archive the data in MASS.","title":"Data Section"},{"location":"tutorials/request/data/#configuration-values","text":"data_version version of the data to archive. Format: v%Y%m%d Default: version of current date end_date end date for the processing. start_date start date for the processing. mass_data_class root of the location of input dataset on MASS, either crum or ens . Default: crum mass_ensemble_member identifier of the ensemble member for PPE simulations model_workflow_id workflow ID of the simulation model. max_file_size maximum netCDF file size in bytes Default: 20e9 (20GB) model_workflow_branch name of the workflow branch of the simulation model. Default: cdds model_workflow_revision workflow revision of the simulation model. Default: HEAD streams restrict only to these streams. If empty, all streams will be processed. variable_list_file path to the user created variables file. output_mass_root full path to the root MASS location to use for archiving the output data, e.g. moose:/adhoc/users/<user.name>/ output_mass_suffix sub-directory in MASS to use when moving data. This directory is appended to the root mass location defined in output_mass_root .","title":"Configuration Values"},{"location":"tutorials/request/data/#examples","text":"Example [ data ] data_version = v20240212 end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = max_file_size = 20e9 start_date = 1979-01-01T00:00:00Z model_workflow_id = u-bp880 model_workflow_branch = cdds model_workflow_revision = HEAD streams = ap4 ap5 ap6 variable_list_file = /home/h03/cdds/variables/variables.txt output_mass_root = moose:/adhoc/users/some.user/ output_mass_suffix = cdds","title":"Examples"},{"location":"tutorials/request/global_attributes/","text":"NetCDF Global Attributes Section The netcdf_global_attributes in the request configuration contains all attributes that will be set in the global attributes section of the CMOR file. Configuration Values attributes attributes that will be set in the global attributes section of the CMOR file. Examples Example NetCDF global attributes of a CORDEX experiment: [ netcdf_global_attributes ] driving_experiment = evaluation driving_experiment_id = evaluation driving_institution_id = MOHC driving_source_id = HadREM3-GA7-05 driving_model_ensemble_member = r1i1p1 driving_experiment_name = evaluation driving_variant_label = r1i1p1f2 nesting_levels = 1 rcm_version_id = v1 project_id = CORDEX-FPSCONV domain_id = EUR-11 domain = Europe source_configuration_id = v1.0 further_info_url = https://furtherinfo.es-doc.org/","title":"Global Attributes Section"},{"location":"tutorials/request/global_attributes/#netcdf-global-attributes-section","text":"The netcdf_global_attributes in the request configuration contains all attributes that will be set in the global attributes section of the CMOR file.","title":"NetCDF Global Attributes Section"},{"location":"tutorials/request/global_attributes/#configuration-values","text":"attributes attributes that will be set in the global attributes section of the CMOR file.","title":"Configuration Values"},{"location":"tutorials/request/global_attributes/#examples","text":"Example NetCDF global attributes of a CORDEX experiment: [ netcdf_global_attributes ] driving_experiment = evaluation driving_experiment_id = evaluation driving_institution_id = MOHC driving_source_id = HadREM3-GA7-05 driving_model_ensemble_member = r1i1p1 driving_experiment_name = evaluation driving_variant_label = r1i1p1f2 nesting_levels = 1 rcm_version_id = v1 project_id = CORDEX-FPSCONV domain_id = EUR-11 domain = Europe source_configuration_id = v1.0 further_info_url = https://furtherinfo.es-doc.org/","title":"Examples"},{"location":"tutorials/request/inheritance/","text":"Inheritance Section The inheritance in the request configuration is optional and contains a path to a template for the request. Configuration Values template path to the template for the request that contains values that will be added to the request configuration. The request configuration can override values in the template. Examples Example [ inheritance ] template = /path/to/request-template.cfg","title":"Inheritance Section"},{"location":"tutorials/request/inheritance/#inheritance-section","text":"The inheritance in the request configuration is optional and contains a path to a template for the request.","title":"Inheritance Section"},{"location":"tutorials/request/inheritance/#configuration-values","text":"template path to the template for the request that contains values that will be added to the request configuration. The request configuration can override values in the template.","title":"Configuration Values"},{"location":"tutorials/request/inheritance/#examples","text":"Example [ inheritance ] template = /path/to/request-template.cfg","title":"Examples"},{"location":"tutorials/request/inventory/","text":"Inventory Section The inventory section in the request configuration contains all settings to connect to and manage the inventory database. Configuration Values inventory_check specify if the inventory should be used to determine of a variables is active or not. Default: False inventory_database_location the path to the inventory database. Examples Example [ inventory ] inventory_check = True inventory_database_location = /home/user/inventory.db","title":"Inventory Section"},{"location":"tutorials/request/inventory/#inventory-section","text":"The inventory section in the request configuration contains all settings to connect to and manage the inventory database.","title":"Inventory Section"},{"location":"tutorials/request/inventory/#configuration-values","text":"inventory_check specify if the inventory should be used to determine of a variables is active or not. Default: False inventory_database_location the path to the inventory database.","title":"Configuration Values"},{"location":"tutorials/request/inventory/#examples","text":"Example [ inventory ] inventory_check = True inventory_database_location = /home/user/inventory.db","title":"Examples"},{"location":"tutorials/request/metadata/","text":"Metadata Section The metadata section in the request configuration contains all metadata settings about the experiment that should be processed. Configuration Values branch_method branching procedure - standard , continuation or no parent base_date used to define the units of the time coordinate in the netCDF file. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 1850-01-01T00:00:00Z Default: 1850-01-01T00:00:00Z calendar that should be used - 360_day or gregorian experiment_id experiment identifier institution_id institution identifier, e.g. MOHC for Met Office Hadley Centre license license restrictions. It ensures that anyone using the files has access to the terms of use. Default: The licenses that the corresponding plugin is provided for the given MIP era. mip the model intercomparison project, e.g. ScenarioMIP mip_era the associated cycle, e.g. CMIP6 , GCDevModel , CORDEX sub_experiment_id identifier of the sub experiment. For example, it is needed for CMIP6 hindcast or forecast experiments to indicate start year. If no sub experiment is given, set it to none Default: none variant_label the label of the variant of the experiment that should be considered. model_id a short name identifying the model, also know as source_id , e.g. HadGEM3-GC31-LL . model_type a text code identifying which model components are used in the given experiments separated by white spaces, e.g. AOGCM AER BGC Configuration values for parent experiment Following configuration values are only needed to be set if the branch_method is standard or continuation : branch_date_in_child branch data with respect to child's time axis. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 1990-01-01T00:00:00Z branch_date_in_parent branch date with respect to parent time axis Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 2000-01-01T00:00:00Z parent_base_date used to define the units of the time coordinate in the netCDF files. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 2005-01-01T00:00:00Z Default: 1850-01-01T00:00:00Z parent_experiment_id parent experiment identifier, e.g. piControl parent_mip the model intercomparison project of the parent experiment parent_mip_era parent's associated MIP cycle, e.g. CMIP6 parent_model_id a short name identifying the parent model, e.g. HadGEM3-GC31-LL . Default : The same model ID of the experiment that should be processed parent_time_units the time units used in the parent experiment. Default: days since 1850-01-01 parent_variant_label the label of the specific variant of the parent experiment that should be considered. Examples CIMP6 experiment without parent [ metadata ] branch_method = no parent calendar = 360_day experiment_id = amip institution_id = MOHC license = CMIP6 model data produced by MOHC is licensed under a Creative Commons Attribution ShareAlike 4.0 International License mip = CMIP mip_era = CMIP6 sub_experiment_id = none variant_label = r1i1p1f4 model_id = UKESM1-0-LL model_type = AGCM AER CHEM standard_names_version = latest standard_names_dir = /home/h03/cdds/etc/standard_names CMIP6 experiment with parent [ metadata ] branch_date_in_child = 1850-01-01T00:00:00Z branch_date_in_parent = 2250-01-01T00:00:00Z branch_method = standard child_base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = historical institution_id = MOHC license = CMIP6 model data produced by MOHC is licensed under a Creative Commons Attribution ShareAlike 4.0 International License. mip = CMIP mip_era = CMIP6 parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = piControl parent_mip = CMIP parent_mip_era = CMIP6 parent_model_id = UKESM1-0-LL parent_time_units = days since 1850-01-01 parent_variant_label = r1i1p1f2 sub_experiment_id = none variant_label = r1i1p1f2 standard_names_version = latest standard_names_dir = /home/h03/cdds/etc/standard_names/ model_id = UKESM1-0-LL model_type = AOGCM BGC AER CHEM","title":"Metadata Section"},{"location":"tutorials/request/metadata/#metadata-section","text":"The metadata section in the request configuration contains all metadata settings about the experiment that should be processed.","title":"Metadata Section"},{"location":"tutorials/request/metadata/#configuration-values","text":"branch_method branching procedure - standard , continuation or no parent base_date used to define the units of the time coordinate in the netCDF file. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 1850-01-01T00:00:00Z Default: 1850-01-01T00:00:00Z calendar that should be used - 360_day or gregorian experiment_id experiment identifier institution_id institution identifier, e.g. MOHC for Met Office Hadley Centre license license restrictions. It ensures that anyone using the files has access to the terms of use. Default: The licenses that the corresponding plugin is provided for the given MIP era. mip the model intercomparison project, e.g. ScenarioMIP mip_era the associated cycle, e.g. CMIP6 , GCDevModel , CORDEX sub_experiment_id identifier of the sub experiment. For example, it is needed for CMIP6 hindcast or forecast experiments to indicate start year. If no sub experiment is given, set it to none Default: none variant_label the label of the variant of the experiment that should be considered. model_id a short name identifying the model, also know as source_id , e.g. HadGEM3-GC31-LL . model_type a text code identifying which model components are used in the given experiments separated by white spaces, e.g. AOGCM AER BGC","title":"Configuration Values"},{"location":"tutorials/request/metadata/#configuration-values-for-parent-experiment","text":"Following configuration values are only needed to be set if the branch_method is standard or continuation : branch_date_in_child branch data with respect to child's time axis. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 1990-01-01T00:00:00Z branch_date_in_parent branch date with respect to parent time axis Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 2000-01-01T00:00:00Z parent_base_date used to define the units of the time coordinate in the netCDF files. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 2005-01-01T00:00:00Z Default: 1850-01-01T00:00:00Z parent_experiment_id parent experiment identifier, e.g. piControl parent_mip the model intercomparison project of the parent experiment parent_mip_era parent's associated MIP cycle, e.g. CMIP6 parent_model_id a short name identifying the parent model, e.g. HadGEM3-GC31-LL . Default : The same model ID of the experiment that should be processed parent_time_units the time units used in the parent experiment. Default: days since 1850-01-01 parent_variant_label the label of the specific variant of the parent experiment that should be considered.","title":"Configuration values for parent experiment"},{"location":"tutorials/request/metadata/#examples","text":"CIMP6 experiment without parent [ metadata ] branch_method = no parent calendar = 360_day experiment_id = amip institution_id = MOHC license = CMIP6 model data produced by MOHC is licensed under a Creative Commons Attribution ShareAlike 4.0 International License mip = CMIP mip_era = CMIP6 sub_experiment_id = none variant_label = r1i1p1f4 model_id = UKESM1-0-LL model_type = AGCM AER CHEM standard_names_version = latest standard_names_dir = /home/h03/cdds/etc/standard_names CMIP6 experiment with parent [ metadata ] branch_date_in_child = 1850-01-01T00:00:00Z branch_date_in_parent = 2250-01-01T00:00:00Z branch_method = standard child_base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = historical institution_id = MOHC license = CMIP6 model data produced by MOHC is licensed under a Creative Commons Attribution ShareAlike 4.0 International License. mip = CMIP mip_era = CMIP6 parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = piControl parent_mip = CMIP parent_mip_era = CMIP6 parent_model_id = UKESM1-0-LL parent_time_units = days since 1850-01-01 parent_variant_label = r1i1p1f2 sub_experiment_id = none variant_label = r1i1p1f2 standard_names_version = latest standard_names_dir = /home/h03/cdds/etc/standard_names/ model_id = UKESM1-0-LL model_type = AOGCM BGC AER CHEM","title":"Examples"},{"location":"tutorials/request/misc/","text":"Miscellany Section The misc in the request configuration contains any settings that do not fit in any other section. Configuration Values atmos_timestep atmospheric time step in seconds Default: The atmospheric time step that the current loaded plugin is provided. use_proc_dir write log files to the appropriate component directory in the proc directory as defined by the common section in the request configuration. Default: True no_overwrite do not overwrite files in data directory defined by the common section in the request configuration. Default: False halo_removal_latitude number of latitude points to be stripped using <start>:<stop> as the format. halo_removal_longitude number of longitude points to be stripped using <start>:<stop> as the format. Example Strip 5 points at the start and end for latitude and strip 10 points at the start and end for longitude: halo_removal_latitude = 5:-5 halo_removal_longitude = 10:-10 force_coordinate_rotation Set to True to enable coordination rotation if coordination system is not rotated by default. Default: False Examples Example [ misc ] atmos_timestep = 900 use_proc_dir = True no_overwrite = False halo_removal_latitude = 5:-5 halo_removal_longitude = 10:-10 force_coordinate_rotation = False","title":"Misc Section"},{"location":"tutorials/request/misc/#miscellany-section","text":"The misc in the request configuration contains any settings that do not fit in any other section.","title":"Miscellany Section"},{"location":"tutorials/request/misc/#configuration-values","text":"atmos_timestep atmospheric time step in seconds Default: The atmospheric time step that the current loaded plugin is provided. use_proc_dir write log files to the appropriate component directory in the proc directory as defined by the common section in the request configuration. Default: True no_overwrite do not overwrite files in data directory defined by the common section in the request configuration. Default: False halo_removal_latitude number of latitude points to be stripped using <start>:<stop> as the format. halo_removal_longitude number of longitude points to be stripped using <start>:<stop> as the format. Example Strip 5 points at the start and end for latitude and strip 10 points at the start and end for longitude: halo_removal_latitude = 5:-5 halo_removal_longitude = 10:-10 force_coordinate_rotation Set to True to enable coordination rotation if coordination system is not rotated by default. Default: False","title":"Configuration Values"},{"location":"tutorials/request/misc/#examples","text":"Example [ misc ] atmos_timestep = 900 use_proc_dir = True no_overwrite = False halo_removal_latitude = 5:-5 halo_removal_longitude = 10:-10 force_coordinate_rotation = False","title":"Examples"}]}