{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Climate Data Dissemination System The Climate Data Dissemination System (CDDS) is a Python-based system that manages the reprocessing of HadGEM3 and UKESM1 climate model data into a standards compliant ( CMOR ) form suitable for publication and sharing. The primary driver behind CDDS was the CMIP6 project and CDDS was used, and is continuing to be used, to deliver a large amount of data to the Centre for Environmental Data Archival (CEDA) for publication to ESGF. CDDS has recently been adapted to allow for the easy addition of both models and projects, provided that they follow the structure for CMIP6, i.e. via predefined activities (MIPS), source ids (models) and experiments. As of version 2.2.4 CDDS supports production of data for CMIP6 and for a Met Office internal project GCModelDev where Met Office scientists are encouraged to request additional activities and experiments that can be used to support their science. Note that the GCModelDev project is not intended to prepare data for publication -- anyone wanting to publish data or prepare it for an external project is encouraged to contact the CDDS team or start a discussion here. Contact You can contact the CDDS team via cdds@metoffice.gov.uk. CMIP6 Processing Simulation tickets for CMIP6 work can be raised on the CDDS Trac system (SRS login required) via Open a new operational simulation ticket (v3.0) CMIP6 Operational Procedure CMIP6 Simulation Ticket Review Procedure","title":"About"},{"location":"#climate-data-dissemination-system","text":"The Climate Data Dissemination System (CDDS) is a Python-based system that manages the reprocessing of HadGEM3 and UKESM1 climate model data into a standards compliant ( CMOR ) form suitable for publication and sharing. The primary driver behind CDDS was the CMIP6 project and CDDS was used, and is continuing to be used, to deliver a large amount of data to the Centre for Environmental Data Archival (CEDA) for publication to ESGF. CDDS has recently been adapted to allow for the easy addition of both models and projects, provided that they follow the structure for CMIP6, i.e. via predefined activities (MIPS), source ids (models) and experiments. As of version 2.2.4 CDDS supports production of data for CMIP6 and for a Met Office internal project GCModelDev where Met Office scientists are encouraged to request additional activities and experiments that can be used to support their science. Note that the GCModelDev project is not intended to prepare data for publication -- anyone wanting to publish data or prepare it for an external project is encouraged to contact the CDDS team or start a discussion here.","title":"Climate Data Dissemination System"},{"location":"#contact","text":"You can contact the CDDS team via cdds@metoffice.gov.uk.","title":"Contact"},{"location":"#cmip6-processing","text":"Simulation tickets for CMIP6 work can be raised on the CDDS Trac system (SRS login required) via Open a new operational simulation ticket (v3.0) CMIP6 Operational Procedure CMIP6 Simulation Ticket Review Procedure","title":"CMIP6 Processing"},{"location":"cdds_components/","text":"List of Components Component Name Description archive The cdds_transfer package enables a user to store the output data products in the MASS archive and make them available for download by the ESGF node run by CEDA. common The cdds_common package will supersedes the HadSDK component. It contains a collection of generic Python code used by one or more of the CDDS components. configure The cdds_configure package enables a user to produce the user configuration file for MIP Convert. convert - extract The extract package enables a user to efficiently extract the climate data that will be used in the dissemination process from MASS. mip_convert The mip_convert package enables a user to produce the output netCDF files for a MIP using model output files. misc - prepare The cdds_prepare package enables a user to create the requested variables list and directory structures in preparation for subsequent CDDS components to be run. qc The cdds_qc package enables a user to check whether the output netCDF files conform to the WGCM CMIP standards. Special Plugin Components Component Name Description QC CF1.7 Plugin The CF1.7 plugin (cdds_qc_plugin_cf17) is an extension of the original CF1.6 checker, providing some additional features and configurability. QC CMIP6 Plugin The CMIP6 Compliance Checker Plugin (cdds_qc_plugin_cmip6) provides a suite of tests related to CMIP6 compliance.","title":"CDDS Components"},{"location":"cdds_components/#list-of-components","text":"Component Name Description archive The cdds_transfer package enables a user to store the output data products in the MASS archive and make them available for download by the ESGF node run by CEDA. common The cdds_common package will supersedes the HadSDK component. It contains a collection of generic Python code used by one or more of the CDDS components. configure The cdds_configure package enables a user to produce the user configuration file for MIP Convert. convert - extract The extract package enables a user to efficiently extract the climate data that will be used in the dissemination process from MASS. mip_convert The mip_convert package enables a user to produce the output netCDF files for a MIP using model output files. misc - prepare The cdds_prepare package enables a user to create the requested variables list and directory structures in preparation for subsequent CDDS components to be run. qc The cdds_qc package enables a user to check whether the output netCDF files conform to the WGCM CMIP standards.","title":"List of Components"},{"location":"cdds_components/#special-plugin-components","text":"Component Name Description QC CF1.7 Plugin The CF1.7 plugin (cdds_qc_plugin_cf17) is an extension of the original CF1.6 checker, providing some additional features and configurability. QC CMIP6 Plugin The CMIP6 Compliance Checker Plugin (cdds_qc_plugin_cmip6) provides a suite of tests related to CMIP6 compliance.","title":"Special Plugin Components"},{"location":"cdds_components/convert/","text":"Warning This documentation is currently under construction and may not be up to date. The cdds.convert component designed to check out, configure and run a copy of the suite, specified in the config. Suite \u200bu-ak283 is currently set up for this purpose. Development workflow To work on suite developments please follow the standard practices set out in the Development Workflow and use the --rose-suite-branch argument to cdds_convert to point at the branch you wish to use when running. Release procedure Create a branch named cdds_ , e.g. cdds_1.0.0 of the suite and make the change shown in changeset of the rose suite u-ak283. Create a branch the config and modify the rose_suite_branch settings, e.g. to cdds_1.0.0@100752 if revision 100752 is appropriate, in the [convert] section of CMIP6.cfg. If a different suite id is being used then the rose_suite setting will also need altering. Review config branch as per standard practises, merge to trunk and update config checkout on disk under the cdds account.","title":"convert"},{"location":"cdds_components/convert/#development-workflow","text":"To work on suite developments please follow the standard practices set out in the Development Workflow and use the --rose-suite-branch argument to cdds_convert to point at the branch you wish to use when running.","title":"Development workflow"},{"location":"cdds_components/convert/#release-procedure","text":"Create a branch named cdds_ , e.g. cdds_1.0.0 of the suite and make the change shown in changeset of the rose suite u-ak283. Create a branch the config and modify the rose_suite_branch settings, e.g. to cdds_1.0.0@100752 if revision 100752 is appropriate, in the [convert] section of CMIP6.cfg. If a different suite id is being used then the rose_suite setting will also need altering. Review config branch as per standard practises, merge to trunk and update config checkout on disk under the cdds account.","title":"Release procedure"},{"location":"cdds_components/extract/","text":"Warning This documentation is currently under construction and may not be up to date. CDDS Extract The Extract package enables a user to extract a subset of climate model output files from the MASS tape archive. This is useful because raw atmosphere and ocean output is stored in massive files, containing multiple fields, and their large volume make all processing very slow. Before data transfer happens, Extract selects only relevant bits of output, necessary to produce the requested, CMORised variables. Since CDDS 2.2, Extract is embedded within the cdds_convert script, and it is no longer necessary to run it as a separate process. It is also worth mentioning that despite working with only a subset of climate model output, data transfer from MASS is typically the longest step in the whole CDDS processing pipeline, can take many days, and is prone to failures caused by MASS misbehaviour. An overview of Extract The user configures the extraction process using the request file. The cdds_extract script then performs the following processing steps: retrieves the data dissemination configuration from the request file. retrieves the requested variables list for the experiment. retrieves the model to MIP mapping for each MIP requested variable logs information on the variables for which authorised mappings are available and variables/mappings that are not available and why. creates directories for holding the retrieved data from MASS, the post-processed data which will be disseminated and the directories used to hold the artifacts used by other cdds processes (e.g. log files) creates appropriate PP and netCDF variable filter files that are used to filter the data retrieved from MASS before it is copied to disk. checks that MASS holds the relevant data collections and constructs MOOSE commands that will retrieve the required model data from these data collections (optionally incorporating the relevant filter files) submits constructed MOOSE commands and reports progress in text logs performs validation on the retrieved data to check it meets processing requirement","title":"extract"},{"location":"cdds_components/extract/#cdds-extract","text":"The Extract package enables a user to extract a subset of climate model output files from the MASS tape archive. This is useful because raw atmosphere and ocean output is stored in massive files, containing multiple fields, and their large volume make all processing very slow. Before data transfer happens, Extract selects only relevant bits of output, necessary to produce the requested, CMORised variables. Since CDDS 2.2, Extract is embedded within the cdds_convert script, and it is no longer necessary to run it as a separate process. It is also worth mentioning that despite working with only a subset of climate model output, data transfer from MASS is typically the longest step in the whole CDDS processing pipeline, can take many days, and is prone to failures caused by MASS misbehaviour.","title":"CDDS Extract"},{"location":"cdds_components/extract/#an-overview-of-extract","text":"The user configures the extraction process using the request file. The cdds_extract script then performs the following processing steps: retrieves the data dissemination configuration from the request file. retrieves the requested variables list for the experiment. retrieves the model to MIP mapping for each MIP requested variable logs information on the variables for which authorised mappings are available and variables/mappings that are not available and why. creates directories for holding the retrieved data from MASS, the post-processed data which will be disseminated and the directories used to hold the artifacts used by other cdds processes (e.g. log files) creates appropriate PP and netCDF variable filter files that are used to filter the data retrieved from MASS before it is copied to disk. checks that MASS holds the relevant data collections and constructs MOOSE commands that will retrieve the required model data from these data collections (optionally incorporating the relevant filter files) submits constructed MOOSE commands and reports progress in text logs performs validation on the retrieved data to check it meets processing requirement","title":"An overview of Extract"},{"location":"cdds_components/qc/","text":"Warning This documentation is currently under construction and may not be up to date.","title":"qc"},{"location":"cdds_components/transfer/","text":"The following is from the CDDS Trac pages and relate to the construction of queues on the RabbitMQ server at CEDA Adding a Queue Description We need to add a new type of queue (to the existing set of \"moose\" and \"admin\"), or an additional queue to the original set (\"moose.available\", \"moose.withdrawn\", \"admin.critical\", \"admin.info\"). We have agreed with CEDA to set up queues per project, e.g. CMIP6_available and CMIP6_withdrawn. There is also a testing set of queues to avoid functional unit tests in cdds_transfer from nuking the queues. Implementation There's no method in the API to create queues directly, although invoking message methods (e.g. publishing a message) will create durable queues automatically. If you want to create the queues in advance, you can do so using pika calls. Example code import os import pika from cdds_transfer import config credentials_file = os . path . expandvars ( '$HOME/.cdds_credentials' ) cfg = config . Config ([ credentials_file ]) # get rabbit connection details from the rabbit section of the # cdds credentials file rabbit_cfg = dict ( cfg . _cp . items ( 'rabbit' )) # Prefix to use to construct queue names mip_era = 'CMIP6' # Name of the exchange on the rabbit server that directs # messages to queues. exchange = 'dds' credentials = pika . PlainCredentials ( rabbit_cfg [ 'userid' ], rabbit_cfg [ 'password' ]) connection_parameters = pika . ConnectionParameters ( rabbit_cfg [ 'host' ], rabbit_cfg [ 'port' ], rabbit_cfg [ 'vhost' ], credentials = credentials , ssl = False ) connection = pika . BlockingConnection ( connection_parameters ) channel = connection . channel () channel . exchange_declare ( exchange = exchange , exchange_type = \"direct\" , durable = True ) for queue in [ \"available\" , \"withdrawn\" ]: queue_name = 'moose. {} _ {} ' . format ( mip_era , queue ) channel . queue_declare ( queue = queue_name , durable = True ) channel . queue_bind ( exchange = exchange , queue = queue_name , routing_key = queue_name ) channel . close () connection . close () Clearing a Queue Description You need to connect to a remote queue and clear out all the messages it contains, optionally keeping back-up copies of the messages you remove in the local message store. Implementation Note: this page describes how to use the API to clear a queue. You can also use RabbitMQ's web management interface to purge messages from a queue, if you have the necessary access permissions. create a cdds_transfer.config.Config object to wrap your local configuration. You will need to configure Rabbit, and you may also need to configure your local directories (if you want to save copies of messages to the local message store). create a cdds_transfer.msg.Queue object to point to the remote queue create a cdds_transfer.msg.Communication object loop over all the messages returned by get_all_messages in reverse order1: invoke the remove_message method (optional) invoke the store_message method to save a copy of the message in the local message store Note: if you don't keep a back-up copy of a message that you delete, there is no way it can be recovered. If you accidentally delete a message without a back-up copy, you will need to regenerate and resend the message. 1 Messages are identified by their delivery_tag, but this is not a unique identifier for a message it is just the position in the queue, so if you attempt to remove delivery_tag 1 then 2 then 3 the change of delivery_tag following each removal will result in the removal of the first, third and fifth messages from the original queue state. Performing the removals in reverse order avoids this problem. This highlights an issue with using the delivery_tag for message identification. Example code from cdds_transfer import config , msg cfg = config . Config ([ '.cdds_credentials' ]) queue = msg . Queue ( 'moose' , 'CMIP6_available' ) comm = msg . Communication ( cfg ) for message in comm . get_all_messages ( queue )[:: - 1 ]: # reverse order comm . remove_message ( queue , message ) # comm.store_message(message) # optional Removing a queue Description A queue needs to be deleted, either because one with a new name and the same function has been created, or because the queue was created by mistake. Implementation The API does not provide a method for deleting a queue. Instead, you will either need to use the RabbitMQ web API, or use the delete_queue method provided by pika to delete the queue. Example code import os import pika from cdds_transfer import config credentials_file = os . path . expandvars ( '$HOME/.cdds_credentials' ) cfg = config . Config ([ credentials_file ]) # get rabbit connection details from the rabbit section of the # cdds credentials file rabbit_cfg = dict ( cfg . _cp . items ( 'rabbit' )) # Name of the queue to be deleted queue_name = 'queue_to_be_deleted' # Set up and open connection credentials = pika . PlainCredentials ( rabbit_cfg [ 'userid' ], rabbit_cfg [ 'password' ]) connection_parameters = pika . ConnectionParameters ( rabbit_cfg [ 'host' ], rabbit_cfg [ 'port' ], rabbit_cfg [ 'vhost' ], credentials = credentials , ssl = False ) connection = pika . BlockingConnection ( connection_parameters ) channel = connection . channel () # Delete the queue channel . queue_delete ( queue = queue_name ) # Close down the connection channel . close () connection . close ()","title":"transfer"},{"location":"cdds_components/transfer/#adding-a-queue","text":"","title":"Adding a Queue"},{"location":"cdds_components/transfer/#description","text":"We need to add a new type of queue (to the existing set of \"moose\" and \"admin\"), or an additional queue to the original set (\"moose.available\", \"moose.withdrawn\", \"admin.critical\", \"admin.info\"). We have agreed with CEDA to set up queues per project, e.g. CMIP6_available and CMIP6_withdrawn. There is also a testing set of queues to avoid functional unit tests in cdds_transfer from nuking the queues.","title":"Description"},{"location":"cdds_components/transfer/#implementation","text":"There's no method in the API to create queues directly, although invoking message methods (e.g. publishing a message) will create durable queues automatically. If you want to create the queues in advance, you can do so using pika calls.","title":"Implementation"},{"location":"cdds_components/transfer/#example-code","text":"import os import pika from cdds_transfer import config credentials_file = os . path . expandvars ( '$HOME/.cdds_credentials' ) cfg = config . Config ([ credentials_file ]) # get rabbit connection details from the rabbit section of the # cdds credentials file rabbit_cfg = dict ( cfg . _cp . items ( 'rabbit' )) # Prefix to use to construct queue names mip_era = 'CMIP6' # Name of the exchange on the rabbit server that directs # messages to queues. exchange = 'dds' credentials = pika . PlainCredentials ( rabbit_cfg [ 'userid' ], rabbit_cfg [ 'password' ]) connection_parameters = pika . ConnectionParameters ( rabbit_cfg [ 'host' ], rabbit_cfg [ 'port' ], rabbit_cfg [ 'vhost' ], credentials = credentials , ssl = False ) connection = pika . BlockingConnection ( connection_parameters ) channel = connection . channel () channel . exchange_declare ( exchange = exchange , exchange_type = \"direct\" , durable = True ) for queue in [ \"available\" , \"withdrawn\" ]: queue_name = 'moose. {} _ {} ' . format ( mip_era , queue ) channel . queue_declare ( queue = queue_name , durable = True ) channel . queue_bind ( exchange = exchange , queue = queue_name , routing_key = queue_name ) channel . close () connection . close ()","title":"Example code"},{"location":"cdds_components/transfer/#clearing-a-queue","text":"","title":"Clearing a Queue"},{"location":"cdds_components/transfer/#description_1","text":"You need to connect to a remote queue and clear out all the messages it contains, optionally keeping back-up copies of the messages you remove in the local message store.","title":"Description"},{"location":"cdds_components/transfer/#implementation_1","text":"Note: this page describes how to use the API to clear a queue. You can also use RabbitMQ's web management interface to purge messages from a queue, if you have the necessary access permissions. create a cdds_transfer.config.Config object to wrap your local configuration. You will need to configure Rabbit, and you may also need to configure your local directories (if you want to save copies of messages to the local message store). create a cdds_transfer.msg.Queue object to point to the remote queue create a cdds_transfer.msg.Communication object loop over all the messages returned by get_all_messages in reverse order1: invoke the remove_message method (optional) invoke the store_message method to save a copy of the message in the local message store Note: if you don't keep a back-up copy of a message that you delete, there is no way it can be recovered. If you accidentally delete a message without a back-up copy, you will need to regenerate and resend the message. 1 Messages are identified by their delivery_tag, but this is not a unique identifier for a message it is just the position in the queue, so if you attempt to remove delivery_tag 1 then 2 then 3 the change of delivery_tag following each removal will result in the removal of the first, third and fifth messages from the original queue state. Performing the removals in reverse order avoids this problem. This highlights an issue with using the delivery_tag for message identification.","title":"Implementation"},{"location":"cdds_components/transfer/#example-code_1","text":"from cdds_transfer import config , msg cfg = config . Config ([ '.cdds_credentials' ]) queue = msg . Queue ( 'moose' , 'CMIP6_available' ) comm = msg . Communication ( cfg ) for message in comm . get_all_messages ( queue )[:: - 1 ]: # reverse order comm . remove_message ( queue , message ) # comm.store_message(message) # optional","title":"Example code"},{"location":"cdds_components/transfer/#removing-a-queue","text":"","title":"Removing a queue"},{"location":"cdds_components/transfer/#description_2","text":"A queue needs to be deleted, either because one with a new name and the same function has been created, or because the queue was created by mistake.","title":"Description"},{"location":"cdds_components/transfer/#implementation_2","text":"The API does not provide a method for deleting a queue. Instead, you will either need to use the RabbitMQ web API, or use the delete_queue method provided by pika to delete the queue.","title":"Implementation"},{"location":"cdds_components/transfer/#example-code_2","text":"import os import pika from cdds_transfer import config credentials_file = os . path . expandvars ( '$HOME/.cdds_credentials' ) cfg = config . Config ([ credentials_file ]) # get rabbit connection details from the rabbit section of the # cdds credentials file rabbit_cfg = dict ( cfg . _cp . items ( 'rabbit' )) # Name of the queue to be deleted queue_name = 'queue_to_be_deleted' # Set up and open connection credentials = pika . PlainCredentials ( rabbit_cfg [ 'userid' ], rabbit_cfg [ 'password' ]) connection_parameters = pika . ConnectionParameters ( rabbit_cfg [ 'host' ], rabbit_cfg [ 'port' ], rabbit_cfg [ 'vhost' ], credentials = credentials , ssl = False ) connection = pika . BlockingConnection ( connection_parameters ) channel = connection . channel () # Delete the queue channel . queue_delete ( queue = queue_name ) # Close down the connection channel . close () connection . close ()","title":"Example code"},{"location":"developer_documentation/","text":"Warning This documentation is currently under construction and may not be up to date. Welcome to the cdds developer documentation. Contributing to CDDS The development work for CDDS is managed via JIRA","title":"Developer Documentation"},{"location":"developer_documentation/#contributing-to-cdds","text":"The development work for CDDS is managed via JIRA","title":"Contributing to CDDS"},{"location":"developer_documentation/building_documentation/","text":"Warning This documentation is currently under construction and may not be up to date. Modifying and Building the CDDS Documentation This page gives an overview of the general philosophy of the CDDS documentation. The current documentation is based upon mkdocs and is built and managed using two main packages. mkdocs-material - A particular mkdocs theme which also extends the base functionality. mike - A tool for managing multiple independent versions of documentation on a particular branch. For most purposes it is only neccessary to reference the mkdocs-material package documentation and the mike documentation. Working with mkdocs The source files are written in markdown, and are kept in the docs directory of the repository. Configuration of the building of mkdocs is done using the mkdocs.yml file. Whilst editing or adding documentation you can preview changes in realtime using by starting a local server. mkdocs serve To generate the actual site that can be uploaded to a web server you would use. mkdocs build However, it should not be needed to run this command directly in order to build and deploy the docs. This is done using the mike package (see next section). Working with mike It is worth familiarising with the overview of mike here. In short though, mike makes it relatively straightforward to manage multiple versions of docs by running the mkdocs build command and automatically commiting this to the gh-pages branch. mike deploy 3.0 Similarly to mkdocs serve , you can use the following command to start a local server to preview the docs. mike serve However, rather than displaying the docs in your CWD , this will serve the docs from the gh-pages branch. Managing Versions and Deployment in Practice Deploying the documentation in practice","title":"Documentation"},{"location":"developer_documentation/building_documentation/#modifying-and-building-the-cdds-documentation","text":"This page gives an overview of the general philosophy of the CDDS documentation. The current documentation is based upon mkdocs and is built and managed using two main packages. mkdocs-material - A particular mkdocs theme which also extends the base functionality. mike - A tool for managing multiple independent versions of documentation on a particular branch. For most purposes it is only neccessary to reference the mkdocs-material package documentation and the mike documentation.","title":"Modifying and Building the CDDS Documentation"},{"location":"developer_documentation/building_documentation/#working-with-mkdocs","text":"The source files are written in markdown, and are kept in the docs directory of the repository. Configuration of the building of mkdocs is done using the mkdocs.yml file. Whilst editing or adding documentation you can preview changes in realtime using by starting a local server. mkdocs serve To generate the actual site that can be uploaded to a web server you would use. mkdocs build However, it should not be needed to run this command directly in order to build and deploy the docs. This is done using the mike package (see next section).","title":"Working with mkdocs"},{"location":"developer_documentation/building_documentation/#working-with-mike","text":"It is worth familiarising with the overview of mike here. In short though, mike makes it relatively straightforward to manage multiple versions of docs by running the mkdocs build command and automatically commiting this to the gh-pages branch. mike deploy 3.0 Similarly to mkdocs serve , you can use the following command to start a local server to preview the docs. mike serve However, rather than displaying the docs in your CWD , this will serve the docs from the gh-pages branch.","title":"Working with mike"},{"location":"developer_documentation/building_documentation/#managing-versions-and-deployment-in-practice","text":"Deploying the documentation in practice","title":"Managing Versions and Deployment in Practice"},{"location":"developer_documentation/coding_guidelines/","text":"Warning This documentation is currently under construction and may not be up to date. Python Code Style Guide Use the Style Guide for Python Code Exception - Limit all lines to a maximum of 120 characters for docstrings and comments. (79 in the the PEP 8 guidelines reference ). All CDDS packages should contain a package_name/package_name/tests/test_coding_standards.py module that uses the pycodestyle package to check PEP 8 conformance. In addition to this, it is recommended to use a tool that checks for errors in Python code, coding standard (e.g., PEP 8) conformance and general code quality e.g., pylint . However, some PEP 8 guidelines are not checked by these tools; please also: Be consistent within a module or function reference . In Python, single-quoted strings and double-quoted strings are the same; pick a rule and stick to it reference . Use Python's implied line continuation inside parentheses, brackets and braces to wrap long lines, rather than using a backslash reference . Add a new line before a binary operator, rather than after reference . Use blank lines in functions, sparingly, to indicate logical sections reference . Always use double quote characters for triple-quoted strings \"\"\" to be consistent with the docstring conventions in PEP 257 and PEP 8. Write comments using complete sentences reference . Use the https://docs.python.org/3.8/library/stdtypes.html#str.format str.format() method to perform a string formatting operation, e.g., 'Coordinates: {latitude}, {longitude}'.format(latitude='37.24N', longitude='-115.81W') , since it is the new standard in Python 3 and is preferred to the % formatting. Include the line (c) British Crown Copyright [year of creation in the form <YYYY>]-[year of last modification in the form <YYYY>], Met Office. as a comment at the top of every module. Naming Conventions Abstract Base Classes should have a name that contains Abstract for clarity. Nouns should be used when naming classes. Use descriptive names that clearly convey a meaning; refrain from using overly general / ambiguous names e.g., data. Use the .cfg extension for configuration files, e.g. those read by configparser. Imports Use absolute imports https://www.python.org/dev/peps/pep-0328/#rationale-for-absolute-imports as they are recommended by PEP 8. Also, being able to tell immediately where the function comes from greatly improves code readability and comprehension (Readability Counts). Example: import my_package.my_subpackage.my_module and use the my_module.my_function syntax, e.g., import os; os.walk. from my_package.my_subpackage.my_module import my_function and use my_function directly. group imports, with a blank line between each group, in the following order: standard library imports, related third party imports, local application/library specific imports [reference]. Within each import group (see above), order the imports alphabetically. place module level \"dunders\" after the module docstring but before any import statements except from __future__ imports [reference]. Use the pattern import numpy as np Typing Use https://docs.python.org/3/library/typing.html for adding type hints and annotations. Use mypy for running static code analysis using aforementioned type hints. Docstrings Warning Historically CDDS used the NumpyDoc format for all docstrings but as of 2022 the PEP-257 format was adopted. This means there is currently a mix of docstring formats used throughout the CDDS code. Docstrings should now be written using https://docutils.sourceforge.io/rst.html as recommended by https://www.python.org/dev/peps/pep-0287/, and is rendered using Sphinx. An detailed example of the reStructuredText style docstrings can be found here https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html Use double backticks `` around argument names so that they are rendered as code in the HTML produced by Sphinx Use the appropriate substitutions for glossary terms. Make use of the docstring conventions http://www.python.org/dev/peps/pep-0257. The docstring is a phrase ending in a period and prescribes the function or method's effect as a command, not as a description [reference]. It is not necessary to write docstrings for non-public classes, methods and functions, see cdds/pylintrc and mip_convert/pylintrc. (The maintenance overhead is reduced when refactoring non-public classes, methods and functions). Below is an example reStructuredText Docstring Example docstring incorporating all of the guidelines above. def my_function ( my_param1 : float , my_param2 : str ) -> int : \"\"\" Return something. Here's a longer description about the something that is returned. It's so long, it goes over one line! :param my_param1: Description of the first parameter ``my_param1``. :type my_param1: float :param my_param2: Description of the second parameter ``my_param2``. :type my_param2: string :raises ValueError If ``my_param1`` is less than 0. :return: Description of anonymous integer return value. :rtype: int \"\"\" Doctests Where appropriate, a https://docs.python.org/3.8/library/doctest.html should be included in an Examples section of the docstring https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard. When multiple examples are provided, they should be separated by blank lines. Comments explaining the examples should have blank lines both above and below them. Entry Point Scripts Script names should not have an extension, should be lowercase, and with words separated by underscores as necessary to improve readability, e.g., just_do_it. It is recommended that scripts call a main() function located in an importable module so that it is possible to run the code in the script from the Python interpreter / a test module e.g., import my_module; my_module.main(). https://docs.python.org/3.8/library/argparse.html should be used to parse command line options. def main ( args ): # Parse the arguments. args = parse_args ( args ) # Create the configured logger. configure_logger ( args . log_name , args . log_level , args . append_log ) # Retrieve the logger. logger = logging . getLogger ( __name__ ) try : exit_code = my_func ( args ) except BaseException as exc : exit_code = 1 logger . exception ( exc ) return exit_code Bash Scripts Use the Google Styleguide for bash scripts as recommended by the Cylc documentation Run scripts through Shellcheck for catching possible bad practice. (The latest version can be installed using conda for easier access and improvements over the centrally installed version)","title":"Coding Guidelines"},{"location":"developer_documentation/coding_guidelines/#python-code","text":"","title":"Python Code"},{"location":"developer_documentation/coding_guidelines/#style-guide","text":"Use the Style Guide for Python Code Exception - Limit all lines to a maximum of 120 characters for docstrings and comments. (79 in the the PEP 8 guidelines reference ). All CDDS packages should contain a package_name/package_name/tests/test_coding_standards.py module that uses the pycodestyle package to check PEP 8 conformance. In addition to this, it is recommended to use a tool that checks for errors in Python code, coding standard (e.g., PEP 8) conformance and general code quality e.g., pylint . However, some PEP 8 guidelines are not checked by these tools; please also: Be consistent within a module or function reference . In Python, single-quoted strings and double-quoted strings are the same; pick a rule and stick to it reference . Use Python's implied line continuation inside parentheses, brackets and braces to wrap long lines, rather than using a backslash reference . Add a new line before a binary operator, rather than after reference . Use blank lines in functions, sparingly, to indicate logical sections reference . Always use double quote characters for triple-quoted strings \"\"\" to be consistent with the docstring conventions in PEP 257 and PEP 8. Write comments using complete sentences reference . Use the https://docs.python.org/3.8/library/stdtypes.html#str.format str.format() method to perform a string formatting operation, e.g., 'Coordinates: {latitude}, {longitude}'.format(latitude='37.24N', longitude='-115.81W') , since it is the new standard in Python 3 and is preferred to the % formatting. Include the line (c) British Crown Copyright [year of creation in the form <YYYY>]-[year of last modification in the form <YYYY>], Met Office. as a comment at the top of every module.","title":"Style Guide"},{"location":"developer_documentation/coding_guidelines/#naming-conventions","text":"Abstract Base Classes should have a name that contains Abstract for clarity. Nouns should be used when naming classes. Use descriptive names that clearly convey a meaning; refrain from using overly general / ambiguous names e.g., data. Use the .cfg extension for configuration files, e.g. those read by configparser.","title":"Naming Conventions"},{"location":"developer_documentation/coding_guidelines/#imports","text":"Use absolute imports https://www.python.org/dev/peps/pep-0328/#rationale-for-absolute-imports as they are recommended by PEP 8. Also, being able to tell immediately where the function comes from greatly improves code readability and comprehension (Readability Counts). Example: import my_package.my_subpackage.my_module and use the my_module.my_function syntax, e.g., import os; os.walk. from my_package.my_subpackage.my_module import my_function and use my_function directly. group imports, with a blank line between each group, in the following order: standard library imports, related third party imports, local application/library specific imports [reference]. Within each import group (see above), order the imports alphabetically. place module level \"dunders\" after the module docstring but before any import statements except from __future__ imports [reference]. Use the pattern import numpy as np","title":"Imports"},{"location":"developer_documentation/coding_guidelines/#typing","text":"Use https://docs.python.org/3/library/typing.html for adding type hints and annotations. Use mypy for running static code analysis using aforementioned type hints.","title":"Typing"},{"location":"developer_documentation/coding_guidelines/#docstrings","text":"Warning Historically CDDS used the NumpyDoc format for all docstrings but as of 2022 the PEP-257 format was adopted. This means there is currently a mix of docstring formats used throughout the CDDS code. Docstrings should now be written using https://docutils.sourceforge.io/rst.html as recommended by https://www.python.org/dev/peps/pep-0287/, and is rendered using Sphinx. An detailed example of the reStructuredText style docstrings can be found here https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html Use double backticks `` around argument names so that they are rendered as code in the HTML produced by Sphinx Use the appropriate substitutions for glossary terms. Make use of the docstring conventions http://www.python.org/dev/peps/pep-0257. The docstring is a phrase ending in a period and prescribes the function or method's effect as a command, not as a description [reference]. It is not necessary to write docstrings for non-public classes, methods and functions, see cdds/pylintrc and mip_convert/pylintrc. (The maintenance overhead is reduced when refactoring non-public classes, methods and functions). Below is an example reStructuredText Docstring Example docstring incorporating all of the guidelines above. def my_function ( my_param1 : float , my_param2 : str ) -> int : \"\"\" Return something. Here's a longer description about the something that is returned. It's so long, it goes over one line! :param my_param1: Description of the first parameter ``my_param1``. :type my_param1: float :param my_param2: Description of the second parameter ``my_param2``. :type my_param2: string :raises ValueError If ``my_param1`` is less than 0. :return: Description of anonymous integer return value. :rtype: int \"\"\"","title":"Docstrings"},{"location":"developer_documentation/coding_guidelines/#doctests","text":"Where appropriate, a https://docs.python.org/3.8/library/doctest.html should be included in an Examples section of the docstring https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard. When multiple examples are provided, they should be separated by blank lines. Comments explaining the examples should have blank lines both above and below them.","title":"Doctests"},{"location":"developer_documentation/coding_guidelines/#entry-point-scripts","text":"Script names should not have an extension, should be lowercase, and with words separated by underscores as necessary to improve readability, e.g., just_do_it. It is recommended that scripts call a main() function located in an importable module so that it is possible to run the code in the script from the Python interpreter / a test module e.g., import my_module; my_module.main(). https://docs.python.org/3.8/library/argparse.html should be used to parse command line options. def main ( args ): # Parse the arguments. args = parse_args ( args ) # Create the configured logger. configure_logger ( args . log_name , args . log_level , args . append_log ) # Retrieve the logger. logger = logging . getLogger ( __name__ ) try : exit_code = my_func ( args ) except BaseException as exc : exit_code = 1 logger . exception ( exc ) return exit_code","title":"Entry Point Scripts"},{"location":"developer_documentation/coding_guidelines/#bash-scripts","text":"Use the Google Styleguide for bash scripts as recommended by the Cylc documentation Run scripts through Shellcheck for catching possible bad practice. (The latest version can be installed using conda for easier access and improvements over the centrally installed version)","title":"Bash Scripts"},{"location":"developer_documentation/development_practices/","text":"Warning This documentation is currently under construction and may not be up to date. Quickstart","title":"Development Practices"},{"location":"developer_documentation/development_practices/#quickstart","text":"","title":"Quickstart"},{"location":"developer_documentation/development_workflow/","text":"Warning This documentation is currently under construction and may not be up to date. Create a Jira Issue Each change should be described in a Jira issue. Before submitting an issue, make sure that the issue explains the purpose of the changes and what changes should be done. New issues should be created in the backlog and marked as . The team will look each week at the new created issues. A issue will be put into a sprint if everyone of the team understand what to do for the issue and all information are provided to proceed the issue. Before creating a new issue, look through the backlog and current sprint if issue is not already reported. Start Working on an Issue Assign the issue to you Put the issue into In progress Create a new branch for the issue Put the branch name on the Jira issue Create a New Branch Create a issue branch The branch name should be start with the issue number and has a brief description of the task. Use underscores in the branch name to separate words. git checkout -b <issue_number>_<brief_description> The commit message should be: <issue_number>: Create a new branch for <brief_description> Don\u2019t wrap the line of the commit message and don\u2019t use more than 72 characters. If you need a longer commit message. Leave one empty line after the first line and then give a more detailed description (now you can wrap this description). After creating the branch, your working copy is already on this branch. You do not need to run an extra switch command like when using SVN. Create a release branch The branch name should start with release and should have the release number in it. For example, you like to create a release branch for all work for a release 2.2.x , the release_2.2.x . git checkout -b release_<release_number> The commit message should contain the release number in it for example: Create branch for release Don\u2019t wrap the line of the commit message and don\u2019t use more than 72 characters. If you need a longer commit message. Leave one empty line after the first line and then give a more detailed description (now you can wrap this description). After creating the branch, your working copy is already on this branch. You do not need to run an extra switch command like when using SVN. Create a branch from a release branch Switch to the release branch, you want to branch from Then, create a new branch like described at Create-a-issue-branch Update Jira issue Add the branch name to your Jira issue Make Changes Even if you are working on a Jira issue, keep an eye on the issue to notice any new comments on the issues. Code Modify the code on the \u201cissue\u201c branch Regularly, commit your changes locally and remotely using: The commit messages should always start with the issue number and have a short description what changed. Be aware, that the requirements for commit message are: Not more than 72 characters per line The first line should start with the issue number and have a really short brief description If more descriptions are necessary, leave an empty line and then write more text, e.g. CDDSO-109 Merge fixing duplicate coordinates error from cmor * Add functional tests for ARISE model * Simplify mean cube calculation After the first commit: Do your additional changes and then use: git commit --amend That combine latest and the current commit to one commit. Using this one allows use to have only one commit per issue even if you \u201cactual\u201c did multiple commits. A push can be done at any time! (You can still do a git commit --amend after a push, because it always use the latest commit command!) Remote commit: git push origin To be more explicit, you can add the branch name you want to commit to: git push origin <branch_name_to_commit_to> Tests Make sure that unit tests covers your code changes, see Unit Tests If there were changes in CDDS Transfer, also run integration tests with RabbitMQ, see CDDS Transfer Tests With RabbitMQ - Run the unit tests regularly It is a good approach to ensure that every time you push a commit to the repository that the tests pass successfully. Documentation Document your code. Try to be short but precise. Briefly describe the work on the Jira issue Reviews For coding reviews, we use the pull requests functionality of Github Go to Github and the pull requests for the cdds-project: https://github.com/MetOffice/cdds/pulls Click on New pull request (green box on the right) Choose as base branch the branch you branched from (often: main) Choose as compare branch your branch Then, click on Create pull request Write into the message box what you did and other useful information for the reviewers Add to your issue the URL link to the pull request for the reviewers Put your issue into Review Assign it to the main reviewer Mention in the team channel that the issue is now ready to review. So, everyone has the change to review your changes. You need at least an approved of the main review before merging the issue If you need to do any changes, assign the issue back to you and put it back into In progress. If the main reviewer is happy with the changes and no comments or issues are needed to addressed anymore, the issue will moved to Approved and re-assigned to you. Now, you can merge your changes Merge changes You have two possibilities how to merge a branch - via command line or using Github. Merge to parent branch Via Command Line Go to the parent branch to merge to (normally: main or the release branch) git checkout <branch_to_merge_to> Make sure that the branch is up-to-date: git pull Merge the issue branch: git merge <your_branch_to_merge> As commit message for a merge use: <issue-number>: Completed work to <short_brief_description_of_task> You could face merging conflicts that you need to solve before you can end the merge. Using Github Go to the Pull Requests view of your branch in Github If your review is approved, there is a green Merge Branch button at the bottom of the page. Click on it. If the this button does not appear or is not clickable, check if your review is really approved and if there is no conflicts you need to solved first. For solving conflicts, Github provides you help with a UI tool remove branch if working on main Cherry pick changes on main branch (or any other branches) If your parent branch is a release branch, you also need to merge the changes into the main branch. Therefore, cherry picking is the best to do. You can do this after merging your changes into the release branch. Cherry picking into other branches are following the same pattern! Via Command Line Find the commit id of the merge commit into the parent branch, you have done in the previous step: Go to the parent branch git checkout <parent-branch> Find your commit ID: git log This list all the previous commits in decreasing order. Each merge entry looks like: commit <commit-id> (<merge-branches>) Merge: <merge-ids> Author: <author-details> Date: <timestamp> <commit-message> Use the <commit-id> of your merge commit. Remember your ! You will need it for the next steps. Do the cherry pick: Go to the branch you want to cherry-pick to: git checkout <branch-to-cerry-pick-to> Cherry pick: git cherry-pick <commit-id> --no-commit This will add the commit\u2019s changes to your working copy without directly committing them. You need to do a git commit by yourself. Commit the changes and check if everything is fine: Check the changes. If they are fine, commit them: git commit Use the key-word cherry-pick in your commit message to make clear that this commit is a cherry-pick. Check if all (unit) tests succeed! Push the changes Using Github For using cherry-picking, see Cherry-picking a commit in GitHub Desktop - GitHub Docs Close the Jira issue Move the Jira issue to Done","title":"Development Workflow"},{"location":"developer_documentation/development_workflow/#create-a-jira-issue","text":"Each change should be described in a Jira issue. Before submitting an issue, make sure that the issue explains the purpose of the changes and what changes should be done. New issues should be created in the backlog and marked as . The team will look each week at the new created issues. A issue will be put into a sprint if everyone of the team understand what to do for the issue and all information are provided to proceed the issue. Before creating a new issue, look through the backlog and current sprint if issue is not already reported.","title":"Create a Jira Issue"},{"location":"developer_documentation/development_workflow/#start-working-on-an-issue","text":"Assign the issue to you Put the issue into In progress Create a new branch for the issue Put the branch name on the Jira issue","title":"Start Working on an Issue"},{"location":"developer_documentation/development_workflow/#create-a-new-branch","text":"","title":"Create a New Branch"},{"location":"developer_documentation/development_workflow/#create-a-issue-branch","text":"The branch name should be start with the issue number and has a brief description of the task. Use underscores in the branch name to separate words. git checkout -b <issue_number>_<brief_description> The commit message should be: <issue_number>: Create a new branch for <brief_description> Don\u2019t wrap the line of the commit message and don\u2019t use more than 72 characters. If you need a longer commit message. Leave one empty line after the first line and then give a more detailed description (now you can wrap this description). After creating the branch, your working copy is already on this branch. You do not need to run an extra switch command like when using SVN.","title":"Create a issue branch"},{"location":"developer_documentation/development_workflow/#create-a-release-branch","text":"The branch name should start with release and should have the release number in it. For example, you like to create a release branch for all work for a release 2.2.x , the release_2.2.x . git checkout -b release_<release_number> The commit message should contain the release number in it for example: Create branch for release Don\u2019t wrap the line of the commit message and don\u2019t use more than 72 characters. If you need a longer commit message. Leave one empty line after the first line and then give a more detailed description (now you can wrap this description). After creating the branch, your working copy is already on this branch. You do not need to run an extra switch command like when using SVN. Create a branch from a release branch Switch to the release branch, you want to branch from Then, create a new branch like described at Create-a-issue-branch Update Jira issue Add the branch name to your Jira issue","title":"Create a release branch"},{"location":"developer_documentation/development_workflow/#make-changes","text":"Even if you are working on a Jira issue, keep an eye on the issue to notice any new comments on the issues.","title":"Make Changes"},{"location":"developer_documentation/development_workflow/#code","text":"Modify the code on the \u201cissue\u201c branch Regularly, commit your changes locally and remotely using: The commit messages should always start with the issue number and have a short description what changed. Be aware, that the requirements for commit message are: Not more than 72 characters per line The first line should start with the issue number and have a really short brief description If more descriptions are necessary, leave an empty line and then write more text, e.g. CDDSO-109 Merge fixing duplicate coordinates error from cmor * Add functional tests for ARISE model * Simplify mean cube calculation After the first commit: Do your additional changes and then use: git commit --amend That combine latest and the current commit to one commit. Using this one allows use to have only one commit per issue even if you \u201cactual\u201c did multiple commits. A push can be done at any time! (You can still do a git commit --amend after a push, because it always use the latest commit command!) Remote commit: git push origin To be more explicit, you can add the branch name you want to commit to: git push origin <branch_name_to_commit_to>","title":"Code"},{"location":"developer_documentation/development_workflow/#tests","text":"Make sure that unit tests covers your code changes, see Unit Tests If there were changes in CDDS Transfer, also run integration tests with RabbitMQ, see CDDS Transfer Tests With RabbitMQ - Run the unit tests regularly It is a good approach to ensure that every time you push a commit to the repository that the tests pass successfully.","title":"Tests"},{"location":"developer_documentation/development_workflow/#documentation","text":"Document your code. Try to be short but precise. Briefly describe the work on the Jira issue","title":"Documentation"},{"location":"developer_documentation/development_workflow/#reviews","text":"For coding reviews, we use the pull requests functionality of Github Go to Github and the pull requests for the cdds-project: https://github.com/MetOffice/cdds/pulls Click on New pull request (green box on the right) Choose as base branch the branch you branched from (often: main) Choose as compare branch your branch Then, click on Create pull request Write into the message box what you did and other useful information for the reviewers Add to your issue the URL link to the pull request for the reviewers Put your issue into Review Assign it to the main reviewer Mention in the team channel that the issue is now ready to review. So, everyone has the change to review your changes. You need at least an approved of the main review before merging the issue If you need to do any changes, assign the issue back to you and put it back into In progress. If the main reviewer is happy with the changes and no comments or issues are needed to addressed anymore, the issue will moved to Approved and re-assigned to you. Now, you can merge your changes","title":"Reviews"},{"location":"developer_documentation/development_workflow/#merge-changes","text":"You have two possibilities how to merge a branch - via command line or using Github.","title":"Merge changes"},{"location":"developer_documentation/development_workflow/#merge-to-parent-branch","text":"","title":"Merge to parent branch"},{"location":"developer_documentation/development_workflow/#via-command-line","text":"Go to the parent branch to merge to (normally: main or the release branch) git checkout <branch_to_merge_to> Make sure that the branch is up-to-date: git pull Merge the issue branch: git merge <your_branch_to_merge> As commit message for a merge use: <issue-number>: Completed work to <short_brief_description_of_task> You could face merging conflicts that you need to solve before you can end the merge.","title":"Via Command Line"},{"location":"developer_documentation/development_workflow/#using-github","text":"Go to the Pull Requests view of your branch in Github If your review is approved, there is a green Merge Branch button at the bottom of the page. Click on it. If the this button does not appear or is not clickable, check if your review is really approved and if there is no conflicts you need to solved first. For solving conflicts, Github provides you help with a UI tool remove branch if working on main","title":"Using Github"},{"location":"developer_documentation/development_workflow/#cherry-pick-changes-on-main-branch-or-any-other-branches","text":"If your parent branch is a release branch, you also need to merge the changes into the main branch. Therefore, cherry picking is the best to do. You can do this after merging your changes into the release branch. Cherry picking into other branches are following the same pattern!","title":"Cherry pick changes on main branch (or any other branches)"},{"location":"developer_documentation/development_workflow/#via-command-line_1","text":"Find the commit id of the merge commit into the parent branch, you have done in the previous step: Go to the parent branch git checkout <parent-branch> Find your commit ID: git log This list all the previous commits in decreasing order. Each merge entry looks like: commit <commit-id> (<merge-branches>) Merge: <merge-ids> Author: <author-details> Date: <timestamp> <commit-message> Use the <commit-id> of your merge commit. Remember your ! You will need it for the next steps. Do the cherry pick: Go to the branch you want to cherry-pick to: git checkout <branch-to-cerry-pick-to> Cherry pick: git cherry-pick <commit-id> --no-commit This will add the commit\u2019s changes to your working copy without directly committing them. You need to do a git commit by yourself. Commit the changes and check if everything is fine: Check the changes. If they are fine, commit them: git commit Use the key-word cherry-pick in your commit message to make clear that this commit is a cherry-pick. Check if all (unit) tests succeed! Push the changes","title":"Via Command Line"},{"location":"developer_documentation/development_workflow/#using-github_1","text":"For using cherry-picking, see Cherry-picking a commit in GitHub Desktop - GitHub Docs","title":"Using Github"},{"location":"developer_documentation/development_workflow/#close-the-jira-issue","text":"Move the Jira issue to Done","title":"Close the Jira issue"},{"location":"developer_documentation/github/","text":"Warning This documentation is currently under construction and may not be up to date. Github Basics The develop branch is called main . When you start working on the project, first of all you need to get a working copy on your developer environment. To do this, clone the CDDS git repository from Github. git clone git@github.com:MetOffice/CDDS.git Setup up SSH For the cdds-inline-testing project, we use ssh. You can import the ssh key into seahorse. That prevents you to type in your git credentials each time when you use a git command. Therefore, you need to use the SSH repository URL: git clone git@github.com:<permission-group>/<repository-name>.git For the CDDS project, we use ssh. You can import the ssh key into seahorse. That prevents you to type in your git credentials each time when you use a git command. Using SSH Key Generate a SSH key for Github or use an already existing key. You can do it via seahorse or on the command line. Generate SSH key for Github using Seahorse: Start seahorse Click on File in the menu and choose New A creation window should pop up. In that, choose Secure Shell Key. Then click on continue Now, you need to add a description of your key, for example \u201cGithub\u201d. Click on Just Create Key It will ask you for a password. You can also use an empty password but it is highly recommended to use one. Click at OK. You need to confirm your chosen password again. After confirmation, the key will be created. You should see the key in the Open SSH keys tab. Now, open a terminal. You can find the public key that you need for Github in your home folder: cd ~/.ssh Your new created public key is `id_rsa. .pub`` file, where is the highest number of the id_rsa files. To get the key phrase that is needed for Github, simply open the public key file with an editor of your choice, for example with vi: vi id_rsa.<number>.pub The file content should start with ssh-rsa . Add your SSH key to Github Login to Github and go to your account settings Go to the SSH and GPG keys tab Click on the green Add SSH key button Copy your public key phrase of your SSH key into the text box Click Add SSH key A confirm password box should be shown. Enter your Github password (not the SSH key password!) and then the key is added. You will get a email confirming that a new key has been added to your Github account. Clone working copy of cdds-testing-project Use following command to clone the cdds-testing-project: git clone git@github.com:MetOffice/CDDS.git Git Basics View status of your working copy git status This command shows you the status of your working copy: On which branch you are What files are not committed but modified, added or removed If files have any conflicts during a merge, switch, etc. Get latest changes You need to pull the latest changes from upstream by hand. Therefor, use following command: git pull This gets the latest changes from the remote repository for the branch you are currently in. Switching between branches git checkout <branch_name> Make sure that you stash or commit any uncommited local changes first otherwise it could be that you get some awful conflicts. Create a new branch Each ticket should be developed in a new \u201cfeature\u201c branch. Also, bug releases etc. should have their own branches. Switch to the branch you want to branch of. This branch is called parent branch: git checkout <parent_branch_name> Then, pull the changes from upstream. Your parent branch needs to be up to date: git pull Create the branch on your local machine and switch in this branch: git checkout -b <name_of_the_new_branch> Push the branch on github: git push origin <name_of_the_new_branch> You can check if your branch is created remotely: git branch -a This command should list your new created branch. Commit and Push Changes: After you have made your changes you first need to commit them to your local repository afterwards you must push them to the remote repository. Commit Changes If you want to commit your changes locally, run: git commit It opens a vim or gvim, where you can specify your commit message. For the requirements of the commit messages see \u201cCommit Messages\u201c section. If you want to have one commit for each ticket then use for the first commit git commit and for all following commits `git comment --amend``. Push Changes Before pushing any changes you first need to commit them locally! If you want to push your changes, you must tell git on which branch to push them: git push To be absolutely sure, that you push on the right branch, you can add the branch name to the git command: git push origin <name_of_the_branch> Commit Messages Template <ticket-number>: <short ticket description not longer than 72 characters> <some additional description wrap after 72 charachters> Rules Git does not wrap commit messages automatically. That makes it hard to see the whole message in a terminal. So following rules will apply for commit messages: The first line of the commit starts with the ticket number, after that there will be a colon and followed by a short description what the ticket is for: The message should not be longer than 72 characters! Do not wrap this first line If you need more descriptions: Leave one line empty after the first main message Write more descriptions. For better reading try not to write more than 72 characters per line Here, you can wrap the lines Try to be short and precise Viewing history of commits Seeing all full commit messages decreasing history git log Seeing first line of the commit messages decreasing history git log --oneline Viewing branches Seeing the information on the branch you are on: git branch List all child branches git branch --list Reset changes Reset all local changes git reset --hard This discard all local changes to all files permanently. Reset changes of a specific file git reset --hard HEAD <file> HEAD is your current branch. Reset changes of last n commits: git reset --hard HEAD~<number-of-changes> The HEAD is your current branch. Reset to the last commit: git reset --hard HEAD~1 Reset to the previous last commit: git reset --hard HEAD~2 Revert changes Undo changes of a specific commit: Find the commit number: git log --online The commit number is the number before each commit message Revert changes to the commit number: git revert <commit-number> The git revert will undo the given commit but will create a new commit without deleting the older one. The git revert command will not touch the commits in your history, even the reverted ones. Stash changes You can temporarily stashes changes you have made to your working copy. You can work on something else and come back late and re-apply them. It helps you when you need to quickly switch context and work on something else on your local working copy. You can switches between branches your stashed changes will be still available. Stashing your work git stash That stashes your local (uncommited) changes, save them for later use and revert them from your working copy. Re-applying your stashed changes git stash pop This removes the changes from your stash and re-applies them to your working copy. If you do not want that you changes will be removed from the stash use: git stash apply The stash apply command is really helpful if you need to apply the changes on multiple branches. Delete Branches This should be only a well-considered option. You can delete a branch locally or remotely: Delete the branch locally: git branch -D <name_of_branch_to_delete> This deletion command force to delete the branch even if it is un-merged. Delete the branch on github: git push origin --delete <name_of_branch_to_delete>","title":"Working with Github and Git"},{"location":"developer_documentation/github/#github-basics","text":"The develop branch is called main . When you start working on the project, first of all you need to get a working copy on your developer environment. To do this, clone the CDDS git repository from Github. git clone git@github.com:MetOffice/CDDS.git","title":"Github Basics"},{"location":"developer_documentation/github/#setup-up-ssh","text":"For the cdds-inline-testing project, we use ssh. You can import the ssh key into seahorse. That prevents you to type in your git credentials each time when you use a git command. Therefore, you need to use the SSH repository URL: git clone git@github.com:<permission-group>/<repository-name>.git For the CDDS project, we use ssh. You can import the ssh key into seahorse. That prevents you to type in your git credentials each time when you use a git command. Using SSH Key Generate a SSH key for Github or use an already existing key. You can do it via seahorse or on the command line.","title":"Setup up SSH"},{"location":"developer_documentation/github/#generate-ssh-key-for-github-using-seahorse","text":"Start seahorse Click on File in the menu and choose New A creation window should pop up. In that, choose Secure Shell Key. Then click on continue Now, you need to add a description of your key, for example \u201cGithub\u201d. Click on Just Create Key It will ask you for a password. You can also use an empty password but it is highly recommended to use one. Click at OK. You need to confirm your chosen password again. After confirmation, the key will be created. You should see the key in the Open SSH keys tab. Now, open a terminal. You can find the public key that you need for Github in your home folder: cd ~/.ssh Your new created public key is `id_rsa. .pub`` file, where is the highest number of the id_rsa files. To get the key phrase that is needed for Github, simply open the public key file with an editor of your choice, for example with vi: vi id_rsa.<number>.pub The file content should start with ssh-rsa .","title":"Generate SSH key for Github using Seahorse:"},{"location":"developer_documentation/github/#add-your-ssh-key-to-github","text":"Login to Github and go to your account settings Go to the SSH and GPG keys tab Click on the green Add SSH key button Copy your public key phrase of your SSH key into the text box Click Add SSH key A confirm password box should be shown. Enter your Github password (not the SSH key password!) and then the key is added. You will get a email confirming that a new key has been added to your Github account. Clone working copy of cdds-testing-project Use following command to clone the cdds-testing-project: git clone git@github.com:MetOffice/CDDS.git","title":"Add your SSH key to Github"},{"location":"developer_documentation/github/#git-basics","text":"View status of your working copy git status This command shows you the status of your working copy: On which branch you are What files are not committed but modified, added or removed If files have any conflicts during a merge, switch, etc.","title":"Git Basics"},{"location":"developer_documentation/github/#get-latest-changes","text":"You need to pull the latest changes from upstream by hand. Therefor, use following command: git pull This gets the latest changes from the remote repository for the branch you are currently in.","title":"Get latest changes"},{"location":"developer_documentation/github/#switching-between-branches","text":"git checkout <branch_name> Make sure that you stash or commit any uncommited local changes first otherwise it could be that you get some awful conflicts.","title":"Switching between branches"},{"location":"developer_documentation/github/#create-a-new-branch","text":"Each ticket should be developed in a new \u201cfeature\u201c branch. Also, bug releases etc. should have their own branches. Switch to the branch you want to branch of. This branch is called parent branch: git checkout <parent_branch_name> Then, pull the changes from upstream. Your parent branch needs to be up to date: git pull Create the branch on your local machine and switch in this branch: git checkout -b <name_of_the_new_branch> Push the branch on github: git push origin <name_of_the_new_branch> You can check if your branch is created remotely: git branch -a This command should list your new created branch.","title":"Create a new branch"},{"location":"developer_documentation/github/#commit-and-push-changes","text":"After you have made your changes you first need to commit them to your local repository afterwards you must push them to the remote repository.","title":"Commit and Push Changes:"},{"location":"developer_documentation/github/#commit-changes","text":"If you want to commit your changes locally, run: git commit It opens a vim or gvim, where you can specify your commit message. For the requirements of the commit messages see \u201cCommit Messages\u201c section. If you want to have one commit for each ticket then use for the first commit git commit and for all following commits `git comment --amend``.","title":"Commit Changes"},{"location":"developer_documentation/github/#push-changes","text":"Before pushing any changes you first need to commit them locally! If you want to push your changes, you must tell git on which branch to push them: git push To be absolutely sure, that you push on the right branch, you can add the branch name to the git command: git push origin <name_of_the_branch>","title":"Push Changes"},{"location":"developer_documentation/github/#commit-messages","text":"","title":"Commit Messages"},{"location":"developer_documentation/github/#template","text":"<ticket-number>: <short ticket description not longer than 72 characters> <some additional description wrap after 72 charachters>","title":"Template"},{"location":"developer_documentation/github/#rules","text":"Git does not wrap commit messages automatically. That makes it hard to see the whole message in a terminal. So following rules will apply for commit messages: The first line of the commit starts with the ticket number, after that there will be a colon and followed by a short description what the ticket is for: The message should not be longer than 72 characters! Do not wrap this first line If you need more descriptions: Leave one line empty after the first main message Write more descriptions. For better reading try not to write more than 72 characters per line Here, you can wrap the lines Try to be short and precise","title":"Rules"},{"location":"developer_documentation/github/#viewing-history-of-commits","text":"Seeing all full commit messages decreasing history git log Seeing first line of the commit messages decreasing history git log --oneline Viewing branches Seeing the information on the branch you are on: git branch List all child branches git branch --list Reset changes Reset all local changes git reset --hard This discard all local changes to all files permanently. Reset changes of a specific file git reset --hard HEAD <file> HEAD is your current branch. Reset changes of last n commits: git reset --hard HEAD~<number-of-changes> The HEAD is your current branch. Reset to the last commit: git reset --hard HEAD~1 Reset to the previous last commit: git reset --hard HEAD~2 Revert changes Undo changes of a specific commit: Find the commit number: git log --online The commit number is the number before each commit message Revert changes to the commit number: git revert <commit-number> The git revert will undo the given commit but will create a new commit without deleting the older one. The git revert command will not touch the commits in your history, even the reverted ones. Stash changes You can temporarily stashes changes you have made to your working copy. You can work on something else and come back late and re-apply them. It helps you when you need to quickly switch context and work on something else on your local working copy. You can switches between branches your stashed changes will be still available. Stashing your work git stash That stashes your local (uncommited) changes, save them for later use and revert them from your working copy. Re-applying your stashed changes git stash pop This removes the changes from your stash and re-applies them to your working copy. If you do not want that you changes will be removed from the stash use: git stash apply The stash apply command is really helpful if you need to apply the changes on multiple branches.","title":"Viewing history of commits"},{"location":"developer_documentation/github/#delete-branches","text":"This should be only a well-considered option. You can delete a branch locally or remotely: Delete the branch locally: git branch -D <name_of_branch_to_delete> This deletion command force to delete the branch even if it is un-merged. Delete the branch on github: git push origin --delete <name_of_branch_to_delete>","title":"Delete Branches"},{"location":"developer_documentation/jira/","text":"Warning This documentation is currently under construction and may not be up to date. Ticket Types There are several ticket types to track the work of a project. Epics An Epic is a body of work that can be break down into multiple user stories (issues). Epics are too much work for one sprint. An epic is almost always delivered over a set of sprints. Issues The team use issues to track individual pieces of work that must be completed. An issue represents a story (Scrum), a bug or a task. Typically, issues represent things like big features, user requirements and software bugs. The work for an issue must be small enough such that it can done in one sprint. Sub-Tasks Issues can have sub-tasks that are assigned and tracked individually. There are following reasons for creating a sub-tasks: Splitting an issue into even smaller chunks Allowing various aspects of an issue to be assigned to different people Creating a to-do list for an issue Customised Types Each issue type can be customise. Additional, you can add your own issue type. For example: Bug Documentation Only users with admin permission can customise issue types. This feature is available on the project settings menu. Features of an issue Estimation The estimation of an issue can be tracked the velocity of a project. Estimating issues in the backlog helps to predict how long portions of the backlog might take to be delivered. The most popular estimation scheme is story points. Story points measure the complexity of one issue relative to the others. The estimation of a issue can be stored in the \"Estimate\" field. Flag Flagging an issue can be useful for: indicating that the assignee of the ticket can not finish his work because of having too little time. Other team members are welcome to take over. marking an issue as blocked, for example by other not done issues or another team. Priority Issues can have priorities. That allows you to rank your issues and what you should work on next. Time Tracking There is a time tracking possible on each issue. So, you can track the time you spend working on the ticket. To start and stop the time tracking, you can simply click on the time tracking button. Breaks are supported and it shows what time you already spent for this issue. Time tracking means hard discipline. Every time when you do a break, you need to click on the time tracking button of the issue and click again when you re-start working. Labels Labels are tags or keywords that can be added to issues. They let you categorise an issue. It is similar to the hashtag ( # ) used in twitter. They also can used for searching an issue. Fix version The fix version of an issue identifies the version when a issue should be done. Links in issues Issues can be linked to keep track of duplicate or related work. If you want to create a link of another issue in your issue: open the issue and and click on link issue choose the reason why you want to link the issue (for example: 'duplicates') choose the issue that you want to link You can also link confluence pages or a web-site to an issue. Attachment Attachments can be added to an issue. You can add folders, text files, PDFs, images, etc. Sprint The sprint field of an issue identifies the sprint in that the issue should be processed. Status Each issue has an status field that specify the current status of an issue - for example: TO DO In progress Review Approved Done Users Types Reporter The reporter is the person who raised the request. Usually, this is the same person who created the issue, but not always - for example: you can create issues on behalf of someone else. Assignee The assignee is the person who is responsible for completing the issue, including sub-tasks. The assignee is working on the issue. An issue with sub-tasks can assigned to multiple persons. The person of the parent issue is the most responsible person (like a kind of lead). Watcher A watcher is a person that has at least read permissions on the project and keep an eye on the project. It is possible to watching an issue that allows persons to keep an eye on a special issue. The watcher will be informed whenever there is any update or changes in that issue. Start watching a issue Go to the issue Click on the eye symbol on the top right Click on Add watchers and add the new watcher Stop watching a issue Go to the issue Click on the eye symbol on the top right Hover over the name of the watcher you want to remove Click on the X Work with issues Sprints A sprint - also known as an iteration - is a short, well defined time-boxed period when work of a set amount of work should be completed. The most often chosen time period for a sprint is two weeks or one month In Jira, you view sprints on a board and assign issues to the sprints. Each individual issue as a sprint field for seeing the sprint that the issue is part of. Backlog The backlog is a list of tasks that represents the outstanding work in a project. Any issues of the backlog can be add to a sprint. If you raise an issue, you should put it into the backlog. The team decides when it will be put into the sprint Board The board displays all issues of the project that are in the current sprint. It gives you an overview: who is working on which issue the status of each individual issue the assignee of each issue if issues are blocked or not Roadmap A good product road map makes sure that everyone working on the product understands the status of work and are aligned on upcoming priorities. Road maps in Jira enable you to quickly create a timeline of your plans, update your priorities and communicate the status of the work. Releases A release present a point in time of your project. Releases can be used to schedule how features are rolled out or as a way to organise work that has been completed for the project. Jira provides a releases feature. You can add releases to each issue to specify which version contains which feature. The releases page shows how much work has been completed in each release version. Reports There are several plugins that can be used to create a report of the current project status. The most important is the Sprint burn down chart. Sprint burn down chart The burn down chart shows the amount of work remaining on a sprint. The following will help you to understand the burn down chart in Jira: The burn down chart in Jira is board-specific. The vertical axis represents the estimation statistic that you have configured - for example issue count or story points. The burn down chart is based on the board\u2019s column mapping. Only issues with status Done (left-most column) will be considered. The grey guideline represent the optimal sprint burn down. Plugins Jira provides several plugins that can be activated. Code You can connect your Github for the project with the Jira project. In our system, this plugin will be fully configured in the near future. Confluence You can link a confluence space to your Jira project. You can see, add, remove and edit each page in the space using the Jira software.","title":"Working with Jira"},{"location":"developer_documentation/jira/#ticket-types","text":"There are several ticket types to track the work of a project.","title":"Ticket Types"},{"location":"developer_documentation/jira/#epics","text":"An Epic is a body of work that can be break down into multiple user stories (issues). Epics are too much work for one sprint. An epic is almost always delivered over a set of sprints.","title":"Epics"},{"location":"developer_documentation/jira/#issues","text":"The team use issues to track individual pieces of work that must be completed. An issue represents a story (Scrum), a bug or a task. Typically, issues represent things like big features, user requirements and software bugs. The work for an issue must be small enough such that it can done in one sprint.","title":"Issues"},{"location":"developer_documentation/jira/#sub-tasks","text":"Issues can have sub-tasks that are assigned and tracked individually. There are following reasons for creating a sub-tasks: Splitting an issue into even smaller chunks Allowing various aspects of an issue to be assigned to different people Creating a to-do list for an issue","title":"Sub-Tasks"},{"location":"developer_documentation/jira/#customised-types","text":"Each issue type can be customise. Additional, you can add your own issue type. For example: Bug Documentation Only users with admin permission can customise issue types. This feature is available on the project settings menu.","title":"Customised Types"},{"location":"developer_documentation/jira/#features-of-an-issue","text":"","title":"Features of an issue"},{"location":"developer_documentation/jira/#estimation","text":"The estimation of an issue can be tracked the velocity of a project. Estimating issues in the backlog helps to predict how long portions of the backlog might take to be delivered. The most popular estimation scheme is story points. Story points measure the complexity of one issue relative to the others. The estimation of a issue can be stored in the \"Estimate\" field.","title":"Estimation"},{"location":"developer_documentation/jira/#flag","text":"Flagging an issue can be useful for: indicating that the assignee of the ticket can not finish his work because of having too little time. Other team members are welcome to take over. marking an issue as blocked, for example by other not done issues or another team.","title":"Flag"},{"location":"developer_documentation/jira/#priority","text":"Issues can have priorities. That allows you to rank your issues and what you should work on next.","title":"Priority"},{"location":"developer_documentation/jira/#time-tracking","text":"There is a time tracking possible on each issue. So, you can track the time you spend working on the ticket. To start and stop the time tracking, you can simply click on the time tracking button. Breaks are supported and it shows what time you already spent for this issue. Time tracking means hard discipline. Every time when you do a break, you need to click on the time tracking button of the issue and click again when you re-start working.","title":"Time Tracking"},{"location":"developer_documentation/jira/#labels","text":"Labels are tags or keywords that can be added to issues. They let you categorise an issue. It is similar to the hashtag ( # ) used in twitter. They also can used for searching an issue.","title":"Labels"},{"location":"developer_documentation/jira/#fix-version","text":"The fix version of an issue identifies the version when a issue should be done.","title":"Fix version"},{"location":"developer_documentation/jira/#links-in-issues","text":"Issues can be linked to keep track of duplicate or related work. If you want to create a link of another issue in your issue: open the issue and and click on link issue choose the reason why you want to link the issue (for example: 'duplicates') choose the issue that you want to link You can also link confluence pages or a web-site to an issue.","title":"Links in issues"},{"location":"developer_documentation/jira/#attachment","text":"Attachments can be added to an issue. You can add folders, text files, PDFs, images, etc.","title":"Attachment"},{"location":"developer_documentation/jira/#sprint","text":"The sprint field of an issue identifies the sprint in that the issue should be processed.","title":"Sprint"},{"location":"developer_documentation/jira/#status","text":"Each issue has an status field that specify the current status of an issue - for example: TO DO In progress Review Approved Done","title":"Status"},{"location":"developer_documentation/jira/#users-types","text":"","title":"Users Types"},{"location":"developer_documentation/jira/#reporter","text":"The reporter is the person who raised the request. Usually, this is the same person who created the issue, but not always - for example: you can create issues on behalf of someone else.","title":"Reporter"},{"location":"developer_documentation/jira/#assignee","text":"The assignee is the person who is responsible for completing the issue, including sub-tasks. The assignee is working on the issue. An issue with sub-tasks can assigned to multiple persons. The person of the parent issue is the most responsible person (like a kind of lead).","title":"Assignee"},{"location":"developer_documentation/jira/#watcher","text":"A watcher is a person that has at least read permissions on the project and keep an eye on the project. It is possible to watching an issue that allows persons to keep an eye on a special issue. The watcher will be informed whenever there is any update or changes in that issue.","title":"Watcher"},{"location":"developer_documentation/jira/#start-watching-a-issue","text":"Go to the issue Click on the eye symbol on the top right Click on Add watchers and add the new watcher","title":"Start watching a issue"},{"location":"developer_documentation/jira/#stop-watching-a-issue","text":"Go to the issue Click on the eye symbol on the top right Hover over the name of the watcher you want to remove Click on the X","title":"Stop watching a issue"},{"location":"developer_documentation/jira/#work-with-issues","text":"","title":"Work with issues"},{"location":"developer_documentation/jira/#sprints","text":"A sprint - also known as an iteration - is a short, well defined time-boxed period when work of a set amount of work should be completed. The most often chosen time period for a sprint is two weeks or one month In Jira, you view sprints on a board and assign issues to the sprints. Each individual issue as a sprint field for seeing the sprint that the issue is part of.","title":"Sprints"},{"location":"developer_documentation/jira/#backlog","text":"The backlog is a list of tasks that represents the outstanding work in a project. Any issues of the backlog can be add to a sprint. If you raise an issue, you should put it into the backlog. The team decides when it will be put into the sprint","title":"Backlog"},{"location":"developer_documentation/jira/#board","text":"The board displays all issues of the project that are in the current sprint. It gives you an overview: who is working on which issue the status of each individual issue the assignee of each issue if issues are blocked or not","title":"Board"},{"location":"developer_documentation/jira/#roadmap","text":"A good product road map makes sure that everyone working on the product understands the status of work and are aligned on upcoming priorities. Road maps in Jira enable you to quickly create a timeline of your plans, update your priorities and communicate the status of the work.","title":"Roadmap"},{"location":"developer_documentation/jira/#releases","text":"A release present a point in time of your project. Releases can be used to schedule how features are rolled out or as a way to organise work that has been completed for the project. Jira provides a releases feature. You can add releases to each issue to specify which version contains which feature. The releases page shows how much work has been completed in each release version.","title":"Releases"},{"location":"developer_documentation/jira/#reports","text":"There are several plugins that can be used to create a report of the current project status. The most important is the Sprint burn down chart.","title":"Reports"},{"location":"developer_documentation/jira/#sprint-burn-down-chart","text":"The burn down chart shows the amount of work remaining on a sprint. The following will help you to understand the burn down chart in Jira: The burn down chart in Jira is board-specific. The vertical axis represents the estimation statistic that you have configured - for example issue count or story points. The burn down chart is based on the board\u2019s column mapping. Only issues with status Done (left-most column) will be considered. The grey guideline represent the optimal sprint burn down.","title":"Sprint burn down chart"},{"location":"developer_documentation/jira/#plugins","text":"Jira provides several plugins that can be activated.","title":"Plugins"},{"location":"developer_documentation/jira/#code","text":"You can connect your Github for the project with the Jira project. In our system, this plugin will be fully configured in the near future.","title":"Code"},{"location":"developer_documentation/jira/#confluence","text":"You can link a confluence space to your Jira project. You can see, add, remove and edit each page in the space using the Jira software.","title":"Confluence"},{"location":"developer_documentation/managing_environments/","text":"Warning This documentation is currently under construction and may not be up to date. The following covers two particular aspects of managing conda environments. Modification of dependencies in environment.yaml files. Creation and naming of the conda development environments. The changes are made to help Eliminate the confusion when there are mismatches between a cdds version and cdds conda development environment name. Prevent accidental creation of unsolvable environment.yaml files when merging from release branches into main or vice versa. Avoid setup_env_for_devel script pointing to an environment that doesn't match that of the environment.yaml for any given commit. Prevent having to modify existing installed cdds conda development environments. Modifying environment.yaml Files If there are modifications to cdds needed that require a new environment then this should be managed in a consistent way. Examples of such changes could include a package version needs changing, new package(s) added old package(s) removed, a channel usage change In the majority of cases, a change will normally be driven by a change to the cdds code base, so will have an existing JIRA issue. However, if this isn't the case then an issue should still be raised to cover any environment change (E.g. if a change to conda channels is needed but no package version changes thus no code changes are needed.). During the completion of the ticket in question, the process should be as follows. (This example assumes a new package is needed for new functionality within cdds and working on main) Create a new conda environment for development work on this particular change. This could be done on the cdds account, although modifying environments can sometimes be tricky. Initially it would be better to do this on a user account first as this minimises the chance of accidentally modifying any existing cdds development environments. There are two approaches adding a package. Update environment_dev.yaml specifying a particular version if this needed otherwise this can be left out and let the solver figure out the best version to use. Create an environment using environment_dev.yaml and then install the package afterwards using conda install Make the required code changes using this new environment e.g. add the new functionality that makes use of the new package. When ready to create a pull request with these changes, create the next development environment on the cdds account using the correct naming convention cdds-X.X_dev-X Update setup_dev_env_for_development to point to the new environment on the cdds account. Update the .yaml files based on what is in the installed version of cdds Make sure all the tests are still passing. By following this process it makes sure that when the code changes are being reviewed the reviewer does not need to create their own environment. the review will be using the actual environment that will be live once the PR is brought into main. the existing development environment on main doesn't need to be modified in any way. when the PR is brought into main the transition is seamless. it is easy to remove the environment and start again if needs be based on feedback in the review without disrupting anything. Adding new environments should be fairly infrequent procedure, but make sure to be aware so when merging the PR back into By adhering to the above process, it should mean that the state of main is always consistent with the environment.yaml files it contains, and the setup script should also be always be pointing to the correct environment. Environment Changes on Bugfix Releases If at all possible, changes to environments on bugfix branches should be avoided as it will increases the amount of work needed introduces more potential things to go wrong Of course sometimes it is unavoidable so in such cases be aware of the following. merging changes made to an enironment.yaml file on a release branch into main is almost always going to be a bad idea and increase the possibility of producing unsolvable environments unless the files are identical. The normal practice of having a PR which brings changes from a dev branch (that branched from a release branch) into the release branch, and then cherry picking the squashed commit directly onto main may be a bad approach in this case. I would suggest that when bringing such changes into main it would be better to create a new branch off of main, cherry pick the bugfix code changes onto branch but revert environment changes. Managing Development Environments Regarding point two, the naming conventions used for cdds development environments have been somewhat inconsistent and can at times lead to confusion. Here are a selection of cdds conda development environments. cdds-2.1_dev-1 cdds-2.1_dev-1 cdds-2.3.2_dev cdds-2.3_dev cdds-2.4.0_dev cdds-2.4.0_dev-1 cdds-2.4_dev_3 cdds-2.5.0_dev-1 A first step would be sticking with dev environments not being tied to specific hotfix versions. This is something that we have already done for some versions and not for others eg. Going forward I think it would be best to adopt the convention that does not reference a specific hotfix version such as (examples given for cdds version 2.5.0). cdds-2.5_dev-0 or cdds-2.5.X_dev-0 (the X is not meant to be replaced) This helps to avoid any confusion that can arise when there is a mismatch over the cdds version number specified in a given copy of cdds and the development environment that setup_env_for_development points to. Such situations can occur when there have been no updates to the environment for a period of time. It does not necessarily mean Additionally, particular attention should be paid to the change of version number on main following a maj/min release. This should be done immediately after the release. Concrete Example Here is a concrete example of the above starting from a 2.5.0 release. The 2.5.0 release branch v2.5_release is created from main Update main by Incrementing cdds version to 2.6.0 Creating a new environment called cdds-2.6_dev-0 Pointing setup_env_dev_for_development to this new cdds-2.6_dev-0 environment As development proceeds on main over time new development environments are created in line with the practices described earlier in this document. E.g. cdds-2.6_dev-1 - (change - added new package) cdds-2.6_dev-2 - (change - added new package) As development also proceeds on v2.5_release concurrently, and although it is not desirable, it may be necessary for a change to the environment. (For the purpose of this example we will assume the latest development environment was cdds-2.5_dev-1 when the v2.5_release branch was made.) A new environment cdds-2.5_dev-2 is created for this change. A new environment cdds-2.6_dev-3 is needed for development on main if this change needs to be brought over (this will almost always be the case). When main reaches a state ready to release 2.6.0 the process continues as above. The 2.6.0 release branch v2.6_release is created from main Then update main by Incrementing cdds version to 2.7.0 Creating a new environment called cdds-2.7_dev-0 Pointing setup_env_dev_for_development to cdds-2.7_dev-0 The resultant list of environments would look something like this. cdds-2.5_dev-1 cdds-2.6_dev-0 cdds-2.6_dev-1 cdds-2.6_dev-2 cdds-2.5_dev-2 cdds-2.6_dev-3 cdds-2.7_dev-0 A downside of the above convention is that there will be a duplicated environment for every 2.X release which differs only in name. I.e, in the above example cdds-2.7_dev-0 and cdds-2.6_dev-3 should be exactly the same, aside from having different names. However, I think this an acceptable inefficiency for preventing confusion around environment versioning. Conda Good to Know When there are multiple channels defined in an environment.yaml there is an implicit priority based on their order, which if you are not aware of, conda may solve environments in ways you weren\u2019t expecting. See notes on --flexible_solve https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html#strict-channel-priority tldr flexible solve is the default and will prefer packages from higher priority channels even if there are more recent versions in other channels. Make use of mamba and the --dry-run option to speed your workflow up. The same versioned package","title":"Managing Conda Environments"},{"location":"developer_documentation/managing_environments/#modifying-environmentyaml-files","text":"If there are modifications to cdds needed that require a new environment then this should be managed in a consistent way. Examples of such changes could include a package version needs changing, new package(s) added old package(s) removed, a channel usage change In the majority of cases, a change will normally be driven by a change to the cdds code base, so will have an existing JIRA issue. However, if this isn't the case then an issue should still be raised to cover any environment change (E.g. if a change to conda channels is needed but no package version changes thus no code changes are needed.). During the completion of the ticket in question, the process should be as follows. (This example assumes a new package is needed for new functionality within cdds and working on main) Create a new conda environment for development work on this particular change. This could be done on the cdds account, although modifying environments can sometimes be tricky. Initially it would be better to do this on a user account first as this minimises the chance of accidentally modifying any existing cdds development environments. There are two approaches adding a package. Update environment_dev.yaml specifying a particular version if this needed otherwise this can be left out and let the solver figure out the best version to use. Create an environment using environment_dev.yaml and then install the package afterwards using conda install Make the required code changes using this new environment e.g. add the new functionality that makes use of the new package. When ready to create a pull request with these changes, create the next development environment on the cdds account using the correct naming convention cdds-X.X_dev-X Update setup_dev_env_for_development to point to the new environment on the cdds account. Update the .yaml files based on what is in the installed version of cdds Make sure all the tests are still passing. By following this process it makes sure that when the code changes are being reviewed the reviewer does not need to create their own environment. the review will be using the actual environment that will be live once the PR is brought into main. the existing development environment on main doesn't need to be modified in any way. when the PR is brought into main the transition is seamless. it is easy to remove the environment and start again if needs be based on feedback in the review without disrupting anything. Adding new environments should be fairly infrequent procedure, but make sure to be aware so when merging the PR back into By adhering to the above process, it should mean that the state of main is always consistent with the environment.yaml files it contains, and the setup script should also be always be pointing to the correct environment.","title":"Modifying environment.yaml Files"},{"location":"developer_documentation/managing_environments/#environment-changes-on-bugfix-releases","text":"If at all possible, changes to environments on bugfix branches should be avoided as it will increases the amount of work needed introduces more potential things to go wrong Of course sometimes it is unavoidable so in such cases be aware of the following. merging changes made to an enironment.yaml file on a release branch into main is almost always going to be a bad idea and increase the possibility of producing unsolvable environments unless the files are identical. The normal practice of having a PR which brings changes from a dev branch (that branched from a release branch) into the release branch, and then cherry picking the squashed commit directly onto main may be a bad approach in this case. I would suggest that when bringing such changes into main it would be better to create a new branch off of main, cherry pick the bugfix code changes onto branch but revert environment changes.","title":"Environment Changes on Bugfix Releases"},{"location":"developer_documentation/managing_environments/#managing-development-environments","text":"Regarding point two, the naming conventions used for cdds development environments have been somewhat inconsistent and can at times lead to confusion. Here are a selection of cdds conda development environments. cdds-2.1_dev-1 cdds-2.1_dev-1 cdds-2.3.2_dev cdds-2.3_dev cdds-2.4.0_dev cdds-2.4.0_dev-1 cdds-2.4_dev_3 cdds-2.5.0_dev-1 A first step would be sticking with dev environments not being tied to specific hotfix versions. This is something that we have already done for some versions and not for others eg. Going forward I think it would be best to adopt the convention that does not reference a specific hotfix version such as (examples given for cdds version 2.5.0). cdds-2.5_dev-0 or cdds-2.5.X_dev-0 (the X is not meant to be replaced) This helps to avoid any confusion that can arise when there is a mismatch over the cdds version number specified in a given copy of cdds and the development environment that setup_env_for_development points to. Such situations can occur when there have been no updates to the environment for a period of time. It does not necessarily mean Additionally, particular attention should be paid to the change of version number on main following a maj/min release. This should be done immediately after the release.","title":"Managing Development Environments"},{"location":"developer_documentation/managing_environments/#concrete-example","text":"Here is a concrete example of the above starting from a 2.5.0 release. The 2.5.0 release branch v2.5_release is created from main Update main by Incrementing cdds version to 2.6.0 Creating a new environment called cdds-2.6_dev-0 Pointing setup_env_dev_for_development to this new cdds-2.6_dev-0 environment As development proceeds on main over time new development environments are created in line with the practices described earlier in this document. E.g. cdds-2.6_dev-1 - (change - added new package) cdds-2.6_dev-2 - (change - added new package) As development also proceeds on v2.5_release concurrently, and although it is not desirable, it may be necessary for a change to the environment. (For the purpose of this example we will assume the latest development environment was cdds-2.5_dev-1 when the v2.5_release branch was made.) A new environment cdds-2.5_dev-2 is created for this change. A new environment cdds-2.6_dev-3 is needed for development on main if this change needs to be brought over (this will almost always be the case). When main reaches a state ready to release 2.6.0 the process continues as above. The 2.6.0 release branch v2.6_release is created from main Then update main by Incrementing cdds version to 2.7.0 Creating a new environment called cdds-2.7_dev-0 Pointing setup_env_dev_for_development to cdds-2.7_dev-0 The resultant list of environments would look something like this. cdds-2.5_dev-1 cdds-2.6_dev-0 cdds-2.6_dev-1 cdds-2.6_dev-2 cdds-2.5_dev-2 cdds-2.6_dev-3 cdds-2.7_dev-0 A downside of the above convention is that there will be a duplicated environment for every 2.X release which differs only in name. I.e, in the above example cdds-2.7_dev-0 and cdds-2.6_dev-3 should be exactly the same, aside from having different names. However, I think this an acceptable inefficiency for preventing confusion around environment versioning.","title":"Concrete Example"},{"location":"developer_documentation/managing_environments/#conda-good-to-know","text":"When there are multiple channels defined in an environment.yaml there is an implicit priority based on their order, which if you are not aware of, conda may solve environments in ways you weren\u2019t expecting. See notes on --flexible_solve https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html#strict-channel-priority tldr flexible solve is the default and will prefer packages from higher priority channels even if there are more recent versions in other channels. Make use of mamba and the --dry-run option to speed your workflow up. The same versioned package","title":"Conda Good to Know"},{"location":"developer_documentation/mip_convert_functional_tests/","text":"Warning This documentation is currently under construction and may not be up to date. Create a test class (pattern: test_ ) in mip_convert.tests.test_functional. . Import the functional tests class for MIP convert tests from mip_convert.tests.test_functional.test_command_line import AbstractFunctionalTests Extends your test class from the AbstractFuntionalTests Implement the get_test_data function: The function should return the test data as an TestData object (e.g. Cmip6TestData or AriseTestData) The available TestData objects are in mip_convert.tests.test_functional.utils.configurations Specify the necessary individual values of the TestData object (like mip_table, variable, specific_info, etc.) For examples, see tests in mip_covert/tests/test_functional Implement your test method that runs check_convert. This function will trigger MIP convert with your given test data. Mark your test as a slow test by add following annotation to your test method @attr ( 'slow' ) Note For debugging a MIP convert functional test, remove the slow annotation of the test. Afterwards you can run the test as usual in your IDE.","title":"Mip Convert Functional Tests"},{"location":"developer_documentation/nightly_tests/","text":"Warning This documentation is currently under construction and may not be up to date.","title":"Nightly Tests"},{"location":"developer_documentation/pytest/","text":"Warning This documentation is currently under construction and may not be up to date. Pytest is the testing framework used in CDDS . It currently serves two main functions. It acts as the testrunner, whereby it collects all the tests and executes them. It is a unit testing library. The version of pytest currently used within cdds is 7.1.2 Link to the pytest 7.1.x Documentation Link to the pytest 7.1.x API Pytest Configuration [tool:pytest] python_files = test_*.py python_functions = test_* console_output_style = progress addopts = -m 'not slow and not integration and not rabbitMQ and not data_request' markers = slow rabbitMQ data_request integration style Running pytest CDDS already provides a script to run all tests via pytest: Activate the conda environment: source setup_env_for_devel Run the run_all_tests script: ./run_all_tests The script runs all unit tests including all integration and slow tests. It does not run the RabbitMQ tests! Tests failures will be logged in the cdds_test_failures.log file. Run RabbitMQ tests The RabbitMQ tests needs a proper RabbitMQ installation. Only, the server els055 and els056 have RabbitMQ properly installed for CDDS. RabbitMQ needs credentials that are specified in your home directory $HOME/.cdds_credentials and has following content: [rabbit] host = <hostname> port = <port number> userid = <user id> use_plain = true vhost = dds_dev password = <password> For the correct values of the fields, please contact @Matthew Mizielinski . Connect to one of the servers via ssh, e.g: ssh els055 Go to your workspace directory for CDDS. (the same as on your local VM) Activate the conda environment: source setup_env_for_devel Run the test script: ./run_rabbitmq_tests Any test failures will be recorded in the cdds_rabbitmq_test_failures.log file. Run single tests Run all tests in a specific package pytest -s <package-folder> For example: run all tests in mip_convert: pytest -s mip_convert Run all tests in a specific module pytest <test_module.py> For example: Run tests in test_coding_standards.py: pytest test_coding_standards.py Run specific test in a TestCase class One option is to us -k . The -k command line option to specify an expression which implements a sub-string match on the test names: pytest <test_module.py> -k <TestClass> For example run all tests implemented in TestLoadCmipPlugin class in the module test_plugin_loader: pytest test_plugin_loader.py -k TestLoadCmipPlugin Additional, you can also use Node IDs to run all tests of a specific class. How to do this, see next section. Run specific test To run a specific test, pytest use Node IDs that is assigned to each collected test and which consists of the module filename followed by specifiers like class names, function names and parameters from parametrization separated by :: characters. A specific test implemented in a specific TestClass can be run by using pytest <test_module.py>::<TestClass>::<test_method> For example, run the test test_load_cdds_plugin in the module test_plugin_loader.py: pytest test_plugin_loader.py::TestLoadPlugin::test_load_cdds_plugin Node IDs are of the form module.py::class::method or module.py::function. Node IDs control which tests are collected, so module.py::class will select all test methods on the class. Useful Command Line Options There are many other options that can be used to configure pytest. Option Description -h show help message and configuration info -k <expression> only run tests which match the given substring expression -v increase verbosity --no-summary disable summary -r <chars> show extra text summary info as specified by chars: (f)ailded, (E)rror, (s)kipped, (x)failed, (X)passed, (p)assed, (P)assed with output, (a)ll excepted passed (p/P), or (A)ll. (w)arnings are enabled by default. --disable-warnings disable warnings summary --tb=<style> traceback print mode (auto/long/short/line/native/ no) -c <file> load configuration from --collect-only only collect tests, don\u2019t execute them --debug=[DEBUG_FILE_NAME] store internal tracing debug information in this log file. Default is pytestdebug.log Pytest Annotations Annotation Description Link @pytest . mark . skip ( message ) Skip an executing test with the given message. @pytest . mark . parametrize ( input_parameter , test_data ) It allows one to define multiple sets of arguments and fixtures at the test function or class. Parametrizing fixtures and test functions \u2014 pytest documentation Using nosetests attributes We still use the nosetests attributes to specify slow tests, integration tests, etc. These attributes can still be used because of pytest-attrib Test can be run with an -a option: pytest -a slow This runs all slow tests. If you want to not run the slow tests, you can do this as followed: pytest -a \"not slow\" The expression given in the -a argument can be even more complex, for example: pytest -a \"slow and integration\" Plugins Pytest has a large plugin ecosystem. pytest-cov Usage.","title":"Pytest"},{"location":"developer_documentation/pytest/#pytest-configuration","text":"[tool:pytest] python_files = test_*.py python_functions = test_* console_output_style = progress addopts = -m 'not slow and not integration and not rabbitMQ and not data_request' markers = slow rabbitMQ data_request integration style","title":"Pytest Configuration"},{"location":"developer_documentation/pytest/#running-pytest","text":"CDDS already provides a script to run all tests via pytest: Activate the conda environment: source setup_env_for_devel Run the run_all_tests script: ./run_all_tests The script runs all unit tests including all integration and slow tests. It does not run the RabbitMQ tests! Tests failures will be logged in the cdds_test_failures.log file. Run RabbitMQ tests The RabbitMQ tests needs a proper RabbitMQ installation. Only, the server els055 and els056 have RabbitMQ properly installed for CDDS. RabbitMQ needs credentials that are specified in your home directory $HOME/.cdds_credentials and has following content: [rabbit] host = <hostname> port = <port number> userid = <user id> use_plain = true vhost = dds_dev password = <password> For the correct values of the fields, please contact @Matthew Mizielinski . Connect to one of the servers via ssh, e.g: ssh els055 Go to your workspace directory for CDDS. (the same as on your local VM) Activate the conda environment: source setup_env_for_devel Run the test script: ./run_rabbitmq_tests Any test failures will be recorded in the cdds_rabbitmq_test_failures.log file. Run single tests Run all tests in a specific package pytest -s <package-folder> For example: run all tests in mip_convert: pytest -s mip_convert Run all tests in a specific module pytest <test_module.py> For example: Run tests in test_coding_standards.py: pytest test_coding_standards.py Run specific test in a TestCase class One option is to us -k . The -k command line option to specify an expression which implements a sub-string match on the test names: pytest <test_module.py> -k <TestClass> For example run all tests implemented in TestLoadCmipPlugin class in the module test_plugin_loader: pytest test_plugin_loader.py -k TestLoadCmipPlugin Additional, you can also use Node IDs to run all tests of a specific class. How to do this, see next section. Run specific test To run a specific test, pytest use Node IDs that is assigned to each collected test and which consists of the module filename followed by specifiers like class names, function names and parameters from parametrization separated by :: characters. A specific test implemented in a specific TestClass can be run by using pytest <test_module.py>::<TestClass>::<test_method> For example, run the test test_load_cdds_plugin in the module test_plugin_loader.py: pytest test_plugin_loader.py::TestLoadPlugin::test_load_cdds_plugin Node IDs are of the form module.py::class::method or module.py::function. Node IDs control which tests are collected, so module.py::class will select all test methods on the class.","title":"Running pytest"},{"location":"developer_documentation/pytest/#useful-command-line-options","text":"There are many other options that can be used to configure pytest. Option Description -h show help message and configuration info -k <expression> only run tests which match the given substring expression -v increase verbosity --no-summary disable summary -r <chars> show extra text summary info as specified by chars: (f)ailded, (E)rror, (s)kipped, (x)failed, (X)passed, (p)assed, (P)assed with output, (a)ll excepted passed (p/P), or (A)ll. (w)arnings are enabled by default. --disable-warnings disable warnings summary --tb=<style> traceback print mode (auto/long/short/line/native/ no) -c <file> load configuration from --collect-only only collect tests, don\u2019t execute them --debug=[DEBUG_FILE_NAME] store internal tracing debug information in this log file. Default is pytestdebug.log","title":"Useful Command Line Options"},{"location":"developer_documentation/pytest/#pytest-annotations","text":"Annotation Description Link @pytest . mark . skip ( message ) Skip an executing test with the given message. @pytest . mark . parametrize ( input_parameter , test_data ) It allows one to define multiple sets of arguments and fixtures at the test function or class. Parametrizing fixtures and test functions \u2014 pytest documentation","title":"Pytest Annotations"},{"location":"developer_documentation/pytest/#using-nosetests-attributes","text":"We still use the nosetests attributes to specify slow tests, integration tests, etc. These attributes can still be used because of pytest-attrib Test can be run with an -a option: pytest -a slow This runs all slow tests. If you want to not run the slow tests, you can do this as followed: pytest -a \"not slow\" The expression given in the -a argument can be even more complex, for example: pytest -a \"slow and integration\"","title":"Using nosetests attributes"},{"location":"developer_documentation/pytest/#plugins","text":"Pytest has a large plugin ecosystem.","title":"Plugins"},{"location":"developer_documentation/pytest/#pytest-cov","text":"Usage.","title":"pytest-cov"},{"location":"developer_documentation/review_process/","text":"Warning This documentation is currently under construction and may not be up to date. The reviewer and reviewee discuss the type of review required. If time is a factor, focus effort on the High Priority items. Agree whether the reviewer can make corrections related to the Low Priority items, e.g., typos or unused imports, directly to the branch. If the reviewer feels they need more information after an initial look at the code, they request a pair review with the reviewee. The reviewer presents the review to the reviewee (with reasons for priority as necessary), either via the Jira ticket (e.g., if the feedback is minimal), a Confluence page, e-mails, or in person, as appropriate. The reviewee assesses the reviewer's recommendations and makes any appropriate changes to the code. Open a new Jira ticket for any low risk issues that do not need to be resolved immediately. The reviewee documents the important points from the review about why things were done (or not done) for whatever reasons on the Jira ticket (for future reference). The reviewee updates the Coding Guidelines appropriately. Review Checklists High Priority Is the code in version control? Does the code work as expected? Does the code manage the risk around availability of resources such as files, databases, mass (assess the risk though first) Does the code check for common errors? Does the code use exceptions appropriately? Are there corresponding unit tests for the code? Can the unit tests be executed? Is there corresponding documentation for the code? Can the documentation be built? Is the documentation easy to understand? Medium Priority Is the code easy to read and understand? Has repetitive code been avoided? Is the code easy to maintain? Low Priority Does the code comply to the coding standards?","title":"Review Process"},{"location":"developer_documentation/review_process/#review-checklists","text":"","title":"Review Checklists"},{"location":"developer_documentation/review_process/#high-priority","text":"Is the code in version control? Does the code work as expected? Does the code manage the risk around availability of resources such as files, databases, mass (assess the risk though first) Does the code check for common errors? Does the code use exceptions appropriately? Are there corresponding unit tests for the code? Can the unit tests be executed? Is there corresponding documentation for the code? Can the documentation be built? Is the documentation easy to understand?","title":"High Priority"},{"location":"developer_documentation/review_process/#medium-priority","text":"Is the code easy to read and understand? Has repetitive code been avoided? Is the code easy to maintain?","title":"Medium Priority"},{"location":"developer_documentation/review_process/#low-priority","text":"Does the code comply to the coding standards?","title":"Low Priority"},{"location":"developer_documentation/unittests/","text":"Warning This documentation is currently under construction and may not be up to date. Running Unit Tests The CDDS code base includes a comprehensive set of unit and integration tests covering mode of the source code. Tests can be run in several ways. Start by navigating to the root directory of the CDDS source code. Ensure the development environment is correctly setup by running: source setup_env_for_devel Then, you can run on of the following commands to run tests: Run all tests on SPICE: sbatch submit_run_all_tests.batch Run all tests locally: ./run_all_tests Run tests for a particular package cdds pytest cdds pytest mip_convert Run tests for a particular test module: py.test hadsdk/hadsdk/tests/test_common.py Writing Unit Tests On some occasions Coding Guidelines WIP | Doctests may be sufficient as unit tests. Location and naming conventions Unit tests should be located in a parallel directory structure to the modules they are testing! For example: tests for the my_package/my_subpackage/my_module.py module should be located in the my_package/tests/my_subpackage/ directory in a test module with the name test_my_module.py Unit tests should not have docstrings. The log produced by pytest prints the name of the test in the form test_my_test_description (test_my_module.TestMyClass) if the test does not have a docstring, which is more helpful than the information that can fit into the first line of a docstring (which is printed if it exists). The pylint: disable = missing-docstring comment tells pylint not to warn about missing docstrings in the test module. Unit Test Example my_module.py # (C) British Crown Copyright 2015-2016, Met Office. # Please see LICENSE.rst for license details. \"\"\" My module. \"\"\" def my_function (): [ ... ] class MyClass (): def my_method1 ( self ): [ ... ] def my_method2 ( self ): [ ... ] test_my_module.py import unittest class TestMyClass ( unittest . TestCase ): \"\"\" Tests for MyClass in my_module.py. \"\"\" def test_ < class_test_description > ( self ): [ ... ] def test_my_method1_ < test_description > ( self ): [ ... ] def test_my_method2_ < test_description > ( self ): [ ... ] class TestMyFunction ( unittest . TestCase ): \"\"\" Tests for my_function in my_module.py. \"\"\" def test_ < first_test_description > ( self ): [ ... ] def test_ < second_test_description > ( self ): [ ... ] if __name__ == '__main__' : unittest . main ()","title":"Unit Testing"},{"location":"developer_documentation/unittests/#running-unit-tests","text":"The CDDS code base includes a comprehensive set of unit and integration tests covering mode of the source code. Tests can be run in several ways. Start by navigating to the root directory of the CDDS source code. Ensure the development environment is correctly setup by running: source setup_env_for_devel Then, you can run on of the following commands to run tests: Run all tests on SPICE: sbatch submit_run_all_tests.batch Run all tests locally: ./run_all_tests Run tests for a particular package cdds pytest cdds pytest mip_convert Run tests for a particular test module: py.test hadsdk/hadsdk/tests/test_common.py","title":"Running Unit Tests"},{"location":"developer_documentation/unittests/#writing-unit-tests","text":"On some occasions Coding Guidelines WIP | Doctests may be sufficient as unit tests. Location and naming conventions Unit tests should be located in a parallel directory structure to the modules they are testing! For example: tests for the my_package/my_subpackage/my_module.py module should be located in the my_package/tests/my_subpackage/ directory in a test module with the name test_my_module.py Unit tests should not have docstrings. The log produced by pytest prints the name of the test in the form test_my_test_description (test_my_module.TestMyClass) if the test does not have a docstring, which is more helpful than the information that can fit into the first line of a docstring (which is printed if it exists). The pylint: disable = missing-docstring comment tells pylint not to warn about missing docstrings in the test module. Unit Test Example my_module.py # (C) British Crown Copyright 2015-2016, Met Office. # Please see LICENSE.rst for license details. \"\"\" My module. \"\"\" def my_function (): [ ... ] class MyClass (): def my_method1 ( self ): [ ... ] def my_method2 ( self ): [ ... ] test_my_module.py import unittest class TestMyClass ( unittest . TestCase ): \"\"\" Tests for MyClass in my_module.py. \"\"\" def test_ < class_test_description > ( self ): [ ... ] def test_my_method1_ < test_description > ( self ): [ ... ] def test_my_method2_ < test_description > ( self ): [ ... ] class TestMyFunction ( unittest . TestCase ): \"\"\" Tests for my_function in my_module.py. \"\"\" def test_ < first_test_description > ( self ): [ ... ] def test_ < second_test_description > ( self ): [ ... ] if __name__ == '__main__' : unittest . main ()","title":"Writing Unit Tests"},{"location":"developer_documentation/deployment/cdds_installation/","text":"CDDS Installation Warning This page is still work-in-progress! Please work closely with the CDDS team when installing CDDS. On Met Office On Jasmin Note This installation process requires a Conda installation of at least v4.9 , earlier versions will raise errors around the setting of environment variables as part of the Conda environment creation commands Login to the cdds account (please see Science Shared Accounts for more details): bash xsudo -u cdds bash -l Activate Conda mamba environment (to make installation quicker): . $HOME /software/miniconda3/bin/activate mamba Obtain environment file: wget https://github.com/MetOffice/CDDS/blob/<tagname>/environment.yml or download it manually from Github. Update locations pointed to within the environment file: sed -i \"s/<location>/X.Y.Z/\" environment.yml Create environment mamba env create -f environment.yml -p $HOME /software/miniconda3/envs/cdds-X.Y.Z where X.Y.Z is the new version number of CDDS Note This has been updated following the roll out of Conda to MO systems. If the -p option is omitted then the installation will end up under $HOME/.conda and will not be visible to other users. Info If the wheel installation fails then you can end up with #!python rather than the full paths \u2013 this is known to be caused by not having _DEV updated in the packages, possibly due to tagging without pulling the release branch from the repository first Activate environment and set CDDS_ENV_COMMAND variable: conda activate cdds-X.Y.Z conda env config vars set CDDS_ENV_COMMAND = \"source $HOME /software/miniconda3/bin/activate cdds-X.Y.Z\" where X.Y.Z is the new version number of CDDS Set default branch name for CDDS Convert workflow ( u-ak283 ). At the Met Office this will be tags/X.Y.W where X.Y.W is the most recent tag of the workflow (this is not updated for every version of CDDS) conda env config vars set CDDS_CONVERT_WORKFLOW_BRANCH = \"tags/X.Y.W\" Set default branch name for CDDS Processing workflow ( u-cy526 ). At the Met Office this will be tags/X.Y.W where X.Y.W is the most recent tag of the workflow (this is not updated for every version of CDDS) conda env config vars set CDDS_PROCESSING_WORKFLOW_BRANCH = \"tags/X.Y.W\" Confirm environment variables: # get out of cdds environment conda deactivate # load environment again conda activate cdds-X.Y.Z # print environment variables echo $LC_ALL echo $CDDS_ENV_COMMAND echo $CDDS_CONVERT_WORKFLOW_BRANCH echo $CDDS_PROCESSING_WORKFLOW_BRANCH You should see en_GB.UTF-8 for LC_ALL plus the command set above for CDDS_ENV_COMMAND and the value for CDDS_CONVERT_WORKFLOW_BRANCH and the value for $CDDS_PROCESSING_WORKFLOW_BRANCH . Warning This section will be updated soon Info The recommended location for installation is miniconda3 environment under /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3 Login to the sciX JASMIN server. Activate conda without loading an environment . /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3/bin/activate Obtain environment file wget https://github.com/MetOffice/CDDS/blob/<tagname>/environment.yml or download it manually from Github Update locations pointed to within the environment file: sed -i \"s/<location>/X.Y.Z/\" environment.yml Create environment conda env create -f environment.yml -n cdds-X.Y.Z where X.Y.Z is the new version number of CDDS Activate environment and set CDDS_ENV_COMMAND variable conda activate cdds-X.Y.Z conda env config vars set CDDS_ENV_COMMAND = \"source /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3/bin/activate cdds-X.Y.Z\" Set default branch name for CDDS Convert workflow ( u-ak283 ). At the Met Office this will be tags/X.Y.W where X.Y.W is the most recent tag of the workflow (this is not updated for every version of CDDS) conda env config vars set CDDS_CONVERT_WORKFLOW_BRANCH = \"tags/X.Y.W\" on JASMIN the tag used, and set here should have an _JASMIN suffix. Set default branch name for CDDS Processing workflow ( u-cy526 ). At the Met Office this will be tags/X.Y.W where X.Y.W is the most recent tag of the workflow (this is not updated for every version of CDDS) conda env config vars set CDDS_PROCESSING_WORKFLOW_BRANCH = \"tags/X.Y.W\" on JASMIN the tag used, and set here should have an _JASMIN suffix. Install nco library conda install -c conda-forge nco Confirm environment variables: # get out of cdds environment conda deactivate # load environment again conda activate cdds-X.Y.Z # print environment variables echo $LC_ALL echo $CDDS_ENV_COMMAND echo $CDDS_CONVERT_WORKFLOW_BRANCH echo $CDDS_PROCESSING_WORKFLOW_BRANCH You should see en_GB.UTF-8 for LC_ALL and the command set above for CDDS_ENV_COMMAND and the value for CDDS_CONVERT_WORKFLOW_BRANCH and the value for $CDDS_PROCESSING_WORKFLOW_BRANCH .","title":"Installation"},{"location":"developer_documentation/deployment/cdds_installation/#cdds-installation","text":"Warning This page is still work-in-progress! Please work closely with the CDDS team when installing CDDS. On Met Office On Jasmin Note This installation process requires a Conda installation of at least v4.9 , earlier versions will raise errors around the setting of environment variables as part of the Conda environment creation commands Login to the cdds account (please see Science Shared Accounts for more details): bash xsudo -u cdds bash -l Activate Conda mamba environment (to make installation quicker): . $HOME /software/miniconda3/bin/activate mamba Obtain environment file: wget https://github.com/MetOffice/CDDS/blob/<tagname>/environment.yml or download it manually from Github. Update locations pointed to within the environment file: sed -i \"s/<location>/X.Y.Z/\" environment.yml Create environment mamba env create -f environment.yml -p $HOME /software/miniconda3/envs/cdds-X.Y.Z where X.Y.Z is the new version number of CDDS Note This has been updated following the roll out of Conda to MO systems. If the -p option is omitted then the installation will end up under $HOME/.conda and will not be visible to other users. Info If the wheel installation fails then you can end up with #!python rather than the full paths \u2013 this is known to be caused by not having _DEV updated in the packages, possibly due to tagging without pulling the release branch from the repository first Activate environment and set CDDS_ENV_COMMAND variable: conda activate cdds-X.Y.Z conda env config vars set CDDS_ENV_COMMAND = \"source $HOME /software/miniconda3/bin/activate cdds-X.Y.Z\" where X.Y.Z is the new version number of CDDS Set default branch name for CDDS Convert workflow ( u-ak283 ). At the Met Office this will be tags/X.Y.W where X.Y.W is the most recent tag of the workflow (this is not updated for every version of CDDS) conda env config vars set CDDS_CONVERT_WORKFLOW_BRANCH = \"tags/X.Y.W\" Set default branch name for CDDS Processing workflow ( u-cy526 ). At the Met Office this will be tags/X.Y.W where X.Y.W is the most recent tag of the workflow (this is not updated for every version of CDDS) conda env config vars set CDDS_PROCESSING_WORKFLOW_BRANCH = \"tags/X.Y.W\" Confirm environment variables: # get out of cdds environment conda deactivate # load environment again conda activate cdds-X.Y.Z # print environment variables echo $LC_ALL echo $CDDS_ENV_COMMAND echo $CDDS_CONVERT_WORKFLOW_BRANCH echo $CDDS_PROCESSING_WORKFLOW_BRANCH You should see en_GB.UTF-8 for LC_ALL plus the command set above for CDDS_ENV_COMMAND and the value for CDDS_CONVERT_WORKFLOW_BRANCH and the value for $CDDS_PROCESSING_WORKFLOW_BRANCH . Warning This section will be updated soon Info The recommended location for installation is miniconda3 environment under /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3 Login to the sciX JASMIN server. Activate conda without loading an environment . /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3/bin/activate Obtain environment file wget https://github.com/MetOffice/CDDS/blob/<tagname>/environment.yml or download it manually from Github Update locations pointed to within the environment file: sed -i \"s/<location>/X.Y.Z/\" environment.yml Create environment conda env create -f environment.yml -n cdds-X.Y.Z where X.Y.Z is the new version number of CDDS Activate environment and set CDDS_ENV_COMMAND variable conda activate cdds-X.Y.Z conda env config vars set CDDS_ENV_COMMAND = \"source /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3/bin/activate cdds-X.Y.Z\" Set default branch name for CDDS Convert workflow ( u-ak283 ). At the Met Office this will be tags/X.Y.W where X.Y.W is the most recent tag of the workflow (this is not updated for every version of CDDS) conda env config vars set CDDS_CONVERT_WORKFLOW_BRANCH = \"tags/X.Y.W\" on JASMIN the tag used, and set here should have an _JASMIN suffix. Set default branch name for CDDS Processing workflow ( u-cy526 ). At the Met Office this will be tags/X.Y.W where X.Y.W is the most recent tag of the workflow (this is not updated for every version of CDDS) conda env config vars set CDDS_PROCESSING_WORKFLOW_BRANCH = \"tags/X.Y.W\" on JASMIN the tag used, and set here should have an _JASMIN suffix. Install nco library conda install -c conda-forge nco Confirm environment variables: # get out of cdds environment conda deactivate # load environment again conda activate cdds-X.Y.Z # print environment variables echo $LC_ALL echo $CDDS_ENV_COMMAND echo $CDDS_CONVERT_WORKFLOW_BRANCH echo $CDDS_PROCESSING_WORKFLOW_BRANCH You should see en_GB.UTF-8 for LC_ALL and the command set above for CDDS_ENV_COMMAND and the value for CDDS_CONVERT_WORKFLOW_BRANCH and the value for $CDDS_PROCESSING_WORKFLOW_BRANCH .","title":"CDDS Installation"},{"location":"developer_documentation/deployment/documentation/","text":"Documentation CDDS uses an X.Y.Z versioning system for releases but for documentation we only publish the most recent bugfix release. For example, we would publish 1.0.0 as 1.0 and any following bugfix releases e.g., 1.0.1 would overwrite that existing 1.0 deployment. It is also not necessary to only publish the documentation as part of the release process. If documentation needs to be added or corrected this can be done independently of a release, as long as it does not reference any change or functionality that has not yet been released. Deployment Procedure Checkout the required release branch for deployment. git checkout vX.Y_release Confirm the branch is up to date. git pull Active the CDDS environment. source setup_env_for_devel Inspect the current list of deployed versions. mike list Deploy the new/updated version of the documentation. (This command should overwrite any existing existing deployments with the same name.) mike deploy X.Y Verify the new deployment works as expected. mike serve Push the local commit made by mike to the gh-pages branch to github. git push origin gh-pages","title":"Documentation"},{"location":"developer_documentation/deployment/documentation/#documentation","text":"CDDS uses an X.Y.Z versioning system for releases but for documentation we only publish the most recent bugfix release. For example, we would publish 1.0.0 as 1.0 and any following bugfix releases e.g., 1.0.1 would overwrite that existing 1.0 deployment. It is also not necessary to only publish the documentation as part of the release process. If documentation needs to be added or corrected this can be done independently of a release, as long as it does not reference any change or functionality that has not yet been released.","title":"Documentation"},{"location":"developer_documentation/deployment/documentation/#deployment-procedure","text":"Checkout the required release branch for deployment. git checkout vX.Y_release Confirm the branch is up to date. git pull Active the CDDS environment. source setup_env_for_devel Inspect the current list of deployed versions. mike list Deploy the new/updated version of the documentation. (This command should overwrite any existing existing deployments with the same name.) mike deploy X.Y Verify the new deployment works as expected. mike serve Push the local commit made by mike to the gh-pages branch to github. git push origin gh-pages","title":"Deployment Procedure"},{"location":"developer_documentation/deployment/infrastructure/","text":"CDDS Infrastructure Warning This page is still work-in-progress!","title":"Infrastructure"},{"location":"developer_documentation/deployment/infrastructure/#cdds-infrastructure","text":"Warning This page is still work-in-progress!","title":"CDDS Infrastructure"},{"location":"developer_documentation/deployment/release_procedure/","text":"Release Procedure Inform users of the upcoming change (if necessary) Info Ask Matthew Mizielinski if it is worth announcing the new version. Make the appropriate changes to the code CDDS Convert workflow (u-ak283) If there have been any changes to the CDDS Convert suite since the last tag, tag the CDDS Convert suite: fcm copy --parents https://code.metoffice.gov.uk/svn/roses-u/a/k/2/8/3/cdds_release_<release_branch_version> https://code.metoffice.gov.uk/svn/roses-u/a/k/2/8/3/tags/<tag> where <tag> is the CDDS version number, e.g. 1.0.1 and <release_branch_version> is major or minor version numbers, e.g. 1.0 . For JASMIN please append _JASMIN to the tag name and use cdds_jasmin_<release_branch_version> as the release branch. If the workflow release branch for this version does not yet exist, create one from the trunk using: fcm bc cdds_release_<release_branch_version> https://code.metoffice.gov.uk/svn/roses-u/a/k/2/8/3/trunk Changes in the CDDS Project Branch from the appropriate release branch git checkout <release_branch> git checkout -b <ticket_number>_v<tag>_release where <release_branch> is the name of the release branch. If an appropriate release branch doesn't exist, create one: git checkout main git checkout -b v<release_version>_release where <release_version> is e.g. 1.0 . Modify the CDDS code Update the development tag in cdds/cdds/__init__.py and mip_covert/mip_convert/__init__.py _DEV = False Use following command to do that: sed -i \"s/_DEV = True/_DEV = False/\" */*/__init__.py Update any version numbers of dependencies that need updating in setup_env_for_cdds Build the documentation Warning There is no release process defined for the new documentations. Please, speak with Jared and Matthew how to release the documentations. If releasing a new minor version of CDDS, e.g. 2.1.0 , update the development environment name in setup_env_for_devel to point to the new version, e.g. cdds-2.1_dev . Update the default CDDS Convert suite value in the conversion section of the request configuration Ensure that: All changes since the last release have been described in the relevant CHANGES.rst files. These should be added as a separate commit to allow cherry picking onto main later Any new files added since the last release that do not have a .py extension are included in MANIFEST.in and setup.py . Create a pull request for the changes. After the pull request is approved, merge the changes into the release branch, but do not squash merge them . This will allow you cherry-pick release notes from the release branch into main. Warning After changing this version number, the setup script won't work until the new version has been installed centrally in the cdds account. The installation process is documented at CDDSInstallation2 . Create a tag Using command line Using GitHub Only those that have admin permissions on the CDDS repository can create tags. Switch to the branch you want to tag (normally, the release branch) and make sure you have pulled changes on github to your local branch \u2013 failure to do this can lead to installation errors that manifest as failure to build wheels Create the tag: git tag <tagname> -a The <tagname> normally is the release version, e.g. v2.1.2 . Push the tag to the branch: git push origin <tagname> To show all tags and check if your tag is successfully created, run: git tag Info Github has a good documentation about release processes, see: Managing releases - GitHub Docs Install the code Follow the instructions provided in the CDDS installation Ensure all the tests pass in the 'real live environment' The test must be executed as cdds user export SRCDIR = $HOME /software/miniconda3/envs/cdds-X.Y.Z/lib/python3.8/site-packages/ echo \"# Executing tests for cdds:\" pytest -s $SRCDIR /cdds --doctest-modules -m 'not slow and not integration and not rabbitMQ and not data_request' pytest -s $SRCDIR /cdds -m slow pytest -s $SRCDIR /cdds -m integration pytest -s $SRCDIR /cdds -m data_request echo \"# Executing tests for mip_convert:\" pytest -s $SRCDIR /mip_convert --doctest-modules -m 'not slow and and not mappings and not superslow' pytest -s $SRCDIR /mip_convert -m mappings pytest -s $SRCDIR /mip_convert -m slow where X.Y.Z is the version number of CDDS. Info Slow unit tests for transfer and cdds_configure will display error messages to standard output. This is intentional, and does not indicate the tests fail (see transfer.tests.test_command_line.TestMainStore.test_transfer_functional_failing_moo() for details). Restore development mode and bump version Update the development tag and version number in <cdds_component>/<cdds_component>/__init__.py : _DEV = True _NUMERICAL_VERSION = '<next_version>' where <next_version> is the next minor version, e.g. 2.1.2 . Commit and push the change directly to the release branch. The commit message should be: <ticket_number>: Restore development mode. Ensure release note changes propagate into the main branch Using cherry-pick Using merge Ensure local copy of both main and release_branch are up to date. On the main branch use the git cherry-pick command to pull in just the CHANGES.rst updates with release notes and commit them. If you are unable to use the cherry-pick for the changes then the following may be useful. git merge the release branch into the trunk e.g., git merge v2.3_release --no-commit Inspect the differences in the local copy of the main branch Revert any changes other than to the CHANGES.rst file Commit and push changes to the main branch. Important Do not delete the release branch! (expect Matthew Mizielinski told you so) Create Release on GitHub Create a release on github from the tag. Include all major release notes and ensure that all links back to Jira work as expected. Create a discussion announcement from the release. Close Jira ticket Set the status of the Jira ticket to Done . Complete the milestone For completing the milestone, have a chat with Matthew Mizielinski which Jira epic needs to be updated or even closed. Info The list of Milestone epics can be found at the road map page in Jira.","title":"Release Procedure"},{"location":"developer_documentation/deployment/release_procedure/#release-procedure","text":"","title":"Release Procedure"},{"location":"developer_documentation/deployment/release_procedure/#inform-users-of-the-upcoming-change-if-necessary","text":"Info Ask Matthew Mizielinski if it is worth announcing the new version.","title":"Inform users of the upcoming change (if necessary)"},{"location":"developer_documentation/deployment/release_procedure/#make-the-appropriate-changes-to-the-code","text":"","title":"Make the appropriate changes to the code"},{"location":"developer_documentation/deployment/release_procedure/#cdds-convert-workflow-u-ak283","text":"If there have been any changes to the CDDS Convert suite since the last tag, tag the CDDS Convert suite: fcm copy --parents https://code.metoffice.gov.uk/svn/roses-u/a/k/2/8/3/cdds_release_<release_branch_version> https://code.metoffice.gov.uk/svn/roses-u/a/k/2/8/3/tags/<tag> where <tag> is the CDDS version number, e.g. 1.0.1 and <release_branch_version> is major or minor version numbers, e.g. 1.0 . For JASMIN please append _JASMIN to the tag name and use cdds_jasmin_<release_branch_version> as the release branch. If the workflow release branch for this version does not yet exist, create one from the trunk using: fcm bc cdds_release_<release_branch_version> https://code.metoffice.gov.uk/svn/roses-u/a/k/2/8/3/trunk","title":"CDDS Convert workflow (u-ak283)"},{"location":"developer_documentation/deployment/release_procedure/#changes-in-the-cdds-project","text":"","title":"Changes in the CDDS Project"},{"location":"developer_documentation/deployment/release_procedure/#branch-from-the-appropriate-release-branch","text":"git checkout <release_branch> git checkout -b <ticket_number>_v<tag>_release where <release_branch> is the name of the release branch. If an appropriate release branch doesn't exist, create one: git checkout main git checkout -b v<release_version>_release where <release_version> is e.g. 1.0 .","title":"Branch from the appropriate release branch"},{"location":"developer_documentation/deployment/release_procedure/#modify-the-cdds-code","text":"Update the development tag in cdds/cdds/__init__.py and mip_covert/mip_convert/__init__.py _DEV = False Use following command to do that: sed -i \"s/_DEV = True/_DEV = False/\" */*/__init__.py Update any version numbers of dependencies that need updating in setup_env_for_cdds Build the documentation Warning There is no release process defined for the new documentations. Please, speak with Jared and Matthew how to release the documentations. If releasing a new minor version of CDDS, e.g. 2.1.0 , update the development environment name in setup_env_for_devel to point to the new version, e.g. cdds-2.1_dev . Update the default CDDS Convert suite value in the conversion section of the request configuration Ensure that: All changes since the last release have been described in the relevant CHANGES.rst files. These should be added as a separate commit to allow cherry picking onto main later Any new files added since the last release that do not have a .py extension are included in MANIFEST.in and setup.py . Create a pull request for the changes. After the pull request is approved, merge the changes into the release branch, but do not squash merge them . This will allow you cherry-pick release notes from the release branch into main. Warning After changing this version number, the setup script won't work until the new version has been installed centrally in the cdds account. The installation process is documented at CDDSInstallation2 .","title":"Modify the CDDS code"},{"location":"developer_documentation/deployment/release_procedure/#create-a-tag","text":"Using command line Using GitHub Only those that have admin permissions on the CDDS repository can create tags. Switch to the branch you want to tag (normally, the release branch) and make sure you have pulled changes on github to your local branch \u2013 failure to do this can lead to installation errors that manifest as failure to build wheels Create the tag: git tag <tagname> -a The <tagname> normally is the release version, e.g. v2.1.2 . Push the tag to the branch: git push origin <tagname> To show all tags and check if your tag is successfully created, run: git tag Info Github has a good documentation about release processes, see: Managing releases - GitHub Docs","title":"Create a tag"},{"location":"developer_documentation/deployment/release_procedure/#install-the-code","text":"Follow the instructions provided in the CDDS installation","title":"Install the code"},{"location":"developer_documentation/deployment/release_procedure/#ensure-all-the-tests-pass-in-the-real-live-environment","text":"The test must be executed as cdds user export SRCDIR = $HOME /software/miniconda3/envs/cdds-X.Y.Z/lib/python3.8/site-packages/ echo \"# Executing tests for cdds:\" pytest -s $SRCDIR /cdds --doctest-modules -m 'not slow and not integration and not rabbitMQ and not data_request' pytest -s $SRCDIR /cdds -m slow pytest -s $SRCDIR /cdds -m integration pytest -s $SRCDIR /cdds -m data_request echo \"# Executing tests for mip_convert:\" pytest -s $SRCDIR /mip_convert --doctest-modules -m 'not slow and and not mappings and not superslow' pytest -s $SRCDIR /mip_convert -m mappings pytest -s $SRCDIR /mip_convert -m slow where X.Y.Z is the version number of CDDS. Info Slow unit tests for transfer and cdds_configure will display error messages to standard output. This is intentional, and does not indicate the tests fail (see transfer.tests.test_command_line.TestMainStore.test_transfer_functional_failing_moo() for details).","title":"Ensure all the tests pass in the 'real live environment'"},{"location":"developer_documentation/deployment/release_procedure/#restore-development-mode-and-bump-version","text":"Update the development tag and version number in <cdds_component>/<cdds_component>/__init__.py : _DEV = True _NUMERICAL_VERSION = '<next_version>' where <next_version> is the next minor version, e.g. 2.1.2 . Commit and push the change directly to the release branch. The commit message should be: <ticket_number>: Restore development mode.","title":"Restore development mode and bump version"},{"location":"developer_documentation/deployment/release_procedure/#ensure-release-note-changes-propagate-into-the-main-branch","text":"Using cherry-pick Using merge Ensure local copy of both main and release_branch are up to date. On the main branch use the git cherry-pick command to pull in just the CHANGES.rst updates with release notes and commit them. If you are unable to use the cherry-pick for the changes then the following may be useful. git merge the release branch into the trunk e.g., git merge v2.3_release --no-commit Inspect the differences in the local copy of the main branch Revert any changes other than to the CHANGES.rst file Commit and push changes to the main branch. Important Do not delete the release branch! (expect Matthew Mizielinski told you so)","title":"Ensure release note changes propagate into the main branch"},{"location":"developer_documentation/deployment/release_procedure/#create-release-on-github","text":"Create a release on github from the tag. Include all major release notes and ensure that all links back to Jira work as expected. Create a discussion announcement from the release.","title":"Create Release on GitHub"},{"location":"developer_documentation/deployment/release_procedure/#close-jira-ticket","text":"Set the status of the Jira ticket to Done .","title":"Close Jira ticket"},{"location":"developer_documentation/deployment/release_procedure/#complete-the-milestone","text":"For completing the milestone, have a chat with Matthew Mizielinski which Jira epic needs to be updated or even closed. Info The list of Milestone epics can be found at the road map page in Jira.","title":"Complete the milestone"},{"location":"developer_documentation/plugins/available_plugins/","text":"Available Plugins CIMP6 Plugin Responsible Code Base CDDS Team Link to code on GitHub The CMIP6 plugin provides all functionality needed for supporting CMIP6 projects. This is the default plugin for CDDS. When no plugin is given, the CMIP6 plugin will be loaded. The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements. CMIP6Plus Plugin Responsible Code Base CDDS Team Link to code on GitHub The CMIP6Plus plugin provides all functionality needed for supporting CMIP6Plus projects. This plugin is loaded if your MIP era in the request configuration file is CMIP6Plus . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements. CORDEX Plugin Responsible Code Base CDDS Team Link to code on GitHub The CORDEX plugin provides all functionality needed for supporting CORDEX project. It is the only plugin that handles regional models and is supported by the CDDS team. This plugin is loaded if your MIP era in the request configuration file is CORDEX . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements. GCModelDev Plugin Responsible Code Base CDDS Team Link to code on GitHub The GCModelDev plugin provides all functionality needed for supporting adhoc use of CDDS. This plugin is loaded if your MIP era in the request configuration file is GCModelDev . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements. EERIE Plugin Responsible Code Base Jon Seddon Link to code on GitHub The EERIE plugin provides all functionality needed for supporting EERIE project. This plugin is loaded if your MIP era in the request configuration file is EERIE . The plugin is implemented by Jon Seddon. ARISE Plugin Responsible Code Base Matthew Mizielinski Link to code on GitHub The ARISE plugin provides all functionality needed for supporting ARISE project. This plugin is loaded if your MIP era in the request configuration file is ARSIE and the module path is given. The plugin is implemented by Matthew Mizielinski.","title":"Available Plugins"},{"location":"developer_documentation/plugins/available_plugins/#available-plugins","text":"","title":"Available Plugins"},{"location":"developer_documentation/plugins/available_plugins/#cimp6-plugin","text":"Responsible Code Base CDDS Team Link to code on GitHub The CMIP6 plugin provides all functionality needed for supporting CMIP6 projects. This is the default plugin for CDDS. When no plugin is given, the CMIP6 plugin will be loaded. The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements.","title":"CIMP6 Plugin"},{"location":"developer_documentation/plugins/available_plugins/#cmip6plus-plugin","text":"Responsible Code Base CDDS Team Link to code on GitHub The CMIP6Plus plugin provides all functionality needed for supporting CMIP6Plus projects. This plugin is loaded if your MIP era in the request configuration file is CMIP6Plus . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements.","title":"CMIP6Plus Plugin"},{"location":"developer_documentation/plugins/available_plugins/#cordex-plugin","text":"Responsible Code Base CDDS Team Link to code on GitHub The CORDEX plugin provides all functionality needed for supporting CORDEX project. It is the only plugin that handles regional models and is supported by the CDDS team. This plugin is loaded if your MIP era in the request configuration file is CORDEX . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements.","title":"CORDEX Plugin"},{"location":"developer_documentation/plugins/available_plugins/#gcmodeldev-plugin","text":"Responsible Code Base CDDS Team Link to code on GitHub The GCModelDev plugin provides all functionality needed for supporting adhoc use of CDDS. This plugin is loaded if your MIP era in the request configuration file is GCModelDev . The plugin is implemented by the CDDS team. If you like any changes to this plugin, simply extend the plugin by a new one or speak with the CDDS team to make some improvements.","title":"GCModelDev Plugin"},{"location":"developer_documentation/plugins/available_plugins/#eerie-plugin","text":"Responsible Code Base Jon Seddon Link to code on GitHub The EERIE plugin provides all functionality needed for supporting EERIE project. This plugin is loaded if your MIP era in the request configuration file is EERIE . The plugin is implemented by Jon Seddon.","title":"EERIE Plugin"},{"location":"developer_documentation/plugins/available_plugins/#arise-plugin","text":"Responsible Code Base Matthew Mizielinski Link to code on GitHub The ARISE plugin provides all functionality needed for supporting ARISE project. This plugin is loaded if your MIP era in the request configuration file is ARSIE and the module path is given. The plugin is implemented by Matthew Mizielinski.","title":"ARISE Plugin"},{"location":"developer_documentation/plugins/plugin_framework/","text":"Plugin Framework Overview The plugin framework was designed using an object-oriented approach. This framework allows you to customise and extend CDDS. Plugins are loaded at runtime from a specific given path. A plugin is a bundle that adds functionality to CDDS such that CDDS can handle a specific model project (like CMIP6 or ARISE). This also allows third party developers to add functionality to CDDS without having access to the source code. The CDDS plugin framework loads the specific plugin into a plugin store that stores it during runtime. This avoids excessive loading of the plugin. The CDDS code gets access to the plugin via the plugin store. The CMIP6 plugin will be loaded by default if no other plugin is given. Plugin Like mentioned, a plugin is a bundle that adds new functionality to CDDS. As a result CDDS can handle a specific model project, also not CMIP6 projects. A CDDS plugin needs to provide all functionality to handle that corresponding project. That includes information about the supported models, girds and streams. The design of a plugin is illustrated in the below diagram: The red boxes represent classes, the green ones enum classes and the yellow ones modules. The blue box represent a singleton class that has the functionality as a cache. A plugin is loaded by the plugin loader and registered to the plugin store. The plugin store caches the plugin during runtime. Each plugin contains information about the supported models, streams and grid labels. The model parameters containing the model information also contains the information about the supported grid that are specified by the grid type (ocean or atmosphere). Available Plugins Name Supported Project Developer Team CMIP6 CMIP6 projects CDDS Team GCModelDev GCModelDev projects CDDS Team ARISE ARISE projects Matthew Mizielinski SNAPSI SNAPSI projects Matthew Mizielinski","title":"Plugin Framework"},{"location":"developer_documentation/plugins/plugin_framework/#plugin-framework","text":"","title":"Plugin Framework"},{"location":"developer_documentation/plugins/plugin_framework/#overview","text":"The plugin framework was designed using an object-oriented approach. This framework allows you to customise and extend CDDS. Plugins are loaded at runtime from a specific given path. A plugin is a bundle that adds functionality to CDDS such that CDDS can handle a specific model project (like CMIP6 or ARISE). This also allows third party developers to add functionality to CDDS without having access to the source code. The CDDS plugin framework loads the specific plugin into a plugin store that stores it during runtime. This avoids excessive loading of the plugin. The CDDS code gets access to the plugin via the plugin store. The CMIP6 plugin will be loaded by default if no other plugin is given.","title":"Overview"},{"location":"developer_documentation/plugins/plugin_framework/#plugin","text":"Like mentioned, a plugin is a bundle that adds new functionality to CDDS. As a result CDDS can handle a specific model project, also not CMIP6 projects. A CDDS plugin needs to provide all functionality to handle that corresponding project. That includes information about the supported models, girds and streams. The design of a plugin is illustrated in the below diagram: The red boxes represent classes, the green ones enum classes and the yellow ones modules. The blue box represent a singleton class that has the functionality as a cache. A plugin is loaded by the plugin loader and registered to the plugin store. The plugin store caches the plugin during runtime. Each plugin contains information about the supported models, streams and grid labels. The model parameters containing the model information also contains the information about the supported grid that are specified by the grid type (ocean or atmosphere).","title":"Plugin"},{"location":"developer_documentation/plugins/plugin_framework/#available-plugins","text":"Name Supported Project Developer Team CMIP6 CMIP6 projects CDDS Team GCModelDev GCModelDev projects CDDS Team ARISE ARISE projects Matthew Mizielinski SNAPSI SNAPSI projects Matthew Mizielinski","title":"Available Plugins"},{"location":"developer_documentation/request/request/","text":"Request Architecture Idea: There is a request object that represents the corresponding request configuration file. Each section in the request configuration file is represented by a request section object: Inheritance contains setting to load template for given request Metadata contains all metadata settings like the model ID or the MIP era Common contains common setting like the path to the root data folder or the path to the external plugin. Data contains all settings that are used to archive the data in MASS. Inventory contains all inventory settings. Conversion contains settings that specify how CDDS is run, e.g., skip any steps when running CDDS. Global Attributes contains all attributes that will be in the global attributes section of the CMOR file. Misc contains any settings that do not fit in any other section. There is a class for each section that inherited from a abstract section class that force the implementation of all necessary functionalities. Implementation: Classes The request class represents the request configuration. It contains all methods and functionality to modify and manipulate the request configuration. The request class contains for each section a corresponding section object. A section class must inherit by the abstract section class that specifies the methods that should be provided to create and update a particular section. The section classes are implemented as data classes. Data classes are used to easily show the developer what settings can be set in the section. The following class diagram gives an idea of the implementation: A request object can be created from a request configuration or from a rose_suite.info . Also, a section can be created from a request configuration or from a rose_suite.info . All settings items of a section can be asked for. The request object also provides a method to write the configuration into an output configuration file. To add all settings to this output configuration file, each settings object provides a method to add the settings to a configuration parser. An abstract section class is used to make sure that all section objects provide the necessary methods. Modules There is a request.py module that contains the implementation of the request object and the methods to create and manipulate this object. The abstract section class is implemented in its own module request_section.py . This module also contains common methods for a specific section class. Each section class has its own module with *_section.py as name pattern (e.g. common_section.py ). For validations of the request, a validation.py module is provided. Following diagram shows the modules and their dependencies:","title":"Request Architecture"},{"location":"developer_documentation/request/request/#request-architecture","text":"","title":"Request Architecture"},{"location":"developer_documentation/request/request/#idea","text":"There is a request object that represents the corresponding request configuration file. Each section in the request configuration file is represented by a request section object: Inheritance contains setting to load template for given request Metadata contains all metadata settings like the model ID or the MIP era Common contains common setting like the path to the root data folder or the path to the external plugin. Data contains all settings that are used to archive the data in MASS. Inventory contains all inventory settings. Conversion contains settings that specify how CDDS is run, e.g., skip any steps when running CDDS. Global Attributes contains all attributes that will be in the global attributes section of the CMOR file. Misc contains any settings that do not fit in any other section. There is a class for each section that inherited from a abstract section class that force the implementation of all necessary functionalities.","title":"Idea:"},{"location":"developer_documentation/request/request/#implementation","text":"","title":"Implementation:"},{"location":"developer_documentation/request/request/#classes","text":"The request class represents the request configuration. It contains all methods and functionality to modify and manipulate the request configuration. The request class contains for each section a corresponding section object. A section class must inherit by the abstract section class that specifies the methods that should be provided to create and update a particular section. The section classes are implemented as data classes. Data classes are used to easily show the developer what settings can be set in the section. The following class diagram gives an idea of the implementation: A request object can be created from a request configuration or from a rose_suite.info . Also, a section can be created from a request configuration or from a rose_suite.info . All settings items of a section can be asked for. The request object also provides a method to write the configuration into an output configuration file. To add all settings to this output configuration file, each settings object provides a method to add the settings to a configuration parser. An abstract section class is used to make sure that all section objects provide the necessary methods.","title":"Classes"},{"location":"developer_documentation/request/request/#modules","text":"There is a request.py module that contains the implementation of the request object and the methods to create and manipulate this object. The abstract section class is implemented in its own module request_section.py . This module also contains common methods for a specific section class. Each section class has its own module with *_section.py as name pattern (e.g. common_section.py ). For validations of the request, a validation.py module is provided. Following diagram shows the modules and their dependencies:","title":"Modules"},{"location":"operational_procedure/","text":"Operational Procedures See the documentation for CMIP6 and GCModelDev .","title":"Operational Procedures"},{"location":"operational_procedure/#operational-procedures","text":"See the documentation for CMIP6 and GCModelDev .","title":"Operational Procedures"},{"location":"operational_procedure/cmip6/","text":"Generating CMORised data with CDDS for CMIP6 / CMIP6 Plus simulations using the CDDS Workflow See also guidance for adhoc generation of CMORised data . Tip Use <script> -h or <script> --help to print information about the script, including available parameters. Example A simulation for the pre-industrial control from UKESM will be used as an example in these instructions. Prerequisites Before running the CDDS Operational Procedure, please ensure that: you own a CDDS operational simulation ticket (see the list of CDDS operational simulation tickets ) that will monitor the processing of a CMIP6 / CMIP6 Plus simulation using CDDS. you belong to the cdds group Tip type groups on the command line to print the groups a user is in you have write permissions to moose:/adhoc/projects/cdds/ on MASS Tip You can check if you have correct permissions by running following command and check if your moose username is included in the access control list output: moo getacl moose:/adhoc/projects/cdds you use a bash shell. CDDS uses Conda which can experience problems running in a shell other than bash. Tip You can check which shell you use by following command: echo $SHELL If the result is not /bin/bash , you can switch to a bash shell by running: /bin/bash If any of the above are not true please contact the CDDS Team for guidance. Packages CDDS is designed to handle a \"package\" of simulation data at one time; a set of variables from a particular simulation run. Multiple \"packages\" can be run for a given simulation to add new or corrected variables to the archive. Each package should be run using a separate processing ( proc ) and data directory. The simplest way to separate two run throughs of CDDS is to use a different package name. This is set either when running the write_request script below or by modifying the request configuration itself. Partial processing of a simulation In certain circumstances it may be desirable to process and submit a subset of an entire simulation, i.e. the first 250 years of the esm-piControl simulation. Please contact the CDDS Team to discuss this prior to starting processing to Get appropriate guidance on the steps needed to correctly construct the requested variables file in CDDS Prepare Arrange for an appropriate Errata to be issued following submission of data sets. What to do when things go wrong On occasion issues will arise with tasks performed by users of CDDS and these will trigger CRITICAL error messages in the logs and usually require user intervention. Many simple issues (MASS/MOOSE or file system problems) can be resolved by re-triggering tasks. When you take any action please ensure that you update your CDDS operational simulation ticket and if support is needed contact the CDDS Team . Set up the CDDS operational simulation ticket Select start work on the CDDS operational simulation ticket (so that the status is in_progress ) to indicate that work is starting. Activate the CDDS install MOHC JASMIN Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.3.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Note The available version numbers for this script can be found here If you wish to deactivate the CDDS environment then you can use the command conda deactivate . Create the request configuration file The request configuration file is constructed from information in the rose-suite.info files within each workflow. Important If the rose-suite.info file contains incorrect information, this will be propagated through CDDS. As such it is critically important that the information in these files is correct To construct the request configuration file take the following steps Set up a working directory mkdir cdds-example-1 cd cdds-example-1 export WORKING_DIR = ` pwd ` Add the location of your working directory to the CDDS operational simulation ticket . Collect required information on the rose workflow for the simulation; workflow id, e.g. u-aw310 branch, e.g. cdds revision Info You can find the revision of the workflow branch for a CMIP6 workflow by using the following command: rosie lookup --prefix = u --query project eq u-cmip6 and id eq u-aw310 and branch eq cdds Create the request configuration file; write_request <workflow id> <branch> <revision> <package name> [ <list of streams> ] -c <path to proc dir> -t <path to data dir> Example Create a request configuration file for the rose suite u-aw310 , branch cdds and package round-20 : write_request u-aw310 cdds 115492 round-20 ap4 ap5 ap6 onm inm \\ -c /project/cdds/proc -t /project/cdds_data Note Be careful when re-running CDDS using the same request configuration file: pre-existing data will cause problems for the extraction tasks and pre-existing logs in the proc directory may cause issues when diagnosing problems. If in doubt use a different package name in the request configuration file. Tip If necessary the start and end dates for processing can be overridden using the --start_date and --end_date arguments. Please consult with the CDDS Team if you believe this is necessary. Info The log file and request configuration file are written to the current working directory Prepare a list of variables to process Warning This method does not refer to the data request or CDDS inventory database (to check which datasets have been previously produced), so care should be taken with the choice of variables. Create a text file with the list of variables or copy and modify an existing list. Each line in the file should have the form <mip table>/<variable name>:<stream> Example For example process the variable tas for the MIP table Amon when processing the ap5 stream: Amon/tas:ap5 Set the value variable_list_file in the request configuration to the path of the created variable file. Note If you are using a workflow with the CMIP6 STASH set up then you can add the default stream to a list of variables using the command stream_mappings --varfile <filename without streams> --outfile <new file with streams> If you are not using a workflow with the CMIP6 STASH configuration then contact us for advice as this process will need to be performed by hand. Configure request configuration Important The request.cfg file contains all information that is needed to process the data through CDDS. The creation of the file does not set all values. So, it must be adjusted manually. You need to adjust your request.cfg : Open the request.cfg via a text editor, e.g. vi or gedit Following values need to be set manually: Value Description variable_list_file Path to your variables file output_mass_root Path to the moose loction where the data should be archived starts with moose: output_mass_suffix Sub-directory in MASS to used when moving data. Note Please check the other values as well and do adjustments as needed. For any help, please contact the CDDS Team . Info The MIP era ( CMIP6 or CIMP6 Plus ) you are using is defined in the value mip_era of the metdata section. Checkout and configure the CDDS workflow Run the following command after replacing values within <> : checkout_processing_workflow <name for processing workflow> \\ <path to request configuration> \\ --workflow_destination . Example Checkout the CDDS processing workflow with the name my-cdds-test and the request file location /home/foo/cdds-example-1/request.cfg : checkout_processing_workflow my-cdds-test \\ /home/foo/cdds-example-1/request.cfg \\ --workflow_destination . Info A directory containing a rose workflow will be placed in a subdirectory under the location specified in --workflow_destination . If this is not specified it will be checked out under ~/roses/ This step is optional: Set some useful environmental variables to access the CDDS directories: export CDDS_PROC_DIR = /<root_proc_dir>/<mip_era>/<mip>/<model_id>_<experiment_id>_<variant_label>/<package>/ export CDDS_DATA_DIR = /<root_data_dir>/<mip_era>/<mip>/<model_id>/<experiment_id>/<variant_label>/<package>/ ls $CDDS_PROC_DIR ls $CDDS_DATA_DIR where you must replace all values within <> . The root_proc_dir and root_data_dir are the values that has been specified in the request configuration. Example Assume: Path to the root proc directory is /home/foo/cdds-example-1/proc . Path to the root data directory is /home/foo/cdds-example-1/data . MIP era is CMIP6 and MIP CMIP . Model ID is UKESM1-0-LL for experiment piControl with variant label r1i1p1f2 and package round-1 Then the command to set the environmental variables is: export CDDS_PROC_DIR = /home/foo/cdds-example-1/data/CMIP6/CMIP/UKESM1-0-LL_piControl_r1i1p1f2/round-1/ export CDDS_DATA_DIR = /home/foo/cdds-example-1/data/CMIP6/CMIP/UKESM1-0-LL/piControl/r1i1p1f2/round-1/ Run the workflow: cd <name for processing workflow> cylc vip . Example If the name of the processing workflow is my-cdds-test , then run: cd my-cdds-test cylc vip . Info Cylc 8 is used for running the processing workflow. You can do this by running following command before running the workflow: export CYLC_VERSION = 8 Monitor conversion workflows For each stream a CDDS Convert workflow will be triggered by the processing workflow. Each of the workflows launched by CDDS Convert requires monitoring. This can be done using the command line tool cylc gui to obtain a window with an updating summary of workflows progress or equivalently the Cylc Review online tools. Conversion workflows will usually be named cdds_<workflow_base_name>_<stream> and each stream will run completely independently. If a workflow has issues, due to task failure, it will stall, and you will receive an e-mail. If you hit issues or are unsure how to proceed update the CDDS operational simulation ticket for your package with anything you believe is relevant (include the location of your working directory) and contact the CDDS Team for advice. The conversion workflows run the following steps run_extract_<stream> Extract Run CDDS Extract for this stream. Runs in long queue with a wall time of 2 days. If there are any issues with extracting data they will be reported in the job.out log file in the workflow and the $CDDS_PROC_DIR/extract/log/cdds_extract_<stream>_<date stamp>.log log file and the task will fail. The extraction task will automatically resubmit 4 times if it fails and manual intervention is required to proceed. Most issues are related to either MASS (i.e. moo commands failing), file system anomalies (failure to create files /directories) or running out of time. Identify issues either by searching for \"CRITICAL\" in the job.out logs in Cylc Review or by using grep CRITICAL $CDDS_PROC_DIR /extract/log/cdds_extract_<stream>_<date stamp>.log If the issue appears to be due to MASS issues you can re-run the failed CDDS Extract job by re-triggering the run_extract_<stream> task via the cylc gui or via the cylc command line tools: cylc trigger cdds_<workflow_base_name>_<stream> run_extract_<stream>:failed If in doubt update your CDDS operational simulation ticket and contact CDDS Team for advice. validate_extract_<stream> Extract Validation Validation of the output is now performed as a separate task from extracting it. This task will report missing or unexpected files and unreadable netcdf files. setup_output_dir_<stream> Setup Output Directory This task will create output directories for conversion output. mip_convert_<stream>_<grid group> MIP Convert Run MIP Convert to produce output files for a small time window for this simulation. Will retry up to 3 times before workflow stalls. CRITICAL issues are appended to $CDDS_PROC_DIR/convert/log/critical_issues.log . These will likely need user action to correct for. So, update your CDDS operational simulation ticket and contact CDDS Team for advice. The CRITICAL log file will not exist if there are no critical issues. A variant named mip_convert_first_<stream>_<grid group> may be launched to align the cycling dates with the concatenation processing. finaliser_<stream> MIP Convert Finaliser This ensures that concatenation tasks are launched once all MIP Convert tasks have been successfully performed for a particular time range. This step should never fail. Note If this task fails, the reason is that the adjustment of the memory and time limits failed. So, please resubmit the task. organise_files_<stream> Organise Files Re-arranges the output files on disk from a directory structure created by the MIP Convert tasks of the form $CDDS_DATA_DIR /output/<stream>_mip_convert/<YYYY-MM-DD>/<grid>/<files> to $CDDS_DATA_DIR /output/<stream>_concat/<MIP table>/<variable name>/<files> Ready for concatenation. A variation named organise_files_final_<stream> does the same thing but at the end of the conversion process. mip_concatenate_setup_<stream> MIP Concatenate Setup This step constructs a list of concatenation jobs that must be performed mip_concatenate_batch_<stream> MIP Concatenate Batch Perform the concatenation commands ( ncrcat ) required to join small files together. Runs in long queue with a wall time of 2 days and can retry up to 3 times before workflow stalls (failures are usually due to running out of time while performing a concatenation). Only one mip_concatenate_batch_<stream> task can run at one time. Issues can be identified using: grep CRITICAL $CDDS_PROC_DIR /convert/log/mip_concatenate_*.log If any critical issues arise or tasks fail update your CDDS operational simulation ticket and contact the CDDS Team for advice. Output data is written to $CDDS_DATA_DIR /output/<stream>/<MIP table>/<variable name>/<files> run_qc_<stream> Quality Check (QC) Run the QC process on output data for this stream Produces a report at: $CDDS_PROC_DIR /qualitycheck/report_<stream>_<datestamp>.json and a list of variables which pass the quality checks at: $CDDS_PROC_DIR /qualitycheck/approved_variables_<stream>_<datestamp>.txt and a log file at: $CDDS_PROC_DIR /qualitycheck/log/qc_run_and_report_<stream>_<datestamp>.log The approved variables file will have one line per successfully produced dataset of the form: <MIP table>/<variable name> ; <Directory containing files> This task will fail if any QC issues are found and will not resubmit. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. run_transfer_<stream> Transfer Archive data for variables that are marked active in the requested variables file produced by CDDS Prepare and have successfully passed the QC checks, i.e. are listed in the approved variables file. Will not automatically retry, even if failure was due to MASS/MOOSE issues. The location in MASS to which these data are archived is determined by the output_mass_suffix argument specified in the request configuration file. Task will fail if There are MASS issues: For example if the following command returns anything there has been a MASS outage and you can re-trigger the task: grep SSC_STORAGE_SYSTEM_UNAVAILABLE $CDDS_PROC_DIR /archive/log/cdds_store_<stream>_<date stamp>.log An attempt is made to archive data that already exists in MASS. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. VERY IMPORTANT Do not delete data from MASS without consultation with Matt Mizielinski . completion_<stream> Completion This is a dummy task that is the last thing to run in the workflow -- this is to allow inter workflow dependencies by allowing the CDDS workflow to monitor whether each per stream workflow has completed. If all goes well the workflow will complete, and you will receive an email confirming that the workflow has shutdown containing content of the form: Message: AUTOMATIC See: http://fcm1/cylc-review/taskjobs/<user id>/<workflow name> Prepare CDDS operational simulation ticket for review & submission Once all workflows for a particular package have completed update your CDDS operational simulation ticket confirming that the Extract, Convert, QC and Transfer tasks have been completed. Note You can check if workflows has completed by using the command cylc gscan or using the cylc review tool. Copy the request JSON file and any logs to $CDDS_PROC_DIR cp request.json *.log $CDDS_PROC_DIR / Add a comment to the CDDS operational simulation ticket specifying the archived data is ready for submission, and include the full path to your request configuration location. Select assign for review to on the CDDS operational simulation ticket (so that the status is reviewing ) and assign the CDDS operational simulation ticket to Matthew Mizielinski by selecting this name from the list. The ticket will then be reviewed according to the CDDS simulation review procedure by members of the CDDS team. Info The review script used by the CDDS team involves running the following command cdds_sim_review <path to the request configuration> checking any CRITICAL issues and following up any other anomalies. Run CDDS Teardown Once the approved ticket has been returned to you following submission, delete the contents of the data directory: cd <path to the data directory> rm -rf input output Delete all workflows used: cdds_clean <path to the request configuration> Update and close the CDDS operational simulation ticket","title":"CMIP6 and CMIP6 Plus"},{"location":"operational_procedure/cmip6/#generating-cmorised-data-with-cdds-for-cmip6-cmip6-plus-simulations-using-the-cdds-workflow","text":"See also guidance for adhoc generation of CMORised data . Tip Use <script> -h or <script> --help to print information about the script, including available parameters. Example A simulation for the pre-industrial control from UKESM will be used as an example in these instructions.","title":"Generating CMORised data with CDDS for CMIP6 / CMIP6 Plus simulations using the CDDS Workflow"},{"location":"operational_procedure/cmip6/#prerequisites","text":"Before running the CDDS Operational Procedure, please ensure that: you own a CDDS operational simulation ticket (see the list of CDDS operational simulation tickets ) that will monitor the processing of a CMIP6 / CMIP6 Plus simulation using CDDS. you belong to the cdds group Tip type groups on the command line to print the groups a user is in you have write permissions to moose:/adhoc/projects/cdds/ on MASS Tip You can check if you have correct permissions by running following command and check if your moose username is included in the access control list output: moo getacl moose:/adhoc/projects/cdds you use a bash shell. CDDS uses Conda which can experience problems running in a shell other than bash. Tip You can check which shell you use by following command: echo $SHELL If the result is not /bin/bash , you can switch to a bash shell by running: /bin/bash If any of the above are not true please contact the CDDS Team for guidance.","title":"Prerequisites"},{"location":"operational_procedure/cmip6/#packages","text":"CDDS is designed to handle a \"package\" of simulation data at one time; a set of variables from a particular simulation run. Multiple \"packages\" can be run for a given simulation to add new or corrected variables to the archive. Each package should be run using a separate processing ( proc ) and data directory. The simplest way to separate two run throughs of CDDS is to use a different package name. This is set either when running the write_request script below or by modifying the request configuration itself.","title":"Packages"},{"location":"operational_procedure/cmip6/#partial-processing-of-a-simulation","text":"In certain circumstances it may be desirable to process and submit a subset of an entire simulation, i.e. the first 250 years of the esm-piControl simulation. Please contact the CDDS Team to discuss this prior to starting processing to Get appropriate guidance on the steps needed to correctly construct the requested variables file in CDDS Prepare Arrange for an appropriate Errata to be issued following submission of data sets.","title":"Partial processing of a simulation"},{"location":"operational_procedure/cmip6/#what-to-do-when-things-go-wrong","text":"On occasion issues will arise with tasks performed by users of CDDS and these will trigger CRITICAL error messages in the logs and usually require user intervention. Many simple issues (MASS/MOOSE or file system problems) can be resolved by re-triggering tasks. When you take any action please ensure that you update your CDDS operational simulation ticket and if support is needed contact the CDDS Team .","title":"What to do when things go wrong"},{"location":"operational_procedure/cmip6/#set-up-the-cdds-operational-simulation-ticket","text":"Select start work on the CDDS operational simulation ticket (so that the status is in_progress ) to indicate that work is starting.","title":"Set up the CDDS operational simulation ticket"},{"location":"operational_procedure/cmip6/#activate-the-cdds-install","text":"MOHC JASMIN Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.3.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Note The available version numbers for this script can be found here If you wish to deactivate the CDDS environment then you can use the command conda deactivate .","title":"Activate the CDDS install"},{"location":"operational_procedure/cmip6/#create-the-request-configuration-file","text":"The request configuration file is constructed from information in the rose-suite.info files within each workflow. Important If the rose-suite.info file contains incorrect information, this will be propagated through CDDS. As such it is critically important that the information in these files is correct To construct the request configuration file take the following steps Set up a working directory mkdir cdds-example-1 cd cdds-example-1 export WORKING_DIR = ` pwd ` Add the location of your working directory to the CDDS operational simulation ticket . Collect required information on the rose workflow for the simulation; workflow id, e.g. u-aw310 branch, e.g. cdds revision Info You can find the revision of the workflow branch for a CMIP6 workflow by using the following command: rosie lookup --prefix = u --query project eq u-cmip6 and id eq u-aw310 and branch eq cdds Create the request configuration file; write_request <workflow id> <branch> <revision> <package name> [ <list of streams> ] -c <path to proc dir> -t <path to data dir> Example Create a request configuration file for the rose suite u-aw310 , branch cdds and package round-20 : write_request u-aw310 cdds 115492 round-20 ap4 ap5 ap6 onm inm \\ -c /project/cdds/proc -t /project/cdds_data Note Be careful when re-running CDDS using the same request configuration file: pre-existing data will cause problems for the extraction tasks and pre-existing logs in the proc directory may cause issues when diagnosing problems. If in doubt use a different package name in the request configuration file. Tip If necessary the start and end dates for processing can be overridden using the --start_date and --end_date arguments. Please consult with the CDDS Team if you believe this is necessary. Info The log file and request configuration file are written to the current working directory","title":"Create the request configuration file"},{"location":"operational_procedure/cmip6/#prepare-a-list-of-variables-to-process","text":"Warning This method does not refer to the data request or CDDS inventory database (to check which datasets have been previously produced), so care should be taken with the choice of variables. Create a text file with the list of variables or copy and modify an existing list. Each line in the file should have the form <mip table>/<variable name>:<stream> Example For example process the variable tas for the MIP table Amon when processing the ap5 stream: Amon/tas:ap5 Set the value variable_list_file in the request configuration to the path of the created variable file. Note If you are using a workflow with the CMIP6 STASH set up then you can add the default stream to a list of variables using the command stream_mappings --varfile <filename without streams> --outfile <new file with streams> If you are not using a workflow with the CMIP6 STASH configuration then contact us for advice as this process will need to be performed by hand.","title":"Prepare a list of variables to process"},{"location":"operational_procedure/cmip6/#configure-request-configuration","text":"Important The request.cfg file contains all information that is needed to process the data through CDDS. The creation of the file does not set all values. So, it must be adjusted manually. You need to adjust your request.cfg : Open the request.cfg via a text editor, e.g. vi or gedit Following values need to be set manually: Value Description variable_list_file Path to your variables file output_mass_root Path to the moose loction where the data should be archived starts with moose: output_mass_suffix Sub-directory in MASS to used when moving data. Note Please check the other values as well and do adjustments as needed. For any help, please contact the CDDS Team . Info The MIP era ( CMIP6 or CIMP6 Plus ) you are using is defined in the value mip_era of the metdata section.","title":"Configure request configuration"},{"location":"operational_procedure/cmip6/#checkout-and-configure-the-cdds-workflow","text":"Run the following command after replacing values within <> : checkout_processing_workflow <name for processing workflow> \\ <path to request configuration> \\ --workflow_destination . Example Checkout the CDDS processing workflow with the name my-cdds-test and the request file location /home/foo/cdds-example-1/request.cfg : checkout_processing_workflow my-cdds-test \\ /home/foo/cdds-example-1/request.cfg \\ --workflow_destination . Info A directory containing a rose workflow will be placed in a subdirectory under the location specified in --workflow_destination . If this is not specified it will be checked out under ~/roses/ This step is optional: Set some useful environmental variables to access the CDDS directories: export CDDS_PROC_DIR = /<root_proc_dir>/<mip_era>/<mip>/<model_id>_<experiment_id>_<variant_label>/<package>/ export CDDS_DATA_DIR = /<root_data_dir>/<mip_era>/<mip>/<model_id>/<experiment_id>/<variant_label>/<package>/ ls $CDDS_PROC_DIR ls $CDDS_DATA_DIR where you must replace all values within <> . The root_proc_dir and root_data_dir are the values that has been specified in the request configuration. Example Assume: Path to the root proc directory is /home/foo/cdds-example-1/proc . Path to the root data directory is /home/foo/cdds-example-1/data . MIP era is CMIP6 and MIP CMIP . Model ID is UKESM1-0-LL for experiment piControl with variant label r1i1p1f2 and package round-1 Then the command to set the environmental variables is: export CDDS_PROC_DIR = /home/foo/cdds-example-1/data/CMIP6/CMIP/UKESM1-0-LL_piControl_r1i1p1f2/round-1/ export CDDS_DATA_DIR = /home/foo/cdds-example-1/data/CMIP6/CMIP/UKESM1-0-LL/piControl/r1i1p1f2/round-1/ Run the workflow: cd <name for processing workflow> cylc vip . Example If the name of the processing workflow is my-cdds-test , then run: cd my-cdds-test cylc vip . Info Cylc 8 is used for running the processing workflow. You can do this by running following command before running the workflow: export CYLC_VERSION = 8","title":"Checkout and configure the CDDS workflow"},{"location":"operational_procedure/cmip6/#monitor-conversion-workflows","text":"For each stream a CDDS Convert workflow will be triggered by the processing workflow. Each of the workflows launched by CDDS Convert requires monitoring. This can be done using the command line tool cylc gui to obtain a window with an updating summary of workflows progress or equivalently the Cylc Review online tools. Conversion workflows will usually be named cdds_<workflow_base_name>_<stream> and each stream will run completely independently. If a workflow has issues, due to task failure, it will stall, and you will receive an e-mail. If you hit issues or are unsure how to proceed update the CDDS operational simulation ticket for your package with anything you believe is relevant (include the location of your working directory) and contact the CDDS Team for advice. The conversion workflows run the following steps run_extract_<stream> Extract Run CDDS Extract for this stream. Runs in long queue with a wall time of 2 days. If there are any issues with extracting data they will be reported in the job.out log file in the workflow and the $CDDS_PROC_DIR/extract/log/cdds_extract_<stream>_<date stamp>.log log file and the task will fail. The extraction task will automatically resubmit 4 times if it fails and manual intervention is required to proceed. Most issues are related to either MASS (i.e. moo commands failing), file system anomalies (failure to create files /directories) or running out of time. Identify issues either by searching for \"CRITICAL\" in the job.out logs in Cylc Review or by using grep CRITICAL $CDDS_PROC_DIR /extract/log/cdds_extract_<stream>_<date stamp>.log If the issue appears to be due to MASS issues you can re-run the failed CDDS Extract job by re-triggering the run_extract_<stream> task via the cylc gui or via the cylc command line tools: cylc trigger cdds_<workflow_base_name>_<stream> run_extract_<stream>:failed If in doubt update your CDDS operational simulation ticket and contact CDDS Team for advice. validate_extract_<stream> Extract Validation Validation of the output is now performed as a separate task from extracting it. This task will report missing or unexpected files and unreadable netcdf files. setup_output_dir_<stream> Setup Output Directory This task will create output directories for conversion output. mip_convert_<stream>_<grid group> MIP Convert Run MIP Convert to produce output files for a small time window for this simulation. Will retry up to 3 times before workflow stalls. CRITICAL issues are appended to $CDDS_PROC_DIR/convert/log/critical_issues.log . These will likely need user action to correct for. So, update your CDDS operational simulation ticket and contact CDDS Team for advice. The CRITICAL log file will not exist if there are no critical issues. A variant named mip_convert_first_<stream>_<grid group> may be launched to align the cycling dates with the concatenation processing. finaliser_<stream> MIP Convert Finaliser This ensures that concatenation tasks are launched once all MIP Convert tasks have been successfully performed for a particular time range. This step should never fail. Note If this task fails, the reason is that the adjustment of the memory and time limits failed. So, please resubmit the task. organise_files_<stream> Organise Files Re-arranges the output files on disk from a directory structure created by the MIP Convert tasks of the form $CDDS_DATA_DIR /output/<stream>_mip_convert/<YYYY-MM-DD>/<grid>/<files> to $CDDS_DATA_DIR /output/<stream>_concat/<MIP table>/<variable name>/<files> Ready for concatenation. A variation named organise_files_final_<stream> does the same thing but at the end of the conversion process. mip_concatenate_setup_<stream> MIP Concatenate Setup This step constructs a list of concatenation jobs that must be performed mip_concatenate_batch_<stream> MIP Concatenate Batch Perform the concatenation commands ( ncrcat ) required to join small files together. Runs in long queue with a wall time of 2 days and can retry up to 3 times before workflow stalls (failures are usually due to running out of time while performing a concatenation). Only one mip_concatenate_batch_<stream> task can run at one time. Issues can be identified using: grep CRITICAL $CDDS_PROC_DIR /convert/log/mip_concatenate_*.log If any critical issues arise or tasks fail update your CDDS operational simulation ticket and contact the CDDS Team for advice. Output data is written to $CDDS_DATA_DIR /output/<stream>/<MIP table>/<variable name>/<files> run_qc_<stream> Quality Check (QC) Run the QC process on output data for this stream Produces a report at: $CDDS_PROC_DIR /qualitycheck/report_<stream>_<datestamp>.json and a list of variables which pass the quality checks at: $CDDS_PROC_DIR /qualitycheck/approved_variables_<stream>_<datestamp>.txt and a log file at: $CDDS_PROC_DIR /qualitycheck/log/qc_run_and_report_<stream>_<datestamp>.log The approved variables file will have one line per successfully produced dataset of the form: <MIP table>/<variable name> ; <Directory containing files> This task will fail if any QC issues are found and will not resubmit. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. run_transfer_<stream> Transfer Archive data for variables that are marked active in the requested variables file produced by CDDS Prepare and have successfully passed the QC checks, i.e. are listed in the approved variables file. Will not automatically retry, even if failure was due to MASS/MOOSE issues. The location in MASS to which these data are archived is determined by the output_mass_suffix argument specified in the request configuration file. Task will fail if There are MASS issues: For example if the following command returns anything there has been a MASS outage and you can re-trigger the task: grep SSC_STORAGE_SYSTEM_UNAVAILABLE $CDDS_PROC_DIR /archive/log/cdds_store_<stream>_<date stamp>.log An attempt is made to archive data that already exists in MASS. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. VERY IMPORTANT Do not delete data from MASS without consultation with Matt Mizielinski . completion_<stream> Completion This is a dummy task that is the last thing to run in the workflow -- this is to allow inter workflow dependencies by allowing the CDDS workflow to monitor whether each per stream workflow has completed. If all goes well the workflow will complete, and you will receive an email confirming that the workflow has shutdown containing content of the form: Message: AUTOMATIC See: http://fcm1/cylc-review/taskjobs/<user id>/<workflow name>","title":"Monitor conversion workflows"},{"location":"operational_procedure/cmip6/#prepare-cdds-operational-simulation-ticket-for-review-submission","text":"Once all workflows for a particular package have completed update your CDDS operational simulation ticket confirming that the Extract, Convert, QC and Transfer tasks have been completed. Note You can check if workflows has completed by using the command cylc gscan or using the cylc review tool. Copy the request JSON file and any logs to $CDDS_PROC_DIR cp request.json *.log $CDDS_PROC_DIR / Add a comment to the CDDS operational simulation ticket specifying the archived data is ready for submission, and include the full path to your request configuration location. Select assign for review to on the CDDS operational simulation ticket (so that the status is reviewing ) and assign the CDDS operational simulation ticket to Matthew Mizielinski by selecting this name from the list. The ticket will then be reviewed according to the CDDS simulation review procedure by members of the CDDS team. Info The review script used by the CDDS team involves running the following command cdds_sim_review <path to the request configuration> checking any CRITICAL issues and following up any other anomalies.","title":"Prepare CDDS operational simulation ticket for review &amp; submission"},{"location":"operational_procedure/cmip6/#run-cdds-teardown","text":"Once the approved ticket has been returned to you following submission, delete the contents of the data directory: cd <path to the data directory> rm -rf input output Delete all workflows used: cdds_clean <path to the request configuration> Update and close the CDDS operational simulation ticket","title":"Run CDDS Teardown"},{"location":"operational_procedure/gcmodeldev/","text":"Generating CMORised data with CDDS for GCModelDev simulations using the CDDS Workflow See also guidance for CMIP6 / CMIP6 Plus generation of CMORised data . Tip Use <script> -h or <script> --help to print information about the script, including available parameters. Example A simulation for the pre-industrial control from HadGEM3 will be used as an example in these instructions. Note The procedure below assumes that you are keeping track of progress using a CDDS progress ticket . For exercises such as CMIP6 this was managed centrally, but as GCModelDev is intended to provide support for ad-hoc processing we recommend you have some form of progress note, you are welcome to use the CDDS Trac for this purpose if you wish. Prerequisites Before running the CDDS Operational Procedure, please ensure that: you have a project framework to work within (see next section) you use a bash shell. CDDS uses Conda which can experience problems running in a shell other than bash. Tip You can check which shell you use by following command: echo $SHELL If the result is not /bin/bash , you can switch to a bash shell by running: /bin/bash If any of the above are not true please contact the CDDS Team for guidance. What to do when things go wrong On occasion issues will arise with tasks performed by users of CDDS and these will trigger CRITICAL error messages in the logs and usually require user intervention. Many simple issues (MASS/MOOSE or file system problems) can be resolved by re-triggering tasks. Support is available via the CDDS Team . The project framework The tools we have were written primarily for CMIP6 to CMORise HadGEM3 and UKESM1 model output, but can be applied to other projects provided an appropriate set of variable and metadata definitions are available. Defining a new project, e.g. CMIP6, requires a reasonable amount of information as does adding an entirely new model configuration and the CDDS team should be involved in discussions to do this if you need it. However, it is straightforward to use an existing project and include new activities and experiments. When run in relaxed mode CDDS will allow you to use any value for the mip (activity id) and experiment_id . We have a general purpose project for users interested in CMORising data for adhoc use called GCModelDev which takes the CMIP6 variable definitions and standards, to which we can add new variables as required. This is not intended for preparing data for immediate publication to locations such as ESGF, but can be used for analysis alongside CMIP6 data and for feeding in to tools that base themselves on the same data structure/standards. Activate the CDDS install MOHC JASMIN Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS progress ticket . Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS progress ticket . Note The available version numbers for this script can be found here (MetOffice github access required) If you wish to deactivate the CDDS environment then you can use the command conda deactivate . Create the request configuration file Info Please, also the documentation of the request configuration . A request configuration file contains a number of fields that guide what CDDS processing does and can be viewed as a \"control\" file with a reasonable number of arguments. The simplest approach is to copy an existing file and edit certain fields. Examples: GCModelDev HadGEM3-GC31-LL GCModelDev HadGEM3-GC31-LL using ens class GCModelDev HadGEM3-GC31-MM GCModelDev UKESM1-0-LL These request files should be suitable for use both within the Met Office and on JASMIN. If you are working with a particular model then to set up a new CDDS processing \"package\", the user would need to alter the experiment_id and/or variant_label fields, possibly the mip, and the workflow_id along with a set of streams Important Check that the mode in the common section of the request configuration file is set to relaxed . In relaxed mode CDDS will allow you to use any value for the mip (activity id) and experiment_id . Check that the mip_era in the metadata section of the request configuration file is set to GCModelDev . You also need to set following values manually: Value Description root_proc_dir Path to the CDDS proc directory root_data_dir Path to the CDDS data directory output_mass_root Path to the moose location where the data should be archived starts with moose: output_mass_suffix Sub-directory in MASS to used when moving data. Info The CDDS data directory is the directory where the model output files are written to. The CDDS proc directory is the directory where all the non-data outputs are written to, like the log files. Note Please check the other values, as well and do adjustments as needed. For any help, please contact the CDDS Team . Prepare a list of variables to process Create a text file with the list of variables or copy and modify an existing list. Each line in the file should have the form <mip table>/<variable name>:<stream> Example For example process the variable tas for the MIP table Amon when processing the ap5 stream: Amon/tas:ap5 Set the value variable_list_file in the request configuration to the path of the created variable file. Checkout and configure the CDDS workflow Run the following command after replacing values within <> : checkout_processing_workflow <name for processing workflow> \\ <path to request configuration> \\ --workflow_destination . Example Checkout the CDDS processing workflow with the name my-cdds-test and the request file location /home/foo/cdds-example-1/request.cfg : checkout_processing_workflow my-cdds-test \\ /home/foo/cdds-example-1/request.cfg \\ --workflow_destination . Info A directory containing a rose workflow will be placed in a subdirectory under the location specified in --workflow_destination . If this is not specified it will be checked out under ~/roses/ This step is optional: Set some useful environmental variables to access the CDDS directories: export CDDS_PROC_DIR = /<root_proc_dir>/<mip_era>/<mip>/<model_id>_<experiment_id>_<variant_label>/<package>/ export CDDS_DATA_DIR = /<root_data_dir>/<mip_era>/<mip>/<model_id>/<experiment_id>/<variant_label>/<package>/ ls $CDDS_PROC_DIR ls $CDDS_DATA_DIR where you must replace all values within <> . The root_proc_dir and root_data_dir are the values that has been specified in the request configuration. Run the workflow: cd <name for processing workflow> cylc vip . Info Cylc 8 is used for running the processing workflow. If your default version of cylc is not cylc 8 (run cylc --version to check) you will need to run the following command before running the workflow: export CYLC_VERSION = 8 Monitor conversion workflows For each stream a CDDS Convert workflow will be triggered by the processing workflow. Each of the workflows launched by CDDS Convert requires monitoring. This can be done using the command line tool cylc gui to obtain a window with an updating summary of workflows progress or equivalently the Cylc Review online tools. Conversion workflows will usually be named cdds_<model id>_<experiment id>_<variant_label>_<stream> and each stream will run completely independently. If a workflow has issues, due to task failure, it will stall, and you will receive an e-mail. If you hit issues or are unsure how to proceed update the CDDS progress ticket for your package with anything you believe is relevant (include the location of your working directory) and contact the CDDS Team for advice. The conversion workflows run the following steps run_extract_<stream> Extract Run CDDS Extract for this stream. Runs in long queue with a wall time of 2 days. If there are any issues with extracting data they will be reported in the job.err log file in the workflow and the $CDDS_PROC_DIR/extract/log/cdds_extract_<stream>_<date stamp>.log log file and the task will fail. The extraction task will automatically resubmit 4 times if it fails and manual intervention is required to proceed. Most issues are related to either MASS (i.e. moo commands failing), file system anomalies (failure to create files /directories) or running out of time. Identify issues either by searching for \"CRITICAL\" in the job.err logs in Cylc Review or by using grep CRITICAL $CDDS_PROC_DIR /extract/log/cdds_extract_<stream>_<date stamp>.log If the issue appears to be due to MASS issues you can re-run the failed CDDS Extract job by re-triggering the run_extract_<stream> task via the cylc gui or via the cylc command line tools: cylc trigger cdds_<model id>_<experiment id>_<variant label>_<stream> run_extract_<stream>:failed If in doubt update your CDDS progress ticket and contact CDDS Team for advice. validate_extract_<stream> Extract Validation Validation of the output is now performed as a separate task from extracting it. This task will report missing or unexpected files and unreadable netcdf files. setup_output_dir_<stream> Setup Output Directory This task will create output directories for conversion output. mip_convert_<stream>_<grid group> MIP Convert Run MIP Convert to produce output files for a small time window for this simulation. Will retry up to 3 times before workflow stalls. CRITICAL issues are appended to $CDDS_PROC_DIR/convert/log/critical_issues.log . These will likely need user action to correct for. So, update your CDDS progress ticket and contact CDDS Team for advice. The CRITICAL log file will not exist if there are no critical issues. A variant named mip_convert_first_<stream>_<grid group> may be launched to align the cycling dates with the concatenation processing. finaliser_<stream> MIP Convert Finaliser This ensures that concatenation tasks are launched once all MIP Convert tasks have been successfully performed for a particular time range. This step should never fail. organise_files_<stream> Organise Files Re-arranges the output files on disk from a directory structure created by the MIP Convert tasks of the form $CDDS_DATA_DIR /output/<stream>_mip_convert/<YYYY-MM-DD>/<grid>/<files> to $CDDS_DATA_DIR /output/<stream>_concat/<MIP table>/<variable name>/<files> Ready for concatenation. A variation named organise_files_final_<stream> does the same thing but at the end of the conversion process. mip_concatenate_setup_<stream> MIP Concatenate Setup This step constructs a list of concatenation jobs that must be performed mip_concatenate_batch_<stream> MIP Concatenate Batch Perform the concatenation commands ( ncrcat ) required to join small files together. Runs in long queue with a wall time of 2 days and can retry up to 3 times before workflow stalls (failures are usually due to running out of time while performing a concatenation). Only one mip_concatenate_batch_<stream> task can run at one time. Issues can be identified using: grep CRITICAL $CDDS_PROC_DIR /convert/log/mip_concatenate_*.log If any critical issues arise or tasks fail update your CDDS progress ticket and contact the CDDS Team for advice. Output data is written to $CDDS_DATA_DIR /output/<stream>/<MIP table>/<variable name>/<files> run_qc_<stream> Quality Check (QC) Run the QC process on output data for this stream Produces a report at: $CDDS_PROC_DIR /qualitycheck/report_<stream>_<datestamp>.json and a list of variables which pass the quality checks at: $CDDS_PROC_DIR /qualitycheck/approved_variables_<stream>_<datestamp>.txt and a log file at: $CDDS_PROC_DIR /qualitycheck/log/qc_run_and_report_<stream>_<datestamp>.log The approved variables file will have one line per successfully produced dataset of the form: <MIP table>/<variable name> ; <Directory containing files> This task will fail if any QC issues are found and will not resubmit. If this occurs please update your CDDS progress ticket and contact the CDDS Team for advice. run_transfer_<stream> Transfer Archive data for variables that are marked active in the requested variables file produced by CDDS Prepare and have successfully passed the QC checks, i.e. are listed in the approved variables file. Will not automatically retry, even if failure was due to MASS/MOOSE issues. The location in MASS to which these data are archived is determined by the output_mass_suffix argument specified in the request configuration file. Task will fail if There are MASS issues: For example if the following command returns anything there has been a MASS outage and you can re-trigger the task: grep SSC_STORAGE_SYSTEM_UNAVAILABLE $CDDS_PROC_DIR /archive/log/cdds_store_<stream>_<date stamp>.log An attempt is made to archive data that already exists in MASS. If this occurs please update your CDDS progress ticket and contact the CDDS Team for advice. VERY IMPORTANT Do not delete data from MASS without consultation with Matt Mizielinski . completion_<stream> Completion This is a dummy task that is the last thing to run in the workflow -- this is to allow inter workflow dependencies by allowing the CDDS workflow to monitor whether each per stream workflow has completed. If all goes well the workflow will complete, and you will receive an email confirming that the workflow has shutdown containing content of the form: Message: AUTOMATIC See: http://fcm1/cylc-review/taskjobs/<user id>/<workflow name>","title":"GCModelDev"},{"location":"operational_procedure/gcmodeldev/#generating-cmorised-data-with-cdds-for-gcmodeldev-simulations-using-the-cdds-workflow","text":"See also guidance for CMIP6 / CMIP6 Plus generation of CMORised data . Tip Use <script> -h or <script> --help to print information about the script, including available parameters. Example A simulation for the pre-industrial control from HadGEM3 will be used as an example in these instructions. Note The procedure below assumes that you are keeping track of progress using a CDDS progress ticket . For exercises such as CMIP6 this was managed centrally, but as GCModelDev is intended to provide support for ad-hoc processing we recommend you have some form of progress note, you are welcome to use the CDDS Trac for this purpose if you wish.","title":"Generating CMORised data with CDDS for GCModelDev simulations using the CDDS Workflow"},{"location":"operational_procedure/gcmodeldev/#prerequisites","text":"Before running the CDDS Operational Procedure, please ensure that: you have a project framework to work within (see next section) you use a bash shell. CDDS uses Conda which can experience problems running in a shell other than bash. Tip You can check which shell you use by following command: echo $SHELL If the result is not /bin/bash , you can switch to a bash shell by running: /bin/bash If any of the above are not true please contact the CDDS Team for guidance.","title":"Prerequisites"},{"location":"operational_procedure/gcmodeldev/#what-to-do-when-things-go-wrong","text":"On occasion issues will arise with tasks performed by users of CDDS and these will trigger CRITICAL error messages in the logs and usually require user intervention. Many simple issues (MASS/MOOSE or file system problems) can be resolved by re-triggering tasks. Support is available via the CDDS Team .","title":"What to do when things go wrong"},{"location":"operational_procedure/gcmodeldev/#the-project-framework","text":"The tools we have were written primarily for CMIP6 to CMORise HadGEM3 and UKESM1 model output, but can be applied to other projects provided an appropriate set of variable and metadata definitions are available. Defining a new project, e.g. CMIP6, requires a reasonable amount of information as does adding an entirely new model configuration and the CDDS team should be involved in discussions to do this if you need it. However, it is straightforward to use an existing project and include new activities and experiments. When run in relaxed mode CDDS will allow you to use any value for the mip (activity id) and experiment_id . We have a general purpose project for users interested in CMORising data for adhoc use called GCModelDev which takes the CMIP6 variable definitions and standards, to which we can add new variables as required. This is not intended for preparing data for immediate publication to locations such as ESGF, but can be used for analysis alongside CMIP6 data and for feeding in to tools that base themselves on the same data structure/standards.","title":"The project framework"},{"location":"operational_procedure/gcmodeldev/#activate-the-cdds-install","text":"MOHC JASMIN Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS progress ticket . Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 3.0.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS progress ticket . Note The available version numbers for this script can be found here (MetOffice github access required) If you wish to deactivate the CDDS environment then you can use the command conda deactivate .","title":"Activate the CDDS install"},{"location":"operational_procedure/gcmodeldev/#create-the-request-configuration-file","text":"Info Please, also the documentation of the request configuration . A request configuration file contains a number of fields that guide what CDDS processing does and can be viewed as a \"control\" file with a reasonable number of arguments. The simplest approach is to copy an existing file and edit certain fields. Examples: GCModelDev HadGEM3-GC31-LL GCModelDev HadGEM3-GC31-LL using ens class GCModelDev HadGEM3-GC31-MM GCModelDev UKESM1-0-LL These request files should be suitable for use both within the Met Office and on JASMIN. If you are working with a particular model then to set up a new CDDS processing \"package\", the user would need to alter the experiment_id and/or variant_label fields, possibly the mip, and the workflow_id along with a set of streams Important Check that the mode in the common section of the request configuration file is set to relaxed . In relaxed mode CDDS will allow you to use any value for the mip (activity id) and experiment_id . Check that the mip_era in the metadata section of the request configuration file is set to GCModelDev . You also need to set following values manually: Value Description root_proc_dir Path to the CDDS proc directory root_data_dir Path to the CDDS data directory output_mass_root Path to the moose location where the data should be archived starts with moose: output_mass_suffix Sub-directory in MASS to used when moving data. Info The CDDS data directory is the directory where the model output files are written to. The CDDS proc directory is the directory where all the non-data outputs are written to, like the log files. Note Please check the other values, as well and do adjustments as needed. For any help, please contact the CDDS Team .","title":"Create the request configuration file"},{"location":"operational_procedure/gcmodeldev/#prepare-a-list-of-variables-to-process","text":"Create a text file with the list of variables or copy and modify an existing list. Each line in the file should have the form <mip table>/<variable name>:<stream> Example For example process the variable tas for the MIP table Amon when processing the ap5 stream: Amon/tas:ap5 Set the value variable_list_file in the request configuration to the path of the created variable file.","title":"Prepare a list of variables to process"},{"location":"operational_procedure/gcmodeldev/#checkout-and-configure-the-cdds-workflow","text":"Run the following command after replacing values within <> : checkout_processing_workflow <name for processing workflow> \\ <path to request configuration> \\ --workflow_destination . Example Checkout the CDDS processing workflow with the name my-cdds-test and the request file location /home/foo/cdds-example-1/request.cfg : checkout_processing_workflow my-cdds-test \\ /home/foo/cdds-example-1/request.cfg \\ --workflow_destination . Info A directory containing a rose workflow will be placed in a subdirectory under the location specified in --workflow_destination . If this is not specified it will be checked out under ~/roses/ This step is optional: Set some useful environmental variables to access the CDDS directories: export CDDS_PROC_DIR = /<root_proc_dir>/<mip_era>/<mip>/<model_id>_<experiment_id>_<variant_label>/<package>/ export CDDS_DATA_DIR = /<root_data_dir>/<mip_era>/<mip>/<model_id>/<experiment_id>/<variant_label>/<package>/ ls $CDDS_PROC_DIR ls $CDDS_DATA_DIR where you must replace all values within <> . The root_proc_dir and root_data_dir are the values that has been specified in the request configuration. Run the workflow: cd <name for processing workflow> cylc vip . Info Cylc 8 is used for running the processing workflow. If your default version of cylc is not cylc 8 (run cylc --version to check) you will need to run the following command before running the workflow: export CYLC_VERSION = 8","title":"Checkout and configure the CDDS workflow"},{"location":"operational_procedure/gcmodeldev/#monitor-conversion-workflows","text":"For each stream a CDDS Convert workflow will be triggered by the processing workflow. Each of the workflows launched by CDDS Convert requires monitoring. This can be done using the command line tool cylc gui to obtain a window with an updating summary of workflows progress or equivalently the Cylc Review online tools. Conversion workflows will usually be named cdds_<model id>_<experiment id>_<variant_label>_<stream> and each stream will run completely independently. If a workflow has issues, due to task failure, it will stall, and you will receive an e-mail. If you hit issues or are unsure how to proceed update the CDDS progress ticket for your package with anything you believe is relevant (include the location of your working directory) and contact the CDDS Team for advice. The conversion workflows run the following steps run_extract_<stream> Extract Run CDDS Extract for this stream. Runs in long queue with a wall time of 2 days. If there are any issues with extracting data they will be reported in the job.err log file in the workflow and the $CDDS_PROC_DIR/extract/log/cdds_extract_<stream>_<date stamp>.log log file and the task will fail. The extraction task will automatically resubmit 4 times if it fails and manual intervention is required to proceed. Most issues are related to either MASS (i.e. moo commands failing), file system anomalies (failure to create files /directories) or running out of time. Identify issues either by searching for \"CRITICAL\" in the job.err logs in Cylc Review or by using grep CRITICAL $CDDS_PROC_DIR /extract/log/cdds_extract_<stream>_<date stamp>.log If the issue appears to be due to MASS issues you can re-run the failed CDDS Extract job by re-triggering the run_extract_<stream> task via the cylc gui or via the cylc command line tools: cylc trigger cdds_<model id>_<experiment id>_<variant label>_<stream> run_extract_<stream>:failed If in doubt update your CDDS progress ticket and contact CDDS Team for advice. validate_extract_<stream> Extract Validation Validation of the output is now performed as a separate task from extracting it. This task will report missing or unexpected files and unreadable netcdf files. setup_output_dir_<stream> Setup Output Directory This task will create output directories for conversion output. mip_convert_<stream>_<grid group> MIP Convert Run MIP Convert to produce output files for a small time window for this simulation. Will retry up to 3 times before workflow stalls. CRITICAL issues are appended to $CDDS_PROC_DIR/convert/log/critical_issues.log . These will likely need user action to correct for. So, update your CDDS progress ticket and contact CDDS Team for advice. The CRITICAL log file will not exist if there are no critical issues. A variant named mip_convert_first_<stream>_<grid group> may be launched to align the cycling dates with the concatenation processing. finaliser_<stream> MIP Convert Finaliser This ensures that concatenation tasks are launched once all MIP Convert tasks have been successfully performed for a particular time range. This step should never fail. organise_files_<stream> Organise Files Re-arranges the output files on disk from a directory structure created by the MIP Convert tasks of the form $CDDS_DATA_DIR /output/<stream>_mip_convert/<YYYY-MM-DD>/<grid>/<files> to $CDDS_DATA_DIR /output/<stream>_concat/<MIP table>/<variable name>/<files> Ready for concatenation. A variation named organise_files_final_<stream> does the same thing but at the end of the conversion process. mip_concatenate_setup_<stream> MIP Concatenate Setup This step constructs a list of concatenation jobs that must be performed mip_concatenate_batch_<stream> MIP Concatenate Batch Perform the concatenation commands ( ncrcat ) required to join small files together. Runs in long queue with a wall time of 2 days and can retry up to 3 times before workflow stalls (failures are usually due to running out of time while performing a concatenation). Only one mip_concatenate_batch_<stream> task can run at one time. Issues can be identified using: grep CRITICAL $CDDS_PROC_DIR /convert/log/mip_concatenate_*.log If any critical issues arise or tasks fail update your CDDS progress ticket and contact the CDDS Team for advice. Output data is written to $CDDS_DATA_DIR /output/<stream>/<MIP table>/<variable name>/<files> run_qc_<stream> Quality Check (QC) Run the QC process on output data for this stream Produces a report at: $CDDS_PROC_DIR /qualitycheck/report_<stream>_<datestamp>.json and a list of variables which pass the quality checks at: $CDDS_PROC_DIR /qualitycheck/approved_variables_<stream>_<datestamp>.txt and a log file at: $CDDS_PROC_DIR /qualitycheck/log/qc_run_and_report_<stream>_<datestamp>.log The approved variables file will have one line per successfully produced dataset of the form: <MIP table>/<variable name> ; <Directory containing files> This task will fail if any QC issues are found and will not resubmit. If this occurs please update your CDDS progress ticket and contact the CDDS Team for advice. run_transfer_<stream> Transfer Archive data for variables that are marked active in the requested variables file produced by CDDS Prepare and have successfully passed the QC checks, i.e. are listed in the approved variables file. Will not automatically retry, even if failure was due to MASS/MOOSE issues. The location in MASS to which these data are archived is determined by the output_mass_suffix argument specified in the request configuration file. Task will fail if There are MASS issues: For example if the following command returns anything there has been a MASS outage and you can re-trigger the task: grep SSC_STORAGE_SYSTEM_UNAVAILABLE $CDDS_PROC_DIR /archive/log/cdds_store_<stream>_<date stamp>.log An attempt is made to archive data that already exists in MASS. If this occurs please update your CDDS progress ticket and contact the CDDS Team for advice. VERY IMPORTANT Do not delete data from MASS without consultation with Matt Mizielinski . completion_<stream> Completion This is a dummy task that is the last thing to run in the workflow -- this is to allow inter workflow dependencies by allowing the CDDS workflow to monitor whether each per stream workflow has completed. If all goes well the workflow will complete, and you will receive an email confirming that the workflow has shutdown containing content of the form: Message: AUTOMATIC See: http://fcm1/cylc-review/taskjobs/<user id>/<workflow name>","title":"Monitor conversion workflows"},{"location":"operational_procedure/sim_review/","text":"Simulation Ticket Review Procedure After a package is processed by following the operational procedure , the user assigns the ticket to a member of the CDDS team to review the processing and then perform the final step to submit the data for publication. This document describes the steps that need to be performed in the review. The check before publishing steps should also be done by the user who processed the data to check that the processing has been successful. The final publication steps can only be performed by a member of the CDDS teams with the required permission. Checklist before publication Info All the following checks can be combined in a single command by running: . ~cdds/bin/setup_env_for_cdds <version> cdds_sim_review <location of request.cfg> Check the critical issues log in the critical_issues.log for each component. See the operational procedure for more details. Info You can use following command to check for critical logs: grep -irn \"critical\" $CDDS_PROC_DIR --exclude \"*.py\" --exclude \"*.svn*\" Check the QC report in $CDDS_PROC_DIR/qualitycheck/report_<date-time>.json . The aggregated_summary and details fields should be empty. Check approved variables list that expected variables have been added. Check that variables with recently reported issues are not in the approved variables list. Check permissions for proc/archive/log . It should have write permissions for everyone, so move_in_mass can work. Info You can check the premissions for proc/archive/log by running following command: ls -l $CDDS_PROC_DIR /transfer/ | grep log The permission should be drwxrwxrwx for the log directory. Look for any partial files left over from the concatenation process using the find command: find $CDDS_DATA_DIR /output/*_* -type f Submission for publication In order to complete the submission, you will need access to the els055 server. Only members of the CDDS team are expected to do this part. After completing the pre-publication checks, add a message to the ticket to confirm that the processing was successful. Log on to els055 . Info If you have not logged on before, please see the section below on setting up els055 . Run the move_in_mass command that was generated when running cdds_sim_review . Example move_in_mass request.json -p --original_state = embargoed --new_state = available --mass_location = production --variables_list_file = approved_variables_YYYY-MM-DDThhmmss.txt When move_in_mass has completed, check for errors in the log file: $CDDS_PROC_DIR/archive/log/move_in_mass_<date-time>.log Info Use grep to search for any CRITICAL messages in the log file: grep \"CRITICAL\" $CDDS_PROC_DIR /archive/log/move_in_mass_<date-time>.log The <date-time> must be replaced to the time stamp of the log file. Confirm that the last line includes the phrase Moving complete . Check the message queue to confirm that messages are waiting for processing. Info For CMIP6, you can use following command to confirm that shows the messages are waiting for processing: list_queue CMIP6_available For CMIP6Plus, you can use following command to confirm that shows the messages are waiting for processing: list_queue CMIP6Plus_available Once you are happy that the move_in_mass command has executed successfully, add a message stating this to the ticket, change the status of the ticket to approved and reassign to the original user for teardown and closing. Setting up els055 Create a new file ~/.cdds_credentials . Contact Matt Mizielinski for the required settings.","title":"Simulation Review Procedure"},{"location":"operational_procedure/sim_review/#simulation-ticket-review-procedure","text":"After a package is processed by following the operational procedure , the user assigns the ticket to a member of the CDDS team to review the processing and then perform the final step to submit the data for publication. This document describes the steps that need to be performed in the review. The check before publishing steps should also be done by the user who processed the data to check that the processing has been successful. The final publication steps can only be performed by a member of the CDDS teams with the required permission.","title":"Simulation Ticket Review Procedure"},{"location":"operational_procedure/sim_review/#checklist-before-publication","text":"Info All the following checks can be combined in a single command by running: . ~cdds/bin/setup_env_for_cdds <version> cdds_sim_review <location of request.cfg> Check the critical issues log in the critical_issues.log for each component. See the operational procedure for more details. Info You can use following command to check for critical logs: grep -irn \"critical\" $CDDS_PROC_DIR --exclude \"*.py\" --exclude \"*.svn*\" Check the QC report in $CDDS_PROC_DIR/qualitycheck/report_<date-time>.json . The aggregated_summary and details fields should be empty. Check approved variables list that expected variables have been added. Check that variables with recently reported issues are not in the approved variables list. Check permissions for proc/archive/log . It should have write permissions for everyone, so move_in_mass can work. Info You can check the premissions for proc/archive/log by running following command: ls -l $CDDS_PROC_DIR /transfer/ | grep log The permission should be drwxrwxrwx for the log directory. Look for any partial files left over from the concatenation process using the find command: find $CDDS_DATA_DIR /output/*_* -type f","title":"Checklist before publication"},{"location":"operational_procedure/sim_review/#submission-for-publication","text":"In order to complete the submission, you will need access to the els055 server. Only members of the CDDS team are expected to do this part. After completing the pre-publication checks, add a message to the ticket to confirm that the processing was successful. Log on to els055 . Info If you have not logged on before, please see the section below on setting up els055 . Run the move_in_mass command that was generated when running cdds_sim_review . Example move_in_mass request.json -p --original_state = embargoed --new_state = available --mass_location = production --variables_list_file = approved_variables_YYYY-MM-DDThhmmss.txt When move_in_mass has completed, check for errors in the log file: $CDDS_PROC_DIR/archive/log/move_in_mass_<date-time>.log Info Use grep to search for any CRITICAL messages in the log file: grep \"CRITICAL\" $CDDS_PROC_DIR /archive/log/move_in_mass_<date-time>.log The <date-time> must be replaced to the time stamp of the log file. Confirm that the last line includes the phrase Moving complete . Check the message queue to confirm that messages are waiting for processing. Info For CMIP6, you can use following command to confirm that shows the messages are waiting for processing: list_queue CMIP6_available For CMIP6Plus, you can use following command to confirm that shows the messages are waiting for processing: list_queue CMIP6Plus_available Once you are happy that the move_in_mass command has executed successfully, add a message stating this to the ticket, change the status of the ticket to approved and reassign to the original user for teardown and closing.","title":"Submission for publication"},{"location":"operational_procedure/sim_review/#setting-up-els055","text":"Create a new file ~/.cdds_credentials . Contact Matt Mizielinski for the required settings.","title":"Setting up els055"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL/","text":"GCModelDev HadGEM3 GC31 LL request file example CDDS v3.0.x In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-experiment-id institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = HadGEM3-GC31-LL model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bg466 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cdds_workflow_branch = trunk cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"GCModelDev HadGEM3-GC31-LL Request"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL/#gcmodeldev-hadgem3-gc31-ll-request-file-example","text":"","title":"GCModelDev HadGEM3 GC31 LL request file example"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL/#cdds-v30x","text":"In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-experiment-id institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = HadGEM3-GC31-LL model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bg466 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cdds_workflow_branch = trunk cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"CDDS v3.0.x"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL_envs/","text":"GCModelDev HadGEM3 GC31 LL request file example (ens MASS data class) CDDS v3.0.x In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-1pctCO2 institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r21i1p1f3 model_id = HadGEM3-GC31-LL model_type = AOGCM AER [netcdf_global_attributes] mass_ensemble_member = r011i1p1f1 [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = ens mass_ensemble_member = r011i1p1f1 start_date = 1950-01-01T00:00:00Z model_workflow_id = u-cp832 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cdds_workflow_branch = trunk cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"GCModelDev HadGEM3-GC31-LL ensemble class Request"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL_envs/#gcmodeldev-hadgem3-gc31-ll-request-file-example-ens-mass-data-class","text":"","title":"GCModelDev HadGEM3 GC31 LL request file example (ens MASS data class)"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-LL_envs/#cdds-v30x","text":"In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-1pctCO2 institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r21i1p1f3 model_id = HadGEM3-GC31-LL model_type = AOGCM AER [netcdf_global_attributes] mass_ensemble_member = r011i1p1f1 [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = ens mass_ensemble_member = r011i1p1f1 start_date = 1950-01-01T00:00:00Z model_workflow_id = u-cp832 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cdds_workflow_branch = trunk cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"CDDS v3.0.x"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-MM/","text":"GCModelDev HadGEM3 GC31 MM request file example CDDS v3.0.x In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-1pctCO2 institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-MM parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = HadGEM3-GC31-MM model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bg618 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = esting # This is appended to output_mass_root [misc] atmos_timestep = 900 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cdds_workflow_branch = trunk cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"GCModelDev HadGEM3-GC31-MM Request"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-MM/#gcmodeldev-hadgem3-gc31-mm-request-file-example","text":"","title":"GCModelDev HadGEM3 GC31 MM request file example"},{"location":"operational_procedure/gcmodeldev_examples/HadGEM3-GC31-MM/#cdds-v30x","text":"In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-1pctCO2 institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = HadGEM3-GC31-MM parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = HadGEM3-GC31-MM model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bg618 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = esting # This is appended to output_mass_root [misc] atmos_timestep = 900 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cdds_workflow_branch = trunk cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"CDDS v3.0.x"},{"location":"operational_procedure/gcmodeldev_examples/UKESM1-0-LL/","text":"GCModelDev UKESM1 0 LL request file example CDDS v3.0.x In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-experiment-id institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = UKESM1-0-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = UKESM1-0-LL model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bc179 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cdds_workflow_branch = trunk cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"GCModelDev UKESM1-0-LL Request"},{"location":"operational_procedure/gcmodeldev_examples/UKESM1-0-LL/#gcmodeldev-ukesm1-0-ll-request-file-example","text":"","title":"GCModelDev UKESM1 0 LL request file example"},{"location":"operational_procedure/gcmodeldev_examples/UKESM1-0-LL/#cdds-v30x","text":"In CDDS v3.0.0 the request file format was changed from json to ini . [metadata] branch_date_in_child = branch_date_in_parent = branch_method = no parent base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = my-experiment-id institution_id = MOHC license = GCModelDev model data is licensed under the Open Government License v3 (https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/) mip = MOHCCP mip_era = GCModelDev parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = parent_mip = parent_mip_era = parent_model_id = UKESM1-0-LL parent_time_units = days since 1850-01-01 parent_variant_label = sub_experiment_id = none variant_label = r1i1p1f3 model_id = UKESM1-0-LL model_type = AOGCM AER [netcdf_global_attributes] [common] external_plugin = external_plugin_location = mip_table_dir = $CDDS_ETC/mip_tables/GCModelDev/0.0.23 mode = relaxed package = round-1 workflow_basename = request_id root_proc_dir = $SCRATCH/cdds_proc # A reasonable default for Met Office, change for JASMIN root_data_dir = $SCRATCH/cdds_data # A reasonable default for Met Office, change for JASMIN root_ancil_dir = $CDDS_ETC/ancil/ root_hybrid_heights_dir = $CDDS_ETC/vertical_coordinates/ root_replacement_coordinates_dir = $CDDS_ETC/horizontal_coordinates/ sites_file = $CDDS_ETC/cfmip2/cfmip2-sites-orog.txt standard_names_version = latest standard_names_dir = $CDDS_ETC/standard_names/ simulation = False log_level = INFO [data] data_version = end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = start_date = 1950-01-01T00:00:00Z model_workflow_id = u-bc179 model_workflow_branch = trunk model_workflow_revision = not used except with data request streams = ap5 onm variable_list_file = <must be included> output_mass_root = moose:/adhoc/users/<moose user id> # Update with user id output_mass_suffix = testing # This is appended to output_mass_root [misc] atmos_timestep = 1200 # This is model dependent use_proc_dir = True no_overwrite = False [inventory] inventory_check = False inventory_database_location = [conversion] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cdds_workflow_branch = trunk cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = model_params_dir = continue_if_mip_convert_failed = False","title":"CDDS v3.0.x"},{"location":"reference/SUMMARY/","text":"","title":"API Documentation"},{"location":"tutorials/add_mapping/","text":"Mapping Hierarchy The mappings within MIP Convert are arranged in a hierarchy with names in the following forms common_mappings.cfg <mip_table>_mappings.cfg <base_model>_mappings.cfg <base_model>_<mip_table_id>_mappings.cfg <model_id>_mappings.cfg <model_id>_<mip_table_id>_mappings.cfg where <base_model> refers to the section of the model id before the first dash - , e.g. \"UKESM1\" or \"HadGEM3\" . Mappings in files furthest down this list overload those specified in files at the top. Model to MIP Mapping Configuration Files Each of the above model to MIP mapping configuration files contains the following sections. [DEFAULT] [COMMON] [variable name] The [DEFAULT] Section The [DEFAULT] section contains options that are propagated to all other sections. This is useful for setting a default value for a option for all sections. A value provided for the same option in any other section will be used for that section over the default value defined in the [DEFAULT] section. The [COMMON] Section The [COMMON] section contains options that may be used by other sections by using the syntax ${COMMON:<option>} . This is useful for setting values for comments or notes that would otherwise be repeated multiple times in the model to MIP mapping configuration files. The [variable name] Section(s) The [variable name] sections provide the model to MIP mapping corresponding to the specified MIP requested variable and contains the following required options: Required Options Description Notes dimension The dimensions of the MIP requested variable. expression The expression required to convert the input variable / input variables to the MIP requested variable. mip_table_id A space-separated list of MIP table identifiers that the model to MIP mapping is valid for. positive The direction of a vertical energy (heat) flux or surface momentum flux (stress) input; use 'up' or 'down' depending on whether the direction is positive when it is directed upward or downward, respectively. This argument is required for vertical energy and salt fluxes, for flux correction fields, and for surface stress. [1] status The status of the MIP requested variable. Valid values are ok and embargoed . 2 units The units of the data of the MIP requested variable i.e., after the expression has been applied. Notes This information is used by CMOR to determine whether a sign change is necessary to make the data consistent with the MIP requirements. For more information, please see the cmor_variable section in the CMOR Documentation. The MIP requested variables are reviewed to ensure they have been produced correctly, MIP requested variables that have not passed review will not be submitted to ESGF and so will not be available for other institutes to use. This is used by CDDS, but does not affect MIP Convert behaviour The following options are optional: Optional Options Description Notes comment The details relating to the model to MIP mapping that should be written to the output netCDF file, e.g., to qualify the details of the model to MIP mapping, add health warnings,etc. notes Any details relating to the model to MIP mapping that should not be written to the output netCDF file, e.g., who added the model to MIP mapping, why, reasons for using this model to MIP mapping over another in certain cases, any other special cases notes, etc. component A space-separated list of components that the model to MIP mapping is valid for. valid_min The minimum valid value for the data of the MIP requested variable; values in the data lower than this value are replaced with zero. Constructing an Expression Each input variable in an expression must contain one of the following: Expression Items File Type Description Notes stash PP LBUSER(4), STASH Code, see Chapter 4 (page 25) of UMDP F03 variable_name netCDF The name of the data variable in the model output files that is used to create the input variable. Example 1. One to One Mapping expression = m01s03i236 expression = sitemptop Example 2a. Constants and Arithmetic Expressions can use numerical values and constants (which must be written using upper case letters; constants are available in :mod: mip_convert.process.constants ): expression = rain_ai * 100. * SECONDS_IN_DAY Example 2b. Constants and Arithmetic For atmospheric tendency diagnostics, the atmospheric model timestep must be specified (the value of the atmospheric model timestep is obtained from the user configuration file, please see the request_section in the user_guide ): expression = m01s30i181 / ATMOS_TIMESTEP Example 3a. Constraints To specify additional constraints, use square brackets: expression = m01s08i223[blev=0.05] expression = pbo[cell_methods=time: mean (interval: 120 s)] Example 3b. Constraints Multiple values for a single constraint should be separated by spaces: expression = m01s30i201[blev=850.0 500.0 250.0] Example 3c. Constraints Multiple constraints within the square brackets should be separated by commas: expression = m01s02i204[lbplev=4, lbtim=122] Example 4. Processor In cases where it is not possible to describe the conversion of the input variable / input variables to the MIP requested variable using a basic expression like the ones above, a function can be specified: expression = my_function_name(m01s03i236) The values of the arguments of the function must follow the same syntax as the basic expression. The following constraints can currently be used in an expression : Expression Items File Type Description Notes blev PP BLEV, level, see Chapter 4 (page 26) of UMDP F03 cell_methods netCDF The cell methods depth netCDF Value of the depth coordinate lbplev PP LBUSER(5), pseudo level, see Chapter 4 (page 25) of UMDP F03 lbproc PP LBPROC, processing code, see Chapter 4 (page 21) of UMDP F03 lbtim PP LBTIM, time indicator, see Chapter 4 (page 17) of UMDP F03 lbtim_ia PP IA component of LBTIM (sampling frequency) How to Add a New Model to MIP mapping Note If you want to contribute your changes to CDDS then you must follow the development workflow. See the Developer Documentation pages for guidance. Requried Information the MIP requested variable name and the MIP table identifier. the constraints, i.e., the data to be read from the model output files to create the input variables. an expression describing how to process the input variables to produce a MIP output variable. the units of the MIP output variable after the expression has been applied. It is not necessary to include the units in model to MIP mappings used for netCDF model output files if the expression consists of a single constraint. the component, which is the domain as described \u200bhere. Determine which configuration file the model to MIP mapping should be added to Model to MIP mapping configuration files are located in the mip_convert/process sub package (their names end in _mappings.cfg ). if there is currently no entry for the MIP requested variable name in the model to MIP mapping configuration files (use, e.g. grep <mip_requested_variable_name> <branch>/mip_convert/mip_convert/process/*_mappings.cfg ), add the model to MIP mapping to common_mappings.cfg. if there is already an entry in common_mappings.cfg , add the model to MIP mapping to the appropriate ` _mappings.cfg`` configuration file. for multiple model to MIP mappings for the same MIP requested variable name, any expressions containing lbproc=128 should be added to common_mappings.cfg , while others should be added to the appropriate ` _mappings.cfg`` configuration file. if there are any issues, please ask Matthew Mizielinski. Add the model to MIP mapping to the appropriate configuration file. The sections in the model to MIP mapping configuration files are the MIP requested variable names and are listed in alphabetical order. If the expression continues beyond the 120 character line limit, add a new line before a binary operator, see \u200bPEP8. If the expression contains a function and an appropriate function does not already exist, add the function to mip_convert/process/processors.py \u200b. The function: can have any number of arguments, each corresponding to a cube. should work directly on the cube(s) (i.e., do not make a copy of the cube(s) or the data). should not update the standard_name or long_name of the cube(s) (the \u200bMIP table contains this information and will be automatically added to the output netCDF files by CMOR). must return a single cube. Test the Model to MIP Mapping Test and verify all mappings before submitting a pull request.","title":"Add a Variable Mapping"},{"location":"tutorials/add_mapping/#mapping-hierarchy","text":"The mappings within MIP Convert are arranged in a hierarchy with names in the following forms common_mappings.cfg <mip_table>_mappings.cfg <base_model>_mappings.cfg <base_model>_<mip_table_id>_mappings.cfg <model_id>_mappings.cfg <model_id>_<mip_table_id>_mappings.cfg where <base_model> refers to the section of the model id before the first dash - , e.g. \"UKESM1\" or \"HadGEM3\" . Mappings in files furthest down this list overload those specified in files at the top.","title":"Mapping Hierarchy"},{"location":"tutorials/add_mapping/#model-to-mip-mapping-configuration-files","text":"Each of the above model to MIP mapping configuration files contains the following sections. [DEFAULT] [COMMON] [variable name]","title":"Model to MIP Mapping Configuration Files"},{"location":"tutorials/add_mapping/#the-default-section","text":"The [DEFAULT] section contains options that are propagated to all other sections. This is useful for setting a default value for a option for all sections. A value provided for the same option in any other section will be used for that section over the default value defined in the [DEFAULT] section.","title":"The [DEFAULT] Section"},{"location":"tutorials/add_mapping/#the-common-section","text":"The [COMMON] section contains options that may be used by other sections by using the syntax ${COMMON:<option>} . This is useful for setting values for comments or notes that would otherwise be repeated multiple times in the model to MIP mapping configuration files.","title":"The [COMMON] Section"},{"location":"tutorials/add_mapping/#the-variable-name-sections","text":"The [variable name] sections provide the model to MIP mapping corresponding to the specified MIP requested variable and contains the following required options: Required Options Description Notes dimension The dimensions of the MIP requested variable. expression The expression required to convert the input variable / input variables to the MIP requested variable. mip_table_id A space-separated list of MIP table identifiers that the model to MIP mapping is valid for. positive The direction of a vertical energy (heat) flux or surface momentum flux (stress) input; use 'up' or 'down' depending on whether the direction is positive when it is directed upward or downward, respectively. This argument is required for vertical energy and salt fluxes, for flux correction fields, and for surface stress. [1] status The status of the MIP requested variable. Valid values are ok and embargoed . 2 units The units of the data of the MIP requested variable i.e., after the expression has been applied. Notes This information is used by CMOR to determine whether a sign change is necessary to make the data consistent with the MIP requirements. For more information, please see the cmor_variable section in the CMOR Documentation. The MIP requested variables are reviewed to ensure they have been produced correctly, MIP requested variables that have not passed review will not be submitted to ESGF and so will not be available for other institutes to use. This is used by CDDS, but does not affect MIP Convert behaviour The following options are optional: Optional Options Description Notes comment The details relating to the model to MIP mapping that should be written to the output netCDF file, e.g., to qualify the details of the model to MIP mapping, add health warnings,etc. notes Any details relating to the model to MIP mapping that should not be written to the output netCDF file, e.g., who added the model to MIP mapping, why, reasons for using this model to MIP mapping over another in certain cases, any other special cases notes, etc. component A space-separated list of components that the model to MIP mapping is valid for. valid_min The minimum valid value for the data of the MIP requested variable; values in the data lower than this value are replaced with zero.","title":"The [variable name] Section(s)"},{"location":"tutorials/add_mapping/#constructing-an-expression","text":"Each input variable in an expression must contain one of the following: Expression Items File Type Description Notes stash PP LBUSER(4), STASH Code, see Chapter 4 (page 25) of UMDP F03 variable_name netCDF The name of the data variable in the model output files that is used to create the input variable. Example 1. One to One Mapping expression = m01s03i236 expression = sitemptop Example 2a. Constants and Arithmetic Expressions can use numerical values and constants (which must be written using upper case letters; constants are available in :mod: mip_convert.process.constants ): expression = rain_ai * 100. * SECONDS_IN_DAY Example 2b. Constants and Arithmetic For atmospheric tendency diagnostics, the atmospheric model timestep must be specified (the value of the atmospheric model timestep is obtained from the user configuration file, please see the request_section in the user_guide ): expression = m01s30i181 / ATMOS_TIMESTEP Example 3a. Constraints To specify additional constraints, use square brackets: expression = m01s08i223[blev=0.05] expression = pbo[cell_methods=time: mean (interval: 120 s)] Example 3b. Constraints Multiple values for a single constraint should be separated by spaces: expression = m01s30i201[blev=850.0 500.0 250.0] Example 3c. Constraints Multiple constraints within the square brackets should be separated by commas: expression = m01s02i204[lbplev=4, lbtim=122] Example 4. Processor In cases where it is not possible to describe the conversion of the input variable / input variables to the MIP requested variable using a basic expression like the ones above, a function can be specified: expression = my_function_name(m01s03i236) The values of the arguments of the function must follow the same syntax as the basic expression. The following constraints can currently be used in an expression : Expression Items File Type Description Notes blev PP BLEV, level, see Chapter 4 (page 26) of UMDP F03 cell_methods netCDF The cell methods depth netCDF Value of the depth coordinate lbplev PP LBUSER(5), pseudo level, see Chapter 4 (page 25) of UMDP F03 lbproc PP LBPROC, processing code, see Chapter 4 (page 21) of UMDP F03 lbtim PP LBTIM, time indicator, see Chapter 4 (page 17) of UMDP F03 lbtim_ia PP IA component of LBTIM (sampling frequency)","title":"Constructing an Expression"},{"location":"tutorials/add_mapping/#how-to-add-a-new-model-to-mip-mapping","text":"Note If you want to contribute your changes to CDDS then you must follow the development workflow. See the Developer Documentation pages for guidance.","title":"How to Add a New Model to MIP mapping"},{"location":"tutorials/add_mapping/#requried-information","text":"the MIP requested variable name and the MIP table identifier. the constraints, i.e., the data to be read from the model output files to create the input variables. an expression describing how to process the input variables to produce a MIP output variable. the units of the MIP output variable after the expression has been applied. It is not necessary to include the units in model to MIP mappings used for netCDF model output files if the expression consists of a single constraint. the component, which is the domain as described \u200bhere.","title":"Requried Information"},{"location":"tutorials/add_mapping/#determine-which-configuration-file-the-model-to-mip-mapping-should-be-added-to","text":"Model to MIP mapping configuration files are located in the mip_convert/process sub package (their names end in _mappings.cfg ). if there is currently no entry for the MIP requested variable name in the model to MIP mapping configuration files (use, e.g. grep <mip_requested_variable_name> <branch>/mip_convert/mip_convert/process/*_mappings.cfg ), add the model to MIP mapping to common_mappings.cfg. if there is already an entry in common_mappings.cfg , add the model to MIP mapping to the appropriate ` _mappings.cfg`` configuration file. for multiple model to MIP mappings for the same MIP requested variable name, any expressions containing lbproc=128 should be added to common_mappings.cfg , while others should be added to the appropriate ` _mappings.cfg`` configuration file. if there are any issues, please ask Matthew Mizielinski.","title":"Determine which configuration file the model to MIP mapping should be added to"},{"location":"tutorials/add_mapping/#add-the-model-to-mip-mapping-to-the-appropriate-configuration-file","text":"The sections in the model to MIP mapping configuration files are the MIP requested variable names and are listed in alphabetical order. If the expression continues beyond the 120 character line limit, add a new line before a binary operator, see \u200bPEP8. If the expression contains a function and an appropriate function does not already exist, add the function to mip_convert/process/processors.py \u200b. The function: can have any number of arguments, each corresponding to a cube. should work directly on the cube(s) (i.e., do not make a copy of the cube(s) or the data). should not update the standard_name or long_name of the cube(s) (the \u200bMIP table contains this information and will be automatically added to the output netCDF files by CMOR). must return a single cube.","title":"Add the model to MIP mapping to the appropriate configuration file."},{"location":"tutorials/add_mapping/#test-the-model-to-mip-mapping","text":"Test and verify all mappings before submitting a pull request.","title":"Test the Model to MIP Mapping"},{"location":"tutorials/add_plugin/","text":"How to build your own plugin Warning This page is still work-in-progress! Warning The interfaces for the plugin can slightly change from version to version. Info There is a demo project that can use as reference: ~kschmatz/workspace/cdds-test-plugins Steps Create a plugin that extends from the CddsPlugin class Create classes that extends from the ModelParameters class Create classes for atmosphere grid and ocean grid that both extend from the GridInfo class Use your grid classes in your model parameters class Use your model parameters classes in your plugin Create your own grid labels Use your grid labels in your plugin Use your plugin with CDDS Preconditions Check that you, add the CDDS project (at least the cdds_common package) is in your Python path. Example To make sure that the CDDS project is in your Python path by using a script load_conda () { # load conda environment . ~cdds/software/miniconda3/bin/activate <cdds_conda_version> } setup_cdds_package () { # add cdds_common containing plugins implementation to PATH # Setup CDDS project local cdds_dir = <path-to-cdds-project> local cdds_package = \"cdds\" # Update PATH: if [ -d $cdds_dir / $cdds_package /bin ] ; then export PATH = $cdds_dir / $cdds_package /bin: $PATH fi # Update PYTHONPATH: if [ -d $cdds_dir / $cdds_package ] ; then export PYTHONPATH = $cdds_dir / $cdds_package : $PYTHONPATH fi } setup_project () { <setup-your-project> } load_conda setup_cdds_package setup_project Set <cdds_conda_version> to the conda version of the CDDS project you want to use (e.g. cdds-3.0_dev-0 ) Set the <path-to-cdds-project> to the path to the CDDS project. Replace <setup-your-project> with the commands you need to setup your project. Step 1: Create a plugin class Create a module <my>_plugin.py where <my> can be replaced with any name you like. A good choice would be the MIP era that the plugin should support. Import CddsPlugin from CDDS project: from cdds.common.plugins.plugins import CddsPlugin Create a plugin class that extends from the CddsPlugin class: class MyPlugin ( CddsPlugin ): Info If you use Pycharm, you have the benefit that you can move your cursor to the class name (here: MyPlugin ) and press ALT + Enter . You should now see an option to Implement abstract methods , select it. The methods you have to implement will be automatically add to your class. The same is with the imports that are needed. Add the __init__ method. This method must call the __init__ method of the super class with the parameter mip_era. The mip_era is the one that your plugin should support. To implement all methods you need a ModelParameters class, a GridLabel and a StreamInfo class. How to create this classes, will be explained in next sections. Example from typing import Type from cdds_common.cdds_plugins.grid import GridLabel from cdds_common.cdds_plugins.models import ModelParameters from cdds_common.cdds_plugins.plugins import CddsPlugin from cdds_common.cdds_plugins.streams import StreamInfo class MyPlugin ( CddsPlugin ): def __init__ ( self ): super ( MyPlugin , self ) . __init__ ( 'my_mip_era' ) def models_parameters ( self , model_id : str ) -> ModelParameters : pass def overload_models_parameters ( self , source_dir : str ) -> None : pass def grid_labels ( self ) -> Type [ GridLabel ]: pass def stream_info ( self ) -> StreamInfo : pass For the moment, we leave the plugin class like it is and take a look at the ModelParameters class and GridLabel class and StreamInfo class. We will come back to the plugin class later again. Step 2: Create the model parameters class You should create one model parameter class for each model you want to support. For demonstration, here we only show you to create one model parameter class. To create more classes, simple repeat the steps. Note You can use the same module or another one. In my demonstration project, I used another module to implement everything that is related to the model parameters. Import the ModelParameters class from the CDDS project: from cdds.common.plugins.models import ModelParameters Create a class that extends from the ModelParameters Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add default __init__ method. Example from typing import List , Dict from cdds_common.cdds_plugins.grid import GridType , GridInfo from cdds_common.cdds_plugins.models import ModelParameters class MyModelParams ( ModelParameters ): def __init__ ( self ): super ( MyModelParams , self ) . __init__ () @property def model_version ( self ) -> str : pass @property def data_request_version ( self ) -> str : pass @property def um_version ( self ) -> str : pass def grid_info ( self , grid_type : GridType ) -> GridInfo : pass def temp_space ( self , stream_id : str ) -> int : pass def memory ( self , stream_id : str ) -> str : pass def cycle_length ( self , stream_id : str ) -> str : pass def sizing_info ( self , frequency : str , shape : str ) -> float : pass def full_sizing_info ( self ) -> Dict [ str , Dict [ str , float ]]: pass def is_model ( self , model_id : str ) -> bool : pass def subdaily_streams ( self ) -> List [ str ]: pass Implement all method except the grid_info method. Next step is to implement the GridInfo classes. Step 3: Implement grid information classes Note You can use the same module or another one. In my demonstration project, I used another module to implement everything that is related to the grid information. Import the GridInfo class and GridType enum from the CDDS project: from cdds.common.plugins.grid import GridType , GridInfo For each grid type, there should be an own grid information class. So, create two classes that extends from the GridInfo - one for the atmosphere grid and one for the ocean grid. Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add the __init__ methods that call the super __init__ method and has a parameter the supported grid type of your grid information class. Implement the methods. Note Often the masks method is not implemented (simple: return None ) for atmosphere grids. Example class for atmosphere grid from typing import List , Dict from cdds.common.plugins.grid import GridType , GridInfo , OceanGridPolarMask class MyAtmosGridInfo ( GridInfo ): def __init__ ( self ): super ( MyAtmosGridInfo , self ) . __init__ ( GridType . ATMOS ) @property def model_info ( self ) -> str : pass @property def nominal_resolution ( self ) -> str : pass @property def longitude ( self ) -> int : pass @property def latitude ( self ) -> int : pass @property def v_latitude ( self ) -> int : pass @property def levels ( self ) -> int : pass @property def masks ( self ) -> Dict [ str , OceanGridPolarMask ]: pass @property def replacement_coordinates_file ( self ) -> str : pass def ancil_filenames ( self ) -> List [ str ]: pass def hybrid_heights_files ( self ) -> List [ str ]: pass Example class for ocean grid from typing import List , Dict from cdds.common.plugins.grid import GridType , GridInfo , OceanGridPolarMask class MyOceanGridInfo ( GridInfo ): def __init__ ( self ): super ( MyOceanGridInfo , self ) . __init__ ( GridType . OCEAN ) @property def model_info ( self ) -> str : pass @property def nominal_resolution ( self ) -> str : pass @property def longitude ( self ) -> int : pass @property def latitude ( self ) -> int : pass @property def v_latitude ( self ) -> int : pass @property def levels ( self ) -> int : pass @property def masks ( self ) -> Dict [ str , OceanGridPolarMask ]: pass @property def replacement_coordinates_file ( self ) -> str : pass def ancil_filenames ( self ) -> List [ str ]: pass def hybrid_heights_files ( self ) -> List [ str ]: git pass The next step is to use your grid information classes in your model parameters class. Step 4: Use your grid information classes in your model parameters class Implement the grid_info method in your model parameters class such that it returns the right grid information class according the grid type. Example def grid_info ( self , grid_type : GridType ) -> GridInfo : if grid_type == GridType . ATMOS : return MyAtmosGridInfo () elif grid_type == GridType . OCEAN : return MyOceanGridInfo () else : raise ValueError ( \"Unsupported grid type: {} \" . format ( GridType )) Step 5: Use your model parameters class in your plugin Go to your plugin class and implement the model_parameters method. This method should return the corresponding model parameters class according the model ID. Example def models_parameters ( self , model_id : str ) -> ModelParameters : model_params = { 'MyModel' : MyModelParams () } return model_params [ model_id ] If you allow overloading the defined parameters in your ModelParameters classes, then implement the overload_models_parameters method. The last to do for finishing the plugin is to implement the GridLabel Step 6: Implement the grid labels Import the GridLabel class from CDDS: from cdds.common.plugins.grid import GridLabel Create a class your own grid label enum class that extend from GridLabel with a __init__ method that as the arguments grid_name , label and extra_info Example class MyGridLabel ( GridLabel ): def __init__ ( self , grid_name : str , label : str , extra_info : bool ) -> None : self . grid_name = grid_name self . label = label self . extra_info = extra_info @classmethod def from_name ( cls , name : str ) -> 'GridLabel' : pass Add the grid label you want to support. Example Here, MyGridLabel should support NATIVE or NATIVE_ZONAL . class MyGridLabel ( GridLabel ): def __init__ ( self , grid_name : str , label : str , extra_info : bool ) -> None : self . grid_name = grid_name self . label = label self . extra_info = extra_info @classmethod def from_name ( cls , name : str ) -> 'GridLabel' : for grid_label in MyGridLabel : if grid_label . grid_name == name . lower (): return grid_label raise KeyError ( 'Not supported grid labels for {} ' . format ( name )) NATIVE = 'native' , 'gn' , False NATIVE_ZONAL = 'native-zonal' , 'gnz' , False Step 7: Use your grid labels in your plugin Implement the grid_labels in your plugin by simply returning your grid label enum class Example def grid_labels ( self ) -> Type [ GridLabel ]: return MyGridLabel Step 8: Implement the stream info class You should create a stream info class for all information around streams. Import the StreamInfo class from the CDDS project: from cdds.common.plugins.streams import StreamInfo Create a class that extends from the StreamInfo Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add __init__ method Implement all necesary methods Example class MyStreamInfo ( StreamInfo ): def __init__ ( self , config_path : str = '' ) -> None : super ( Cmip6StreamInfo , self ) . __init__ ( config_path ) def _load_streams ( self , configuration : Dict [ str , Any ]) -> None : pass def retrieve_stream_id ( self , variable : str , mip_table : str ) -> Tuple [ str , str ]: pass Step 9: Use your stream info in your plugin Use the stream_info in your plugin by simply returning your stream info Example def stream_info ( self ) -> StreamInfo : return MyStreamInfo () Congratulations, you finished to implement the plugin. Now, you can use it with CDDS. Step 10: Use your plugin with CDDS Add your implementation to the PYTHONPATH (and PATH ). Otherwise, CDDS will not be able to find it. Change the value of the external_plugin_location in the request.cfg file to the path to your plugin implementation Change the value of the external_plugin in the request.cfg file to the module path to your plugin Example: Change values for the ARISE plugin external_plugin_location = /project/cdds/arise external_plugin = arise.plugin","title":"Add a Plugin"},{"location":"tutorials/add_plugin/#how-to-build-your-own-plugin","text":"Warning This page is still work-in-progress! Warning The interfaces for the plugin can slightly change from version to version. Info There is a demo project that can use as reference: ~kschmatz/workspace/cdds-test-plugins","title":"How to build your own plugin"},{"location":"tutorials/add_plugin/#steps","text":"Create a plugin that extends from the CddsPlugin class Create classes that extends from the ModelParameters class Create classes for atmosphere grid and ocean grid that both extend from the GridInfo class Use your grid classes in your model parameters class Use your model parameters classes in your plugin Create your own grid labels Use your grid labels in your plugin Use your plugin with CDDS","title":"Steps"},{"location":"tutorials/add_plugin/#preconditions","text":"Check that you, add the CDDS project (at least the cdds_common package) is in your Python path. Example To make sure that the CDDS project is in your Python path by using a script load_conda () { # load conda environment . ~cdds/software/miniconda3/bin/activate <cdds_conda_version> } setup_cdds_package () { # add cdds_common containing plugins implementation to PATH # Setup CDDS project local cdds_dir = <path-to-cdds-project> local cdds_package = \"cdds\" # Update PATH: if [ -d $cdds_dir / $cdds_package /bin ] ; then export PATH = $cdds_dir / $cdds_package /bin: $PATH fi # Update PYTHONPATH: if [ -d $cdds_dir / $cdds_package ] ; then export PYTHONPATH = $cdds_dir / $cdds_package : $PYTHONPATH fi } setup_project () { <setup-your-project> } load_conda setup_cdds_package setup_project Set <cdds_conda_version> to the conda version of the CDDS project you want to use (e.g. cdds-3.0_dev-0 ) Set the <path-to-cdds-project> to the path to the CDDS project. Replace <setup-your-project> with the commands you need to setup your project.","title":"Preconditions"},{"location":"tutorials/add_plugin/#step-1-create-a-plugin-class","text":"Create a module <my>_plugin.py where <my> can be replaced with any name you like. A good choice would be the MIP era that the plugin should support. Import CddsPlugin from CDDS project: from cdds.common.plugins.plugins import CddsPlugin Create a plugin class that extends from the CddsPlugin class: class MyPlugin ( CddsPlugin ): Info If you use Pycharm, you have the benefit that you can move your cursor to the class name (here: MyPlugin ) and press ALT + Enter . You should now see an option to Implement abstract methods , select it. The methods you have to implement will be automatically add to your class. The same is with the imports that are needed. Add the __init__ method. This method must call the __init__ method of the super class with the parameter mip_era. The mip_era is the one that your plugin should support. To implement all methods you need a ModelParameters class, a GridLabel and a StreamInfo class. How to create this classes, will be explained in next sections. Example from typing import Type from cdds_common.cdds_plugins.grid import GridLabel from cdds_common.cdds_plugins.models import ModelParameters from cdds_common.cdds_plugins.plugins import CddsPlugin from cdds_common.cdds_plugins.streams import StreamInfo class MyPlugin ( CddsPlugin ): def __init__ ( self ): super ( MyPlugin , self ) . __init__ ( 'my_mip_era' ) def models_parameters ( self , model_id : str ) -> ModelParameters : pass def overload_models_parameters ( self , source_dir : str ) -> None : pass def grid_labels ( self ) -> Type [ GridLabel ]: pass def stream_info ( self ) -> StreamInfo : pass For the moment, we leave the plugin class like it is and take a look at the ModelParameters class and GridLabel class and StreamInfo class. We will come back to the plugin class later again.","title":"Step 1: Create a plugin class"},{"location":"tutorials/add_plugin/#step-2-create-the-model-parameters-class","text":"You should create one model parameter class for each model you want to support. For demonstration, here we only show you to create one model parameter class. To create more classes, simple repeat the steps. Note You can use the same module or another one. In my demonstration project, I used another module to implement everything that is related to the model parameters. Import the ModelParameters class from the CDDS project: from cdds.common.plugins.models import ModelParameters Create a class that extends from the ModelParameters Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add default __init__ method. Example from typing import List , Dict from cdds_common.cdds_plugins.grid import GridType , GridInfo from cdds_common.cdds_plugins.models import ModelParameters class MyModelParams ( ModelParameters ): def __init__ ( self ): super ( MyModelParams , self ) . __init__ () @property def model_version ( self ) -> str : pass @property def data_request_version ( self ) -> str : pass @property def um_version ( self ) -> str : pass def grid_info ( self , grid_type : GridType ) -> GridInfo : pass def temp_space ( self , stream_id : str ) -> int : pass def memory ( self , stream_id : str ) -> str : pass def cycle_length ( self , stream_id : str ) -> str : pass def sizing_info ( self , frequency : str , shape : str ) -> float : pass def full_sizing_info ( self ) -> Dict [ str , Dict [ str , float ]]: pass def is_model ( self , model_id : str ) -> bool : pass def subdaily_streams ( self ) -> List [ str ]: pass Implement all method except the grid_info method. Next step is to implement the GridInfo classes.","title":"Step 2: Create the model parameters class"},{"location":"tutorials/add_plugin/#step-3-implement-grid-information-classes","text":"Note You can use the same module or another one. In my demonstration project, I used another module to implement everything that is related to the grid information. Import the GridInfo class and GridType enum from the CDDS project: from cdds.common.plugins.grid import GridType , GridInfo For each grid type, there should be an own grid information class. So, create two classes that extends from the GridInfo - one for the atmosphere grid and one for the ocean grid. Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add the __init__ methods that call the super __init__ method and has a parameter the supported grid type of your grid information class. Implement the methods. Note Often the masks method is not implemented (simple: return None ) for atmosphere grids. Example class for atmosphere grid from typing import List , Dict from cdds.common.plugins.grid import GridType , GridInfo , OceanGridPolarMask class MyAtmosGridInfo ( GridInfo ): def __init__ ( self ): super ( MyAtmosGridInfo , self ) . __init__ ( GridType . ATMOS ) @property def model_info ( self ) -> str : pass @property def nominal_resolution ( self ) -> str : pass @property def longitude ( self ) -> int : pass @property def latitude ( self ) -> int : pass @property def v_latitude ( self ) -> int : pass @property def levels ( self ) -> int : pass @property def masks ( self ) -> Dict [ str , OceanGridPolarMask ]: pass @property def replacement_coordinates_file ( self ) -> str : pass def ancil_filenames ( self ) -> List [ str ]: pass def hybrid_heights_files ( self ) -> List [ str ]: pass Example class for ocean grid from typing import List , Dict from cdds.common.plugins.grid import GridType , GridInfo , OceanGridPolarMask class MyOceanGridInfo ( GridInfo ): def __init__ ( self ): super ( MyOceanGridInfo , self ) . __init__ ( GridType . OCEAN ) @property def model_info ( self ) -> str : pass @property def nominal_resolution ( self ) -> str : pass @property def longitude ( self ) -> int : pass @property def latitude ( self ) -> int : pass @property def v_latitude ( self ) -> int : pass @property def levels ( self ) -> int : pass @property def masks ( self ) -> Dict [ str , OceanGridPolarMask ]: pass @property def replacement_coordinates_file ( self ) -> str : pass def ancil_filenames ( self ) -> List [ str ]: pass def hybrid_heights_files ( self ) -> List [ str ]: git pass The next step is to use your grid information classes in your model parameters class.","title":"Step 3: Implement grid information classes"},{"location":"tutorials/add_plugin/#step-4-use-your-grid-information-classes-in-your-model-parameters-class","text":"Implement the grid_info method in your model parameters class such that it returns the right grid information class according the grid type. Example def grid_info ( self , grid_type : GridType ) -> GridInfo : if grid_type == GridType . ATMOS : return MyAtmosGridInfo () elif grid_type == GridType . OCEAN : return MyOceanGridInfo () else : raise ValueError ( \"Unsupported grid type: {} \" . format ( GridType ))","title":"Step 4: Use your grid information classes in your model parameters class"},{"location":"tutorials/add_plugin/#step-5-use-your-model-parameters-class-in-your-plugin","text":"Go to your plugin class and implement the model_parameters method. This method should return the corresponding model parameters class according the model ID. Example def models_parameters ( self , model_id : str ) -> ModelParameters : model_params = { 'MyModel' : MyModelParams () } return model_params [ model_id ] If you allow overloading the defined parameters in your ModelParameters classes, then implement the overload_models_parameters method. The last to do for finishing the plugin is to implement the GridLabel","title":"Step 5: Use your model parameters class in your plugin"},{"location":"tutorials/add_plugin/#step-6-implement-the-grid-labels","text":"Import the GridLabel class from CDDS: from cdds.common.plugins.grid import GridLabel Create a class your own grid label enum class that extend from GridLabel with a __init__ method that as the arguments grid_name , label and extra_info Example class MyGridLabel ( GridLabel ): def __init__ ( self , grid_name : str , label : str , extra_info : bool ) -> None : self . grid_name = grid_name self . label = label self . extra_info = extra_info @classmethod def from_name ( cls , name : str ) -> 'GridLabel' : pass Add the grid label you want to support. Example Here, MyGridLabel should support NATIVE or NATIVE_ZONAL . class MyGridLabel ( GridLabel ): def __init__ ( self , grid_name : str , label : str , extra_info : bool ) -> None : self . grid_name = grid_name self . label = label self . extra_info = extra_info @classmethod def from_name ( cls , name : str ) -> 'GridLabel' : for grid_label in MyGridLabel : if grid_label . grid_name == name . lower (): return grid_label raise KeyError ( 'Not supported grid labels for {} ' . format ( name )) NATIVE = 'native' , 'gn' , False NATIVE_ZONAL = 'native-zonal' , 'gnz' , False","title":"Step 6: Implement the grid labels"},{"location":"tutorials/add_plugin/#step-7-use-your-grid-labels-in-your-plugin","text":"Implement the grid_labels in your plugin by simply returning your grid label enum class Example def grid_labels ( self ) -> Type [ GridLabel ]: return MyGridLabel","title":"Step 7: Use your grid labels in your plugin"},{"location":"tutorials/add_plugin/#step-8-implement-the-stream-info-class","text":"You should create a stream info class for all information around streams. Import the StreamInfo class from the CDDS project: from cdds.common.plugins.streams import StreamInfo Create a class that extends from the StreamInfo Add the abstract methods that must be implemented (Pycharm can help you with that see in the First Step) Add __init__ method Implement all necesary methods Example class MyStreamInfo ( StreamInfo ): def __init__ ( self , config_path : str = '' ) -> None : super ( Cmip6StreamInfo , self ) . __init__ ( config_path ) def _load_streams ( self , configuration : Dict [ str , Any ]) -> None : pass def retrieve_stream_id ( self , variable : str , mip_table : str ) -> Tuple [ str , str ]: pass","title":"Step 8: Implement the stream info class"},{"location":"tutorials/add_plugin/#step-9-use-your-stream-info-in-your-plugin","text":"Use the stream_info in your plugin by simply returning your stream info Example def stream_info ( self ) -> StreamInfo : return MyStreamInfo () Congratulations, you finished to implement the plugin. Now, you can use it with CDDS.","title":"Step 9: Use your stream info in your plugin"},{"location":"tutorials/add_plugin/#step-10-use-your-plugin-with-cdds","text":"Add your implementation to the PYTHONPATH (and PATH ). Otherwise, CDDS will not be able to find it. Change the value of the external_plugin_location in the request.cfg file to the path to your plugin implementation Change the value of the external_plugin in the request.cfg file to the module path to your plugin Example: Change values for the ARISE plugin external_plugin_location = /project/cdds/arise external_plugin = arise.plugin","title":"Step 10: Use your plugin with CDDS"},{"location":"tutorials/json_model_parameter/","text":"Update Model Parameters JSON File Stream File Frequencies The stream_file_frequency section contains a key value pairs where the key the frequency is and the value the list of streams. Following frequencies are supported: hourly , daily , 10 day , quarterly , monthly Example Assuming ap4 and ap5 are monthly streams and ap6 is a daily stream: \"stream_file_frequency\" : { \"monthly\" : [ \"ap4\" , \"ap5\" ], \"daily\" : [ \"ap6\" ] } Cylc length / Memory / Temporary space There are following three sections that have an entry for each stream: cylc_length lengths of the suite cylc for each stream Example \"cycle_length\" : { \"ap4\" : \"P5Y\" , \"ap5\" : \"P5Y\" , \"ap6\" : \"P1Y\" } memory memory usages of each stream Note This is the starting point for memory and adaptive memory, storage and time limits are used. Example \"memory\" : { \"ap4\" : \"12G\" , \"ap5\" : \"8G\" , \"ap6\" : \"30G\" } temp_space temporary space of each stream Note This is the starting point for memory and adaptive memory, storage and time limits are used. Example \"temp_space\" : { \"ap4\" : 98304 , \"ap5\" : 40960 , \"ap6\" : 98304 } Sizing information The entry sizing_info represents the sizing information and has JSON objects represent the information as value. Each JSON object has as key the frequency and as value the shape with its coordinates. Note The frequency is the global attribute from the files being concatenated. Example \"sizing_info\" : { \"mon\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 }, \"monPt\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 } } Sub daily streams There are one entry for the subdaily streams subdaily_streams that contains an array of subdaily streams. Example \"subdaily_streams\" : [ \"ap6\" , \"ap7\" , \"ap8\" , \"ap9\" , \"apt\" ] Grid information There are two supported grids - atmosphere and ocean. The entries are atoms and ocean . Atmosphere Grid Information Following information should be provided: atmos_timestep the atmosphere timestep. The timestep is often different for different model resolutions. model_info a simple description of the grid, e.g. N96 grid . It is interpolated into grid attribute . nominal_resolution the nominal resolution. This needs to agree with the Controlled Vocabulary for the project. longitude the number of longitude points in the atmosphere grid latitude the number of latitude points in the atmosphere grid (T points and U points) v_latitude The number of latitude points on the atmosphere grid for data on V points levels the number of vertical levels ancil_filenames the ancillary file names. These are looked for in the directory <root_ancil_dir>/<source_id> as specified in the request config file hybrid_heights_files the hybrid heights files Example \"atmos\" : { \"atmos_timestep\" : 1200 , \"model_info\" : \"N96 grid\" , \"nominal_resolution\" : \"250 km\" , \"longitude\" : 192 , \"latitude\" : 144 , \"v_latitude\" : 145 , \"levels\" : 85 , \"ancil_filenames\" : [ \"qrparm.landfrac.pp\" , \"qrparm.soil.pp\" ], \"hybrid_heights_files\" : [ \"atmosphere_theta_levels_85.txt\" , \"atmosphere_rho_levels_86.txt\" ] } Ocean Grid Information Following information should be provided: model_info a simple description of the grid, e.g. N96 grid . It is interpolated into grid attribute . nominal_resolution the nominal resolution. This needs to agree with the Controlled Vocabulary for the project. longitude the number of longitude points in the ocean grid latitude the number of latitude points in the ocean grid (T points and U points) v_latitude the number of latitude points on the ocean grid for data on V points levels the number of vertical levels ancil_filenames the ancillary file names. These are looked for in the directory <root_ancil_dir>/<source_id> as specified in the request config file replacement_coordinates_file the replacement coordinates file for CICE model output hybrid_heights_files the hybrid heights files. Note This is not relevant for the ocean, but still needs to be present. masked a json object of ocean grid polar masks for the grid. Each masked entry as a grid label and its masked value split by the slice_latitude and slice_longitude . These are used to mask duplicate cells along the polar rows. Example \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] } } halo_options the ncks options needed to move ocean halo rows and columns. Each option are given by grid label. They are used when extracting data from MASS. Example \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } Example \"ocean\" : { \"model_info\" : \"eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics\" , \"nominal_resolution\" : \"100 km\" , \"longitude\" : 360 , \"latitude\" : 330 , \"levels\" : 75 , \"replacement_coordinates_file\" : \"cice_eORCA1_coords.nc\" , \"ancil_filenames\" : [ \"ocean_constants.nc\" , \"ocean_byte_masks.nc\" ], \"hybrid_heights_files\" : [], \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] }, \"cice-U\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] } }, \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } } Example Example { \"stream_file_frequency\" : { \"monthly\" : [ \"ap4\" , \"ap5\" ], \"10 day\" : [ \"ap6\" ] }, \"cycle_length\" : { \"ap4\" : \"P5Y\" , \"ap5\" : \"P5Y\" , \"ap6\" : \"P1Y\" }, \"memory\" : { \"ap4\" : \"12G\" , \"ap5\" : \"8G\" , \"ap6\" : \"30G\" }, \"temp_space\" : { \"ap4\" : 98304 , \"ap5\" : 40960 , \"ap6\" : 98304 }, \"sizing_info\" : { \"mon\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 }, \"day\" : { \"default\" : 20 , \"144-192\" : 100 , \"19-144-192\" : 20 }, }, \"subdaily_streams\" : [ \"ap6\" ], \"grid_info\" : { \"atmos\" : { \"atmos_timestep\" : 1200 , \"model_info\" : \"N96 grid\" , \"nominal_resolution\" : \"250 km\" , \"longitude\" : 192 , \"latitude\" : 144 , \"v_latitude\" : 145 , \"levels\" : 85 , \"ancil_filenames\" : [ \"qrparm.landfrac.pp\" , \"qrparm.soil.pp\" ], \"hybrid_heights_files\" : [ \"atmosphere_theta_levels_85.txt\" , \"atmosphere_rho_levels_86.txt\" ] }, \"ocean\" : { \"model_info\" : \"eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics\" , \"nominal_resolution\" : \"100 km\" , \"longitude\" : 360 , \"latitude\" : 330 , \"levels\" : 75 , \"replacement_coordinates_file\" : \"cice_eORCA1_coords.nc\" , \"ancil_filenames\" : [ \"ocean_constants.nc\" , \"ocean_byte_masks.nc\" , \"ocean_basin.nc\" , \"diaptr_basin_masks.nc\" , \"ocean_zostoga.nc\" ], \"hybrid_heights_files\" : [], \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] }, \"cice-U\" : { \"slice_latitude\" : [ -1 , null , null ] } }, \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } } } } Check Model Parameters JSON File There is a little tool, that checks the basic of your model parameters file: validate_model_params <request file> where the <request file> is the path to your request that uses your model parameters JSON file.","title":"Model Parameters JSON File"},{"location":"tutorials/json_model_parameter/#update-model-parameters-json-file","text":"","title":"Update Model Parameters JSON File"},{"location":"tutorials/json_model_parameter/#stream-file-frequencies","text":"The stream_file_frequency section contains a key value pairs where the key the frequency is and the value the list of streams. Following frequencies are supported: hourly , daily , 10 day , quarterly , monthly Example Assuming ap4 and ap5 are monthly streams and ap6 is a daily stream: \"stream_file_frequency\" : { \"monthly\" : [ \"ap4\" , \"ap5\" ], \"daily\" : [ \"ap6\" ] }","title":"Stream File Frequencies"},{"location":"tutorials/json_model_parameter/#cylc-length-memory-temporary-space","text":"There are following three sections that have an entry for each stream: cylc_length lengths of the suite cylc for each stream Example \"cycle_length\" : { \"ap4\" : \"P5Y\" , \"ap5\" : \"P5Y\" , \"ap6\" : \"P1Y\" } memory memory usages of each stream Note This is the starting point for memory and adaptive memory, storage and time limits are used. Example \"memory\" : { \"ap4\" : \"12G\" , \"ap5\" : \"8G\" , \"ap6\" : \"30G\" } temp_space temporary space of each stream Note This is the starting point for memory and adaptive memory, storage and time limits are used. Example \"temp_space\" : { \"ap4\" : 98304 , \"ap5\" : 40960 , \"ap6\" : 98304 }","title":"Cylc length / Memory / Temporary space"},{"location":"tutorials/json_model_parameter/#sizing-information","text":"The entry sizing_info represents the sizing information and has JSON objects represent the information as value. Each JSON object has as key the frequency and as value the shape with its coordinates. Note The frequency is the global attribute from the files being concatenated. Example \"sizing_info\" : { \"mon\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 }, \"monPt\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 } }","title":"Sizing information"},{"location":"tutorials/json_model_parameter/#sub-daily-streams","text":"There are one entry for the subdaily streams subdaily_streams that contains an array of subdaily streams. Example \"subdaily_streams\" : [ \"ap6\" , \"ap7\" , \"ap8\" , \"ap9\" , \"apt\" ]","title":"Sub daily streams"},{"location":"tutorials/json_model_parameter/#grid-information","text":"There are two supported grids - atmosphere and ocean. The entries are atoms and ocean .","title":"Grid information"},{"location":"tutorials/json_model_parameter/#atmosphere-grid-information","text":"Following information should be provided: atmos_timestep the atmosphere timestep. The timestep is often different for different model resolutions. model_info a simple description of the grid, e.g. N96 grid . It is interpolated into grid attribute . nominal_resolution the nominal resolution. This needs to agree with the Controlled Vocabulary for the project. longitude the number of longitude points in the atmosphere grid latitude the number of latitude points in the atmosphere grid (T points and U points) v_latitude The number of latitude points on the atmosphere grid for data on V points levels the number of vertical levels ancil_filenames the ancillary file names. These are looked for in the directory <root_ancil_dir>/<source_id> as specified in the request config file hybrid_heights_files the hybrid heights files Example \"atmos\" : { \"atmos_timestep\" : 1200 , \"model_info\" : \"N96 grid\" , \"nominal_resolution\" : \"250 km\" , \"longitude\" : 192 , \"latitude\" : 144 , \"v_latitude\" : 145 , \"levels\" : 85 , \"ancil_filenames\" : [ \"qrparm.landfrac.pp\" , \"qrparm.soil.pp\" ], \"hybrid_heights_files\" : [ \"atmosphere_theta_levels_85.txt\" , \"atmosphere_rho_levels_86.txt\" ] }","title":"Atmosphere Grid Information"},{"location":"tutorials/json_model_parameter/#ocean-grid-information","text":"Following information should be provided: model_info a simple description of the grid, e.g. N96 grid . It is interpolated into grid attribute . nominal_resolution the nominal resolution. This needs to agree with the Controlled Vocabulary for the project. longitude the number of longitude points in the ocean grid latitude the number of latitude points in the ocean grid (T points and U points) v_latitude the number of latitude points on the ocean grid for data on V points levels the number of vertical levels ancil_filenames the ancillary file names. These are looked for in the directory <root_ancil_dir>/<source_id> as specified in the request config file replacement_coordinates_file the replacement coordinates file for CICE model output hybrid_heights_files the hybrid heights files. Note This is not relevant for the ocean, but still needs to be present. masked a json object of ocean grid polar masks for the grid. Each masked entry as a grid label and its masked value split by the slice_latitude and slice_longitude . These are used to mask duplicate cells along the polar rows. Example \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] } } halo_options the ncks options needed to move ocean halo rows and columns. Each option are given by grid label. They are used when extracting data from MASS. Example \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } Example \"ocean\" : { \"model_info\" : \"eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics\" , \"nominal_resolution\" : \"100 km\" , \"longitude\" : 360 , \"latitude\" : 330 , \"levels\" : 75 , \"replacement_coordinates_file\" : \"cice_eORCA1_coords.nc\" , \"ancil_filenames\" : [ \"ocean_constants.nc\" , \"ocean_byte_masks.nc\" ], \"hybrid_heights_files\" : [], \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] }, \"cice-U\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] } }, \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } }","title":"Ocean Grid Information"},{"location":"tutorials/json_model_parameter/#example","text":"Example { \"stream_file_frequency\" : { \"monthly\" : [ \"ap4\" , \"ap5\" ], \"10 day\" : [ \"ap6\" ] }, \"cycle_length\" : { \"ap4\" : \"P5Y\" , \"ap5\" : \"P5Y\" , \"ap6\" : \"P1Y\" }, \"memory\" : { \"ap4\" : \"12G\" , \"ap5\" : \"8G\" , \"ap6\" : \"30G\" }, \"temp_space\" : { \"ap4\" : 98304 , \"ap5\" : 40960 , \"ap6\" : 98304 }, \"sizing_info\" : { \"mon\" : { \"default\" : 100 , \"85-144-192\" : 50 , \"85-145-192\" : 50 , \"86-144-192\" : 50 , \"75-330-360\" : 50 }, \"day\" : { \"default\" : 20 , \"144-192\" : 100 , \"19-144-192\" : 20 }, }, \"subdaily_streams\" : [ \"ap6\" ], \"grid_info\" : { \"atmos\" : { \"atmos_timestep\" : 1200 , \"model_info\" : \"N96 grid\" , \"nominal_resolution\" : \"250 km\" , \"longitude\" : 192 , \"latitude\" : 144 , \"v_latitude\" : 145 , \"levels\" : 85 , \"ancil_filenames\" : [ \"qrparm.landfrac.pp\" , \"qrparm.soil.pp\" ], \"hybrid_heights_files\" : [ \"atmosphere_theta_levels_85.txt\" , \"atmosphere_rho_levels_86.txt\" ] }, \"ocean\" : { \"model_info\" : \"eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics\" , \"nominal_resolution\" : \"100 km\" , \"longitude\" : 360 , \"latitude\" : 330 , \"levels\" : 75 , \"replacement_coordinates_file\" : \"cice_eORCA1_coords.nc\" , \"ancil_filenames\" : [ \"ocean_constants.nc\" , \"ocean_byte_masks.nc\" , \"ocean_basin.nc\" , \"diaptr_basin_masks.nc\" , \"ocean_zostoga.nc\" ], \"hybrid_heights_files\" : [], \"masked\" : { \"grid-V\" : { \"slice_latitude\" : [ -1 , null , null ], \"slice_longitude\" : [ 180 , null , null ] }, \"cice-U\" : { \"slice_latitude\" : [ -1 , null , null ] } }, \"halo_options\" : { \"grid-T\" : [ \"-dx,1,360\" , \"-dy,1,330\" ], \"grid-U\" : [ \"-dx,1,360\" , \"-dy,1,330\" ] } } } }","title":"Example"},{"location":"tutorials/json_model_parameter/#check-model-parameters-json-file","text":"There is a little tool, that checks the basic of your model parameters file: validate_model_params <request file> where the <request file> is the path to your request that uses your model parameters JSON file.","title":"Check Model Parameters JSON File"},{"location":"tutorials/mip_convert/","text":"Overview The mip_convert package enables a user to produce the output netCDF files for a MIP using model output files. graph LR A[model output .pp data] --> C[MIP Convert + CMOR]; B[model output .nc data] --> C; C --> D[CF/CMIP6 Compliant .nc data]; The user makes requests for one or more MIP requested variables by providing specific information (including the appropriate MIP requested variable names) in the user configuration file. The information required to produce the MIP requested variables is gathered from the user configuration file, the model to MIP mapping configuration files and the appropriate MIP table, in that order. The following steps are then performed for each MIP requested variable name in the user configuration file to produce the output netCDF files: load the relevant data from the model output files into one or more input variables depending on whether there is a one-to-one / simple arithmetic relationship between the MIP output variable and the input variables or if the MIP output variable is based on an arithmetic combination of two or more input variables, respectively, using Iris and the information provided in the user configuration file and the model to MIP mapping configuration files. process the input variable / input variables to produce the MIP output variable using the information provided in the model to MIP mapping configuration files. save the MIP output variable to an output netCDF file using CMOR and the information provided in the user configuration file and the appropriate MIP table. Recommended Reading The Design Considerations and Overview section in the CMOR Documentation. Quick Start Guide Download the template user configuration file mip_convert.cfg . Make the appropriate edits to the template user configuration file using the information provided in the \"User Configuration File\" section and the specified sections in the CMOR Documentation . Source an environment with cdds and verify that mip_convert runs. mip_convert -h Produce the output netCDF files by running mip_convert and passing in the modified user configuration file as an argument. mip_convert mip_convert.cfg Check the exit code echo $? Exit Code Meaning 0 No errors were raised during processing. 1 An exception was raised and no MIP requested variables were produced. 2 One or more MIP requested variables were produced but not all variables were produced. See the CRITICAL messages in the log for further information about the MIP requested variables not produced. Check that the output netCDF files are as expected. For help or to report an issue, please see support . Selected MIP Convert Arguments Argument Description config_file The name of the user configuration file. For more information, please see the MIP Convert user guide -s or --stream_identifiers The stream identifiers to process. If all streams should be processed, do not specify this option. --relaxed-cmor If specified, CMIP6 style validation is not performed by CMOR. If the validation is run then the following fields are not checked; model_id ( source_id ), experiment_id , further_info_url , grid_label , parent_experiment_id , sub_experiment_id . --mip_era The MIP era (e.g. CMIP6). --external_plugin Module path to external CDDS plugin (e.g. arise.plugin ) --external_plugin_location Path to the external plugin implementation (e.g. /project/cdds/arise ) Example Usage Run for all streams with full checking of metadata mip_convert mip_convert.cfg Run for a single stream in relaxed mode mip_convert mip_convert.cfg -s ap4 --relaxed_cmor User Configuration File Reference The user configuration file provides the information required by MIP Convert to produce the output netCDF files. It contains the following sections, some of which are optional. Section Summary [COMMON] Convenience for setting up shared config values. [cmor_setup] Passed through to CMOR's cmor_setup() routine. [cmor_dataset] Passed through to CMOR's cmor_data_set_json() routine. [request] Configure mip_convert including input data. [stream_<stream_id>] The variables to produce from a particular <stream_id> . [masking] Apply polar row masking if needed. [halo_removal] Apply stripping of halo columns and rows if needed. [slicing_periods] Using the slicing period for a particular stream if given. [global_attributes] Add global attributes to the output netCDF. COMMON The optional [COMMON] section. cmor_setup The [cmor_setup] section contains the following options which are used by cmor_setup() . For a description of each option please see the documentation for cmor_setup() . Option Required by Used by CMOR Name mip_table_dir MIP Convert CMOR + MIP Convert inpath netcdf_file_action CMOR set_verbosity CMOR exit_control CMOR cmor_log_file CMOR log_file create_subdirectories CMOR Tip When configuring a user configuration file, the mip_table_dir is likely to be the only value that will need modification. cmor_dataset The required cmor_dataset section contains the following options used for cmor_data_set_json() Option Required by Used by Notes branch_method MIP Convert + CMOR MIP Convert + CMOR [1] calendar MIP Convert + CMOR MIP Convert + CMOR [2] comment CMOR 1 contact CMOR [1] experiment_id MIP Convert + CMOR MIP Convert + CMOR [1] grid CMOR CMOR [1] grid_label CMOR CMOR [1] institution_id MIP Convert + CMOR MIP Convert + CMOR [1] license CMOR CMOR [1] mip CMOR CMOR 1 mip_era MIP Convert + CMOR MIP Convert + CMOR [1] model_id MIP Convert + CMOR MIP Convert + CMOR 1 model_type CMOR CMOR 1 nominal_resolution CMOR CMOR [1] output_dir MIP Convert + CMOR MIP Convert + CMOR [7] output_file_template CMOR output_path_template CMOR references CMOR [1] sub_experiment_id MIP Convert + CMOR MIP Convert [1] variant_info CMOR [1] variant_label MIP Convert + CMOR MIP Convert + CMOR [1] Notes For a description of each option, please see the CMIP6 Global Attributes document _. See calendars for allowed values. It is recommended to use the comment to record any perturbed physicsdetails. See MIP. See model identifier. See model type. See outpath in the documentation for cmor_dataset_json _. MIP Convert determines: the experiment , institution , source , sub_experiment from the CV file using the experiment_id , institution_id , source_id and sub_experiment_id , respectively the forcing_index , initialization_index , physics_index and realization_index from the variant_label the further_info_url and tracking_prefix based on the information from the CV file the history Whenever a parent experiment exists the following options must also be specified. Option Used by Notes branch_date_in_child MIP Convert 1 [3] branch_date_in_parent MIP Convert 1 [3] parent_base_date MIP Convert 1 parent_experiment_id CMOR [3] parent_mip_era CMOR [3] parent_model_id CMOR 3 parent_time_units CMOR [3] parent_variant_label CMOR [3] Notes CMOR requires branch_time_in_child and branch_time_in_parent , which is determined from the options base_date (see the request <request_section> section) / parent_base_date (the base date of the child_experiment_id / parent_experiment_id ) and branch_date_in_child / branch_date_in_parent (the date in the child_experiment_id / parent_experiment_id from which the experiment branches) from the cmor_dataset <cmor_dataset_section> section in the |user configuration file| by taking the difference (in days) between the branch_date_in_child / branch_date_in_parent and the base_date / parent_base_date . If branch_date_in_child or branch_date_in_parent is N/A then branch_time_in_parent is set to 0. Dates should be provided in the form YYYY-MM-DDThh:mm:ssZ . For a description of each option, please see the CMIP6 Global Attributes document . See parent_source_id in the CMIP6 Global Attributes document . request The required request section contains the following options which are used only by MIP Convert. Option Required Description Notes ancil_files A space separated list of the full paths to any required ancillary files. atmos_timestep The atmospheric model timestep in integer seconds. [1] base_date Yes The date in the form YYYY-MM-DDThh:mm:ss . [2] deflate_level The deflation level when writing the output netCDF file from 0 (no compression) to 9 (maximum compression). force_coordinate_rotation If set to True , output data will be forced to include rotated coordinates and true lat-lon coordinates. hybrid_heights_file A space separated list of the full path to the files containing the information about the hybrid heights. [3] mask_slice Yes Optional slicing expression for masking data in the form of n:m,i:j , or no_mask 4 model_output_dir Yes The full path to the root directory containing the model output files. [5] reference_time Yes The reference time used to construct reftime and leadtime coordinates. Only used if these coordinates are specified corresponding variable entries in the MIP table replacement_coordinates_file The full path to the netCDF file containing area variables that refer to the horizontal coordinates that should be used to replace the corresponding values in the model output files. [6] run_bounds Yes The start and end time in the form <start_time> <end_time> , where <start_time> and <end_time> are in the form YYYY-MM-DDThh:mm:ss . shuffle Whether to shuffle when writing the output netCDF file. sites_file The full path to the file containing the information about the sites. [7] suite_id Yes The suite identifier of the model. Notes The atmos_timestep is required for atmospheric tendency diagnostics, which have model to MIP mappings that depend on the atmospheric model timestep, i.e., the expression contains ATMOS_TIMESTEP . The base_date is used to define the units of the time coordinate in the output netCDF file and is specified by the MIP. The file containing the information about the hybrid heights has the following columns; the model level number (int) the a_value (float) the a_lower_bound (float) the a_upper_bound (float) the b_value (float) the b_lower_bound (float) the b_upper_bound (float) If not specified, mip_convert will try to retrieve masking expressions from plugins (this is a default behaviour for CMIP6-like processing). Putting no_mask into configuration file allows mip_convert to process model output that does not require any masking; custom masks can be specified and passed to mip_convert without plugins dependencies. It is expected that the model output files are located in the directory <model_output_dir>/<suite_id>/<stream_id>/ , where the <suite_id> is the suite identifier and the <stream_id> is the |stream identifier|. Note that MIP Convert will load all the files in this directory and then use the run_bounds to select the required data; when selecting a short time period from a large number of |model output files| it is recommended to copy the relevant files to an empty directory to save time when loading. Currently, only CICE horizontal coordinates can be replaced. The file containing the information about the sites has the following columns; the site number (int) the longitude (float, from 0 to 360) [degrees] the latitude (float, from -90 to 90) [degrees] the orography (float) [metres] and a comment (string). This is usually only used to mask ocean/seaice data as part of a CDDS run. stream_<\\stream_id> The required [stream_<stream_id>] section, where the <stream_id> is the stream identifier, contains options equal to the name of the MIP table and values equal to a space-separated list of MIP requested variable names. Multiple [stream_<stream_id>] sections can be defined. Note All output netCDF files are created for a stream before moving onto the next stream. Example If we wanted to produce the following variables Amon/tas , Amon/pr , Emon/ps , Amon/tasmax , day/tasmin using the CMIP6 tables. We need two stream_stream_id sections as the list of MIP requested variables span streams ap5 and ap6 . Within each of these sections we specify the mip table in the form <MIP_ERA>_<MIP_TABLE>: Then the list of variables names as a space seperated list. [stream_ap5] CMIP6_Amon: tas pr CMIP6_Emon: ps [stream_ap6] CMIP6_Amon: tasmax CMIP6_day: tasmin masking The optional masking section is used if a particular stream needs to be masked. This is usually only used for polar row masking in NEMO & CICE output Example [masking] stream_inm_cice-T: -1:,180: halo_removal The optional halo removal section is used if for a particular stream haloes are to be removed. Example [halo_removal] stream_apa: 5:,:-10 stream_ap6: 20:-15 slicing_periods The optional slicing_periods section is used if for a particular stream a period for slicing is specified. For all streams that have no specified slicing period the default slicing period is year . Example [slicing_periods] stream_apa: month stream_ap6: year global_attributes The optional global_attributes section. Any information provided in the optional global_attributes <global_attributes> section will be written to the header of the output netCDF files.","title":"MIP Convert User Guide"},{"location":"tutorials/mip_convert/#overview","text":"The mip_convert package enables a user to produce the output netCDF files for a MIP using model output files. graph LR A[model output .pp data] --> C[MIP Convert + CMOR]; B[model output .nc data] --> C; C --> D[CF/CMIP6 Compliant .nc data]; The user makes requests for one or more MIP requested variables by providing specific information (including the appropriate MIP requested variable names) in the user configuration file. The information required to produce the MIP requested variables is gathered from the user configuration file, the model to MIP mapping configuration files and the appropriate MIP table, in that order. The following steps are then performed for each MIP requested variable name in the user configuration file to produce the output netCDF files: load the relevant data from the model output files into one or more input variables depending on whether there is a one-to-one / simple arithmetic relationship between the MIP output variable and the input variables or if the MIP output variable is based on an arithmetic combination of two or more input variables, respectively, using Iris and the information provided in the user configuration file and the model to MIP mapping configuration files. process the input variable / input variables to produce the MIP output variable using the information provided in the model to MIP mapping configuration files. save the MIP output variable to an output netCDF file using CMOR and the information provided in the user configuration file and the appropriate MIP table.","title":"Overview"},{"location":"tutorials/mip_convert/#recommended-reading","text":"The Design Considerations and Overview section in the CMOR Documentation.","title":"Recommended Reading"},{"location":"tutorials/mip_convert/#quick-start-guide","text":"Download the template user configuration file mip_convert.cfg . Make the appropriate edits to the template user configuration file using the information provided in the \"User Configuration File\" section and the specified sections in the CMOR Documentation . Source an environment with cdds and verify that mip_convert runs. mip_convert -h Produce the output netCDF files by running mip_convert and passing in the modified user configuration file as an argument. mip_convert mip_convert.cfg Check the exit code echo $? Exit Code Meaning 0 No errors were raised during processing. 1 An exception was raised and no MIP requested variables were produced. 2 One or more MIP requested variables were produced but not all variables were produced. See the CRITICAL messages in the log for further information about the MIP requested variables not produced. Check that the output netCDF files are as expected. For help or to report an issue, please see support .","title":"Quick Start Guide"},{"location":"tutorials/mip_convert/#selected-mip-convert-arguments","text":"Argument Description config_file The name of the user configuration file. For more information, please see the MIP Convert user guide -s or --stream_identifiers The stream identifiers to process. If all streams should be processed, do not specify this option. --relaxed-cmor If specified, CMIP6 style validation is not performed by CMOR. If the validation is run then the following fields are not checked; model_id ( source_id ), experiment_id , further_info_url , grid_label , parent_experiment_id , sub_experiment_id . --mip_era The MIP era (e.g. CMIP6). --external_plugin Module path to external CDDS plugin (e.g. arise.plugin ) --external_plugin_location Path to the external plugin implementation (e.g. /project/cdds/arise )","title":"Selected MIP Convert Arguments"},{"location":"tutorials/mip_convert/#example-usage","text":"Run for all streams with full checking of metadata mip_convert mip_convert.cfg Run for a single stream in relaxed mode mip_convert mip_convert.cfg -s ap4 --relaxed_cmor","title":"Example Usage"},{"location":"tutorials/mip_convert/#user-configuration-file-reference","text":"The user configuration file provides the information required by MIP Convert to produce the output netCDF files. It contains the following sections, some of which are optional. Section Summary [COMMON] Convenience for setting up shared config values. [cmor_setup] Passed through to CMOR's cmor_setup() routine. [cmor_dataset] Passed through to CMOR's cmor_data_set_json() routine. [request] Configure mip_convert including input data. [stream_<stream_id>] The variables to produce from a particular <stream_id> . [masking] Apply polar row masking if needed. [halo_removal] Apply stripping of halo columns and rows if needed. [slicing_periods] Using the slicing period for a particular stream if given. [global_attributes] Add global attributes to the output netCDF.","title":"User Configuration File Reference"},{"location":"tutorials/mip_convert/#common","text":"The optional [COMMON] section.","title":"COMMON"},{"location":"tutorials/mip_convert/#cmor_setup","text":"The [cmor_setup] section contains the following options which are used by cmor_setup() . For a description of each option please see the documentation for cmor_setup() . Option Required by Used by CMOR Name mip_table_dir MIP Convert CMOR + MIP Convert inpath netcdf_file_action CMOR set_verbosity CMOR exit_control CMOR cmor_log_file CMOR log_file create_subdirectories CMOR Tip When configuring a user configuration file, the mip_table_dir is likely to be the only value that will need modification.","title":"cmor_setup"},{"location":"tutorials/mip_convert/#cmor_dataset","text":"The required cmor_dataset section contains the following options used for cmor_data_set_json() Option Required by Used by Notes branch_method MIP Convert + CMOR MIP Convert + CMOR [1] calendar MIP Convert + CMOR MIP Convert + CMOR [2] comment CMOR 1 contact CMOR [1] experiment_id MIP Convert + CMOR MIP Convert + CMOR [1] grid CMOR CMOR [1] grid_label CMOR CMOR [1] institution_id MIP Convert + CMOR MIP Convert + CMOR [1] license CMOR CMOR [1] mip CMOR CMOR 1 mip_era MIP Convert + CMOR MIP Convert + CMOR [1] model_id MIP Convert + CMOR MIP Convert + CMOR 1 model_type CMOR CMOR 1 nominal_resolution CMOR CMOR [1] output_dir MIP Convert + CMOR MIP Convert + CMOR [7] output_file_template CMOR output_path_template CMOR references CMOR [1] sub_experiment_id MIP Convert + CMOR MIP Convert [1] variant_info CMOR [1] variant_label MIP Convert + CMOR MIP Convert + CMOR [1] Notes For a description of each option, please see the CMIP6 Global Attributes document _. See calendars for allowed values. It is recommended to use the comment to record any perturbed physicsdetails. See MIP. See model identifier. See model type. See outpath in the documentation for cmor_dataset_json _. MIP Convert determines: the experiment , institution , source , sub_experiment from the CV file using the experiment_id , institution_id , source_id and sub_experiment_id , respectively the forcing_index , initialization_index , physics_index and realization_index from the variant_label the further_info_url and tracking_prefix based on the information from the CV file the history Whenever a parent experiment exists the following options must also be specified. Option Used by Notes branch_date_in_child MIP Convert 1 [3] branch_date_in_parent MIP Convert 1 [3] parent_base_date MIP Convert 1 parent_experiment_id CMOR [3] parent_mip_era CMOR [3] parent_model_id CMOR 3 parent_time_units CMOR [3] parent_variant_label CMOR [3] Notes CMOR requires branch_time_in_child and branch_time_in_parent , which is determined from the options base_date (see the request <request_section> section) / parent_base_date (the base date of the child_experiment_id / parent_experiment_id ) and branch_date_in_child / branch_date_in_parent (the date in the child_experiment_id / parent_experiment_id from which the experiment branches) from the cmor_dataset <cmor_dataset_section> section in the |user configuration file| by taking the difference (in days) between the branch_date_in_child / branch_date_in_parent and the base_date / parent_base_date . If branch_date_in_child or branch_date_in_parent is N/A then branch_time_in_parent is set to 0. Dates should be provided in the form YYYY-MM-DDThh:mm:ssZ . For a description of each option, please see the CMIP6 Global Attributes document . See parent_source_id in the CMIP6 Global Attributes document .","title":"cmor_dataset"},{"location":"tutorials/mip_convert/#request","text":"The required request section contains the following options which are used only by MIP Convert. Option Required Description Notes ancil_files A space separated list of the full paths to any required ancillary files. atmos_timestep The atmospheric model timestep in integer seconds. [1] base_date Yes The date in the form YYYY-MM-DDThh:mm:ss . [2] deflate_level The deflation level when writing the output netCDF file from 0 (no compression) to 9 (maximum compression). force_coordinate_rotation If set to True , output data will be forced to include rotated coordinates and true lat-lon coordinates. hybrid_heights_file A space separated list of the full path to the files containing the information about the hybrid heights. [3] mask_slice Yes Optional slicing expression for masking data in the form of n:m,i:j , or no_mask 4 model_output_dir Yes The full path to the root directory containing the model output files. [5] reference_time Yes The reference time used to construct reftime and leadtime coordinates. Only used if these coordinates are specified corresponding variable entries in the MIP table replacement_coordinates_file The full path to the netCDF file containing area variables that refer to the horizontal coordinates that should be used to replace the corresponding values in the model output files. [6] run_bounds Yes The start and end time in the form <start_time> <end_time> , where <start_time> and <end_time> are in the form YYYY-MM-DDThh:mm:ss . shuffle Whether to shuffle when writing the output netCDF file. sites_file The full path to the file containing the information about the sites. [7] suite_id Yes The suite identifier of the model. Notes The atmos_timestep is required for atmospheric tendency diagnostics, which have model to MIP mappings that depend on the atmospheric model timestep, i.e., the expression contains ATMOS_TIMESTEP . The base_date is used to define the units of the time coordinate in the output netCDF file and is specified by the MIP. The file containing the information about the hybrid heights has the following columns; the model level number (int) the a_value (float) the a_lower_bound (float) the a_upper_bound (float) the b_value (float) the b_lower_bound (float) the b_upper_bound (float) If not specified, mip_convert will try to retrieve masking expressions from plugins (this is a default behaviour for CMIP6-like processing). Putting no_mask into configuration file allows mip_convert to process model output that does not require any masking; custom masks can be specified and passed to mip_convert without plugins dependencies. It is expected that the model output files are located in the directory <model_output_dir>/<suite_id>/<stream_id>/ , where the <suite_id> is the suite identifier and the <stream_id> is the |stream identifier|. Note that MIP Convert will load all the files in this directory and then use the run_bounds to select the required data; when selecting a short time period from a large number of |model output files| it is recommended to copy the relevant files to an empty directory to save time when loading. Currently, only CICE horizontal coordinates can be replaced. The file containing the information about the sites has the following columns; the site number (int) the longitude (float, from 0 to 360) [degrees] the latitude (float, from -90 to 90) [degrees] the orography (float) [metres] and a comment (string). This is usually only used to mask ocean/seaice data as part of a CDDS run.","title":"request"},{"location":"tutorials/mip_convert/#stream_stream_id","text":"The required [stream_<stream_id>] section, where the <stream_id> is the stream identifier, contains options equal to the name of the MIP table and values equal to a space-separated list of MIP requested variable names. Multiple [stream_<stream_id>] sections can be defined. Note All output netCDF files are created for a stream before moving onto the next stream. Example If we wanted to produce the following variables Amon/tas , Amon/pr , Emon/ps , Amon/tasmax , day/tasmin using the CMIP6 tables. We need two stream_stream_id sections as the list of MIP requested variables span streams ap5 and ap6 . Within each of these sections we specify the mip table in the form <MIP_ERA>_<MIP_TABLE>: Then the list of variables names as a space seperated list. [stream_ap5] CMIP6_Amon: tas pr CMIP6_Emon: ps [stream_ap6] CMIP6_Amon: tasmax CMIP6_day: tasmin","title":"stream_&lt;\\stream_id&gt;"},{"location":"tutorials/mip_convert/#masking","text":"The optional masking section is used if a particular stream needs to be masked. This is usually only used for polar row masking in NEMO & CICE output Example [masking] stream_inm_cice-T: -1:,180:","title":"masking"},{"location":"tutorials/mip_convert/#halo_removal","text":"The optional halo removal section is used if for a particular stream haloes are to be removed. Example [halo_removal] stream_apa: 5:,:-10 stream_ap6: 20:-15","title":"halo_removal"},{"location":"tutorials/mip_convert/#slicing_periods","text":"The optional slicing_periods section is used if for a particular stream a period for slicing is specified. For all streams that have no specified slicing period the default slicing period is year . Example [slicing_periods] stream_apa: month stream_ap6: year","title":"slicing_periods"},{"location":"tutorials/mip_convert/#global_attributes","text":"The optional global_attributes section. Any information provided in the optional global_attributes <global_attributes> section will be written to the header of the output netCDF files.","title":"global_attributes"},{"location":"tutorials/search_inventory/","text":"Script: search_inventory In release v1.6.0 we've added an inventory (updated each night) that is used by CDDS Prepare to avoid the need for combined \"approved_variables\" files to avoid duplicating previously provided data. The script allows the user to search the inventory using data set id patterns which can include wildcards instead of a particular \"facet\". The inventory database We generate a small sqlite3 database each night from the contents of MASS. As such this only contains data that we have produced within the Met Office and archived to MASS. Data set ids and \"facet\" structure The dataset ids used in CMIP6 are made up of a set of \"facets\", strings describing some aspect of what the data relates to. A facet may be the name of the MIP, experiment_id or variable name. The format for CMIP6 is CMIP6.<activity id (MIP)>.<institution id>.<source_id (model)>.<experiment id>.<variant label>.<MIP table>.<variable id>.<grid label> e.g. CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.historical.r1i1p1f2.Amon.tas.gn Search for datasets matching a facet pattern. The data set id used for the inventory is a set of 9 facets joined by dots. To search for all UKESM1 daily precipitation datasets you can run search_inventory CMIP6.*.*.UKESM1-0-LL.*.*.day.pr.* Which at the time of writing returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r1i1p1f2 day pr gn available v20190813 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r1i1p1f2.day.pr.gn CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r2i1p1f2 day pr gn available v20191104 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r2i1p1f2.day.pr.gn ... CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r4i1p1f2 day pr gn available v20190814 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r4i1p1f2.day.pr.gn CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r8i1p1f2 day pr gn available v20190906 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r8i1p1f2.day.pr.gn A total of 239 records were found. Locating data in MASS If you require the list of files in MASS for a particular dataset then the -s option will provide this for each data set matching the pattern in the inventory. For example search_inventory CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.*.pr.gn -s returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 Amon pr gn available v20190507 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.Amon.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_201501-204912.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_205001-210012.nc CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 day pr gn available v20190813 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.day.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20150101-20491230.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20500101-21001230.nc A total of 2 records were found. Note that in this case a moo ls command is run for each data set found.","title":"Search Inventory"},{"location":"tutorials/search_inventory/#script-search_inventory","text":"In release v1.6.0 we've added an inventory (updated each night) that is used by CDDS Prepare to avoid the need for combined \"approved_variables\" files to avoid duplicating previously provided data. The script allows the user to search the inventory using data set id patterns which can include wildcards instead of a particular \"facet\".","title":"Script: search_inventory"},{"location":"tutorials/search_inventory/#the-inventory-database","text":"We generate a small sqlite3 database each night from the contents of MASS. As such this only contains data that we have produced within the Met Office and archived to MASS.","title":"The inventory database"},{"location":"tutorials/search_inventory/#data-set-ids-and-facet-structure","text":"The dataset ids used in CMIP6 are made up of a set of \"facets\", strings describing some aspect of what the data relates to. A facet may be the name of the MIP, experiment_id or variable name. The format for CMIP6 is CMIP6.<activity id (MIP)>.<institution id>.<source_id (model)>.<experiment id>.<variant label>.<MIP table>.<variable id>.<grid label> e.g. CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.historical.r1i1p1f2.Amon.tas.gn","title":"Data set ids and \"facet\" structure"},{"location":"tutorials/search_inventory/#search-for-datasets-matching-a-facet-pattern","text":"The data set id used for the inventory is a set of 9 facets joined by dots. To search for all UKESM1 daily precipitation datasets you can run search_inventory CMIP6.*.*.UKESM1-0-LL.*.*.day.pr.* Which at the time of writing returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r1i1p1f2 day pr gn available v20190813 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r1i1p1f2.day.pr.gn CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r2i1p1f2 day pr gn available v20191104 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r2i1p1f2.day.pr.gn ... CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r4i1p1f2 day pr gn available v20190814 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r4i1p1f2.day.pr.gn CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r8i1p1f2 day pr gn available v20190906 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r8i1p1f2.day.pr.gn A total of 239 records were found.","title":"Search for datasets matching a facet pattern."},{"location":"tutorials/search_inventory/#locating-data-in-mass","text":"If you require the list of files in MASS for a particular dataset then the -s option will provide this for each data set matching the pattern in the inventory. For example search_inventory CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.*.pr.gn -s returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 Amon pr gn available v20190507 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.Amon.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_201501-204912.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_205001-210012.nc CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 day pr gn available v20190813 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.day.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20150101-20491230.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20500101-21001230.nc A total of 2 records were found. Note that in this case a moo ls command is run for each data set found.","title":"Locating data in MASS"},{"location":"tutorials/request/common/","text":"Common Section The common section in the request configuration contains common setting like the path to the root data folder or the path to the external plugin. Configuration Values external_plugin module path to external CDDS plugin, e.g. cdds_arise.arise_plugin external_plugin_location path to the external plugin implementation, e.g. /home/h03/cdds/arise mip_table_dir path to the directory containing the inforamtion about the MIP tables, e.g. /home/h03/cdds/etc/mip_tables/CMIP6/01.00.29/ Default: The path to the MIP table directoring specified in the corresponding plugin mode mode that should be used to CMORised the data - strict or relaxed . If mode is strict , CMOR will fail if there are warnings regarding compliance of CMOR standards. If mode is relaxed , CMOR will only fail if cubes present serve discrepancies with CMOR standards. Default: strict package package name used to distinguish different run throughs of CDDS, e.g. round-1 . workflow_basename name of the workflow name of the CDDS suite that will trigger during the CDDS processing suite Default : <model_id>_<experiment_id>_<variant_label> - all three values are defined in the metadata section root_proc_dir root path to the proc directory where the non-data outputs are written, e.g. log files. root_data_dir root path to the data directory where the output files of the model are written to. root_ancil_dir root path to the location of the ancillary files. The files should be located in a sub-directory of this path with the name of the model ID. Default: Path to the directory containing the ancillary files located in the CDDS home directory root_hybrid_heights_dir root path to the location of the hybrid heights files. Default:: Path to the directory containing hybrid heights files located in the CDDS home directory root_replacement_coordinates_dir root path to the location of the replacement coordinates files. Default: Path to the directory containing the replacement coordinates files located in the CDDS home directory sites_files path to the file containing the sites information. Default: Path to the file containing the site information located in the CDDS home directory standard_names_dir the directory containing the standard names that should be used. Default : Path to the standard names directory located in the CDDS home directory standard_names_version the version of the standard name directory that should be used. Default: latest simulation if set to True CDDS operation will be simulated log_level identify which log level should be used for logging - CRITICAL , INFO or DEBUG Default: INFO Examples Example [ common ] external_plugin = external_plugin_location = mip_table_dir = /home/h03/cdds/etc/mip_tables/CMIP6/01.00.29/ mode = strict package = round-1-part-1 workflow_basename = UKESM1-0-LL_historical_r1i1p1f2 root_proc_dir = /project/user/proc root_data_dir = /project/user/cdds_data root_ancil_dir = /home/h03/cdds/etc/ancil/ root_hybrid_heights_dir = /home/h03/cdds/etc/vertical_coordinates/ root_replacement_coordinates_dir = /home/h03/cdds/etc/horizontal_coordinates/ sites_file = /home/h03/cdds/etc/cfmip2/cfmip2-sites-orog.txt simulation = False log_level = INFO","title":"Common Section"},{"location":"tutorials/request/common/#common-section","text":"The common section in the request configuration contains common setting like the path to the root data folder or the path to the external plugin.","title":"Common Section"},{"location":"tutorials/request/common/#configuration-values","text":"external_plugin module path to external CDDS plugin, e.g. cdds_arise.arise_plugin external_plugin_location path to the external plugin implementation, e.g. /home/h03/cdds/arise mip_table_dir path to the directory containing the inforamtion about the MIP tables, e.g. /home/h03/cdds/etc/mip_tables/CMIP6/01.00.29/ Default: The path to the MIP table directoring specified in the corresponding plugin mode mode that should be used to CMORised the data - strict or relaxed . If mode is strict , CMOR will fail if there are warnings regarding compliance of CMOR standards. If mode is relaxed , CMOR will only fail if cubes present serve discrepancies with CMOR standards. Default: strict package package name used to distinguish different run throughs of CDDS, e.g. round-1 . workflow_basename name of the workflow name of the CDDS suite that will trigger during the CDDS processing suite Default : <model_id>_<experiment_id>_<variant_label> - all three values are defined in the metadata section root_proc_dir root path to the proc directory where the non-data outputs are written, e.g. log files. root_data_dir root path to the data directory where the output files of the model are written to. root_ancil_dir root path to the location of the ancillary files. The files should be located in a sub-directory of this path with the name of the model ID. Default: Path to the directory containing the ancillary files located in the CDDS home directory root_hybrid_heights_dir root path to the location of the hybrid heights files. Default:: Path to the directory containing hybrid heights files located in the CDDS home directory root_replacement_coordinates_dir root path to the location of the replacement coordinates files. Default: Path to the directory containing the replacement coordinates files located in the CDDS home directory sites_files path to the file containing the sites information. Default: Path to the file containing the site information located in the CDDS home directory standard_names_dir the directory containing the standard names that should be used. Default : Path to the standard names directory located in the CDDS home directory standard_names_version the version of the standard name directory that should be used. Default: latest simulation if set to True CDDS operation will be simulated log_level identify which log level should be used for logging - CRITICAL , INFO or DEBUG Default: INFO","title":"Configuration Values"},{"location":"tutorials/request/common/#examples","text":"Example [ common ] external_plugin = external_plugin_location = mip_table_dir = /home/h03/cdds/etc/mip_tables/CMIP6/01.00.29/ mode = strict package = round-1-part-1 workflow_basename = UKESM1-0-LL_historical_r1i1p1f2 root_proc_dir = /project/user/proc root_data_dir = /project/user/cdds_data root_ancil_dir = /home/h03/cdds/etc/ancil/ root_hybrid_heights_dir = /home/h03/cdds/etc/vertical_coordinates/ root_replacement_coordinates_dir = /home/h03/cdds/etc/horizontal_coordinates/ sites_file = /home/h03/cdds/etc/cfmip2/cfmip2-sites-orog.txt simulation = False log_level = INFO","title":"Examples"},{"location":"tutorials/request/config_request/","text":"Update Request Configuration The request configuration consists of following different sections: inheritance contains a setting that specify a template for the request. This section is optional. metadata contains all metadata settings about the experiment that should be processed, like the model ID or the MIP era. common contains common setting like the path to the root data folder or the path to the external plugin. data contains all settings that are used to archive the data in MASS. inventory contains all settings that are need to connect to and manage the inventory database. conversion contains settings that specify how CDDS is run, e.g., skip any steps when running CDDS. netcdf_global_attributes contains all attributes that will be set in the global attributes section of the CMOR file. misc contains any settings that do not fit in any other section.","title":"Request Configuration"},{"location":"tutorials/request/config_request/#update-request-configuration","text":"The request configuration consists of following different sections: inheritance contains a setting that specify a template for the request. This section is optional. metadata contains all metadata settings about the experiment that should be processed, like the model ID or the MIP era. common contains common setting like the path to the root data folder or the path to the external plugin. data contains all settings that are used to archive the data in MASS. inventory contains all settings that are need to connect to and manage the inventory database. conversion contains settings that specify how CDDS is run, e.g., skip any steps when running CDDS. netcdf_global_attributes contains all attributes that will be set in the global attributes section of the CMOR file. misc contains any settings that do not fit in any other section.","title":"Update Request Configuration"},{"location":"tutorials/request/conversion/","text":"Conversion Section The conversion in the request configuration contains settings that specify how CDDS is run. Configuration Values skip_extract skip the extract task at the start of the CDDS suite for each stream. Default : False , on Jasmin True skip_extract_validation skip validation at the end of the extract task. Default: False skip_configure skip the configure step of the CDDS suite for each stream. Default: False skip_qc skip the quality control task of the CDDS suite for each stream. Default: False skip_archive skip the archive task at the end of the CDDS suite for each stream. Default: False , on Jasmin True cdds_workflow_branch branch of the CDDS suite that should be used. Default: trunk , on Jasmin the latest CDDS relase branch, e.g. cdds_jasmin_2.3 cylc_args arguments to be passed to cylc vip for the CDDS processing suite. Default: -v no_email_notifications no email notifications will be sent from the suites. Default: True scale_memory_limits memory scaling factor to be applied to all batch jobs. Please, contact the CDDS team for advice. override_cycling_frequency override default frequency for specified stream. Each stream should be specified along with the cycling frequency using the format <stream>=<frequency> , e.g. ap7=P3M ap8=P1M . slicing slicing period for specified stream. Each stream should be specified along with the slicing period using the format <stream>=<period> , e.g. ap7=year ap8=month . Only, year and month as period are supported. model_params_dir the path to the directory containing the model parameters that should be overloaded the model parameters in the plugin. continue_if_mip_convert_failed specify if the MIP convert task will fail if any errors occur or only if the data of all streams can not be processed. Default: False delete_preexisting_proc_dir specify if a pre-existing CDDS proc directory will be deleted when creating the CDDS directories. Default: False delete_preexisting_data_dir specify if a pre-existing CDDS data directory will be deleted when creating the CDDS directories. Default: False Examples Example [ conversion ] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cdds_workflow_branch = trunk cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = ap7=P3M model_params_dir = /home/user/model_params.json continue_if_mip_convert_failed = False delete_preexisting_proc_dir = False delete_preexisting_data_dir = False","title":"Conversion Section"},{"location":"tutorials/request/conversion/#conversion-section","text":"The conversion in the request configuration contains settings that specify how CDDS is run.","title":"Conversion Section"},{"location":"tutorials/request/conversion/#configuration-values","text":"skip_extract skip the extract task at the start of the CDDS suite for each stream. Default : False , on Jasmin True skip_extract_validation skip validation at the end of the extract task. Default: False skip_configure skip the configure step of the CDDS suite for each stream. Default: False skip_qc skip the quality control task of the CDDS suite for each stream. Default: False skip_archive skip the archive task at the end of the CDDS suite for each stream. Default: False , on Jasmin True cdds_workflow_branch branch of the CDDS suite that should be used. Default: trunk , on Jasmin the latest CDDS relase branch, e.g. cdds_jasmin_2.3 cylc_args arguments to be passed to cylc vip for the CDDS processing suite. Default: -v no_email_notifications no email notifications will be sent from the suites. Default: True scale_memory_limits memory scaling factor to be applied to all batch jobs. Please, contact the CDDS team for advice. override_cycling_frequency override default frequency for specified stream. Each stream should be specified along with the cycling frequency using the format <stream>=<frequency> , e.g. ap7=P3M ap8=P1M . slicing slicing period for specified stream. Each stream should be specified along with the slicing period using the format <stream>=<period> , e.g. ap7=year ap8=month . Only, year and month as period are supported. model_params_dir the path to the directory containing the model parameters that should be overloaded the model parameters in the plugin. continue_if_mip_convert_failed specify if the MIP convert task will fail if any errors occur or only if the data of all streams can not be processed. Default: False delete_preexisting_proc_dir specify if a pre-existing CDDS proc directory will be deleted when creating the CDDS directories. Default: False delete_preexisting_data_dir specify if a pre-existing CDDS data directory will be deleted when creating the CDDS directories. Default: False","title":"Configuration Values"},{"location":"tutorials/request/conversion/#examples","text":"Example [ conversion ] skip_extract = False skip_extract_validation = False skip_configure = False skip_qc = False skip_archive = False cdds_workflow_branch = trunk cylc_args = -v no_email_notifications = True scale_memory_limits = override_cycling_frequency = ap7=P3M model_params_dir = /home/user/model_params.json continue_if_mip_convert_failed = False delete_preexisting_proc_dir = False delete_preexisting_data_dir = False","title":"Examples"},{"location":"tutorials/request/data/","text":"Data Section The data section in the request configuration contains all settings that are used to archive the data in MASS. Configuration Values data_version version of the data to archive. Format: v%Y%m%d Default: version of current date end_date end date for the processing. start_date start date for the processing. mass_data_class root of the location of input dataset on MASS, either crum or ens . Default: crum mass_ensemble_member identifier of the ensemble member for PPE simulations model_workflow_id workflow ID of the simulation model. max_file_size maximum netCDF file size in bytes Default: 20e9 (20GB) model_workflow_branch name of the workflow branch of the simulation model. Default: cdds model_workflow_revision workflow revision of the simulation model. Default: HEAD streams restrict only to these streams. If empty, all streams will be processed. variable_list_file path to the user created variables file. output_mass_root full path to the root MASS location to use for archiving the output data, e.g. moose:/adhoc/users/<user.name>/ output_mass_suffix sub-directory in MASS to use when moving data. This directory is appended to the root mass location defined in output_mass_root . Examples Example [ data ] data_version = v20240212 end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = max_file_size = 20e9 start_date = 1979-01-01T00:00:00Z model_workflow_id = u-bp880 model_workflow_branch = cdds model_workflow_revision = HEAD streams = ap4 ap5 ap6 variable_list_file = /home/h03/cdds/variables/variables.txt output_mass_root = moose:/adhoc/users/some.user/ output_mass_suffix = cdds","title":"Data Section"},{"location":"tutorials/request/data/#data-section","text":"The data section in the request configuration contains all settings that are used to archive the data in MASS.","title":"Data Section"},{"location":"tutorials/request/data/#configuration-values","text":"data_version version of the data to archive. Format: v%Y%m%d Default: version of current date end_date end date for the processing. start_date start date for the processing. mass_data_class root of the location of input dataset on MASS, either crum or ens . Default: crum mass_ensemble_member identifier of the ensemble member for PPE simulations model_workflow_id workflow ID of the simulation model. max_file_size maximum netCDF file size in bytes Default: 20e9 (20GB) model_workflow_branch name of the workflow branch of the simulation model. Default: cdds model_workflow_revision workflow revision of the simulation model. Default: HEAD streams restrict only to these streams. If empty, all streams will be processed. variable_list_file path to the user created variables file. output_mass_root full path to the root MASS location to use for archiving the output data, e.g. moose:/adhoc/users/<user.name>/ output_mass_suffix sub-directory in MASS to use when moving data. This directory is appended to the root mass location defined in output_mass_root .","title":"Configuration Values"},{"location":"tutorials/request/data/#examples","text":"Example [ data ] data_version = v20240212 end_date = 2015-01-01T00:00:00Z mass_data_class = crum mass_ensemble_member = max_file_size = 20e9 start_date = 1979-01-01T00:00:00Z model_workflow_id = u-bp880 model_workflow_branch = cdds model_workflow_revision = HEAD streams = ap4 ap5 ap6 variable_list_file = /home/h03/cdds/variables/variables.txt output_mass_root = moose:/adhoc/users/some.user/ output_mass_suffix = cdds","title":"Examples"},{"location":"tutorials/request/global_attributes/","text":"NetCDF Global Attributes Section The netcdf_global_attributes in the request configuration contains all attributes that will be set in the global attributes section of the CMOR file. Configuration Values attributes attributes that will be set in the global attributes section of the CMOR file. Examples Example NetCDF global attributes of a CORDEX experiment: [ netcdf_global_attributes ] driving_experiment = evaluation driving_experiment_id = evaluation driving_institution_id = MOHC driving_source_id = HadREM3-GA7-05 driving_model_ensemble_member = r1i1p1 driving_experiment_name = evaluation driving_variant_label = r1i1p1f2 nesting_levels = 1 rcm_version_id = v1 project_id = CORDEX-FPSCONV domain_id = EUR-11 domain = Europe source_configuration_id = v1.0 further_info_url = https://furtherinfo.es-doc.org/","title":"Global Attributes Section"},{"location":"tutorials/request/global_attributes/#netcdf-global-attributes-section","text":"The netcdf_global_attributes in the request configuration contains all attributes that will be set in the global attributes section of the CMOR file.","title":"NetCDF Global Attributes Section"},{"location":"tutorials/request/global_attributes/#configuration-values","text":"attributes attributes that will be set in the global attributes section of the CMOR file.","title":"Configuration Values"},{"location":"tutorials/request/global_attributes/#examples","text":"Example NetCDF global attributes of a CORDEX experiment: [ netcdf_global_attributes ] driving_experiment = evaluation driving_experiment_id = evaluation driving_institution_id = MOHC driving_source_id = HadREM3-GA7-05 driving_model_ensemble_member = r1i1p1 driving_experiment_name = evaluation driving_variant_label = r1i1p1f2 nesting_levels = 1 rcm_version_id = v1 project_id = CORDEX-FPSCONV domain_id = EUR-11 domain = Europe source_configuration_id = v1.0 further_info_url = https://furtherinfo.es-doc.org/","title":"Examples"},{"location":"tutorials/request/inheritance/","text":"Inheritance Section The inheritance in the request configuration is optional and contains a path to a template for the request. Configuration Values template path to the template for the request that contains values that will be added to the request configuration. The request configuration can override values in the template. Examples Example [ inheritance ] template = /path/to/request-template.cfg","title":"Inheritance Section"},{"location":"tutorials/request/inheritance/#inheritance-section","text":"The inheritance in the request configuration is optional and contains a path to a template for the request.","title":"Inheritance Section"},{"location":"tutorials/request/inheritance/#configuration-values","text":"template path to the template for the request that contains values that will be added to the request configuration. The request configuration can override values in the template.","title":"Configuration Values"},{"location":"tutorials/request/inheritance/#examples","text":"Example [ inheritance ] template = /path/to/request-template.cfg","title":"Examples"},{"location":"tutorials/request/inventory/","text":"Inventory Section The inventory section in the request configuration contains all settings to connect to and manage the inventory database. Configuration Values inventory_check specify if the inventory should be used to determine of a variables is active or not. Default: False inventory_database_location the path to the inventory database. Examples Example [ inventory ] inventory_check = True inventory_database_location = /home/user/inventory.db","title":"Inventory Section"},{"location":"tutorials/request/inventory/#inventory-section","text":"The inventory section in the request configuration contains all settings to connect to and manage the inventory database.","title":"Inventory Section"},{"location":"tutorials/request/inventory/#configuration-values","text":"inventory_check specify if the inventory should be used to determine of a variables is active or not. Default: False inventory_database_location the path to the inventory database.","title":"Configuration Values"},{"location":"tutorials/request/inventory/#examples","text":"Example [ inventory ] inventory_check = True inventory_database_location = /home/user/inventory.db","title":"Examples"},{"location":"tutorials/request/metadata/","text":"Metadata Section The metadata section in the request configuration contains all metadata settings about the experiment that should be processed. Configuration Values branch_method branching procedure - standard , continuation or no parent base_date used to define the units of the time coordinate in the netCDF file. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 1850-01-01T00:00:00Z Default: 1850-01-01T00:00:00Z calendar that should be used - 360_day or gregorian experiment_id experiment identifier institution_id institution identifier, e.g. MOHC for Met Office Hadley Centre license license restrictions. It ensures that anyone using the files has access to the terms of use. Default: The licenses that the corresponding plugin is provided for the given MIP era. mip the model intercomparison project, e.g. ScenarioMIP mip_era the associated cycle, e.g. CMIP6 , GCDevModel , CORDEX sub_experiment_id identifier of the sub experiment. For example, it is needed for CMIP6 hindcast or forecast experiments to indicate start year. If no sub experiment is given, set it to none Default: none variant_label the label of the variant of the experiment that should be considered. model_id a short name identifying the model, also know as source_id , e.g. HadGEM3-GC31-LL . model_type a text code identifying which model components are used in the given experiments separated by white spaces, e.g. AOGCM AER BGC Configuration values for parent experiment Following configuration values are only needed to be set if the branch_method is standard or continuation : branch_date_in_child branch data with respect to child's time axis. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 1990-01-01T00:00:00Z branch_date_in_parent branch date with respect to parent time axis Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 2000-01-01T00:00:00Z parent_base_date used to define the units of the time coordinate in the netCDF files. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 2005-01-01T00:00:00Z Default: 1850-01-01T00:00:00Z parent_experiment_id parent experiment identifier, e.g. piControl parent_mip the model intercomparison project of the parent experiment parent_mip_era parent's associated MIP cycle, e.g. CMIP6 parent_model_id a short name identifying the parent model, e.g. HadGEM3-GC31-LL . Default : The same model ID of the experiment that should be processed parent_time_units the time units used in the parent experiment. Default: days since 1850-01-01 parent_variant_label the label of the specific variant of the parent experiment that should be considered. Examples CIMP6 experiment without parent [ metadata ] branch_method = no parent calendar = 360_day experiment_id = amip institution_id = MOHC license = CMIP6 model data produced by MOHC is licensed under a Creative Commons Attribution ShareAlike 4.0 International License mip = CMIP mip_era = CMIP6 sub_experiment_id = none variant_label = r1i1p1f4 model_id = UKESM1-0-LL model_type = AGCM AER CHEM standard_names_version = latest standard_names_dir = /home/h03/cdds/etc/standard_names CMIP6 experiment with parent [ metadata ] branch_date_in_child = 1850-01-01T00:00:00Z branch_date_in_parent = 2250-01-01T00:00:00Z branch_method = standard child_base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = historical institution_id = MOHC license = CMIP6 model data produced by MOHC is licensed under a Creative Commons Attribution ShareAlike 4.0 International License. mip = CMIP mip_era = CMIP6 parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = piControl parent_mip = CMIP parent_mip_era = CMIP6 parent_model_id = UKESM1-0-LL parent_time_units = days since 1850-01-01 parent_variant_label = r1i1p1f2 sub_experiment_id = none variant_label = r1i1p1f2 standard_names_version = latest standard_names_dir = /home/h03/cdds/etc/standard_names/ model_id = UKESM1-0-LL model_type = AOGCM BGC AER CHEM","title":"Metadata Section"},{"location":"tutorials/request/metadata/#metadata-section","text":"The metadata section in the request configuration contains all metadata settings about the experiment that should be processed.","title":"Metadata Section"},{"location":"tutorials/request/metadata/#configuration-values","text":"branch_method branching procedure - standard , continuation or no parent base_date used to define the units of the time coordinate in the netCDF file. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 1850-01-01T00:00:00Z Default: 1850-01-01T00:00:00Z calendar that should be used - 360_day or gregorian experiment_id experiment identifier institution_id institution identifier, e.g. MOHC for Met Office Hadley Centre license license restrictions. It ensures that anyone using the files has access to the terms of use. Default: The licenses that the corresponding plugin is provided for the given MIP era. mip the model intercomparison project, e.g. ScenarioMIP mip_era the associated cycle, e.g. CMIP6 , GCDevModel , CORDEX sub_experiment_id identifier of the sub experiment. For example, it is needed for CMIP6 hindcast or forecast experiments to indicate start year. If no sub experiment is given, set it to none Default: none variant_label the label of the variant of the experiment that should be considered. model_id a short name identifying the model, also know as source_id , e.g. HadGEM3-GC31-LL . model_type a text code identifying which model components are used in the given experiments separated by white spaces, e.g. AOGCM AER BGC","title":"Configuration Values"},{"location":"tutorials/request/metadata/#configuration-values-for-parent-experiment","text":"Following configuration values are only needed to be set if the branch_method is standard or continuation : branch_date_in_child branch data with respect to child's time axis. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 1990-01-01T00:00:00Z branch_date_in_parent branch date with respect to parent time axis Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 2000-01-01T00:00:00Z parent_base_date used to define the units of the time coordinate in the netCDF files. Format: yyyy-mm-ddTHH:MM:SSZ , e.g. 2005-01-01T00:00:00Z Default: 1850-01-01T00:00:00Z parent_experiment_id parent experiment identifier, e.g. piControl parent_mip the model intercomparison project of the parent experiment parent_mip_era parent's associated MIP cycle, e.g. CMIP6 parent_model_id a short name identifying the parent model, e.g. HadGEM3-GC31-LL . Default : The same model ID of the experiment that should be processed parent_time_units the time units used in the parent experiment. Default: days since 1850-01-01 parent_variant_label the label of the specific variant of the parent experiment that should be considered.","title":"Configuration values for parent experiment"},{"location":"tutorials/request/metadata/#examples","text":"CIMP6 experiment without parent [ metadata ] branch_method = no parent calendar = 360_day experiment_id = amip institution_id = MOHC license = CMIP6 model data produced by MOHC is licensed under a Creative Commons Attribution ShareAlike 4.0 International License mip = CMIP mip_era = CMIP6 sub_experiment_id = none variant_label = r1i1p1f4 model_id = UKESM1-0-LL model_type = AGCM AER CHEM standard_names_version = latest standard_names_dir = /home/h03/cdds/etc/standard_names CMIP6 experiment with parent [ metadata ] branch_date_in_child = 1850-01-01T00:00:00Z branch_date_in_parent = 2250-01-01T00:00:00Z branch_method = standard child_base_date = 1850-01-01T00:00:00Z calendar = 360_day experiment_id = historical institution_id = MOHC license = CMIP6 model data produced by MOHC is licensed under a Creative Commons Attribution ShareAlike 4.0 International License. mip = CMIP mip_era = CMIP6 parent_base_date = 1850-01-01T00:00:00Z parent_experiment_id = piControl parent_mip = CMIP parent_mip_era = CMIP6 parent_model_id = UKESM1-0-LL parent_time_units = days since 1850-01-01 parent_variant_label = r1i1p1f2 sub_experiment_id = none variant_label = r1i1p1f2 standard_names_version = latest standard_names_dir = /home/h03/cdds/etc/standard_names/ model_id = UKESM1-0-LL model_type = AOGCM BGC AER CHEM","title":"Examples"},{"location":"tutorials/request/misc/","text":"Miscellany Section The misc in the request configuration contains any settings that do not fit in any other section. Configuration Values atmos_timestep atmospheric time step in seconds Default: The atmospheric time step that the current loaded plugin is provided. use_proc_dir write log files to the appropriate component directory in the proc directory as defined by the common section in the request configuration. Default: True no_overwrite do not overwrite files in data directory defined by the common section in the request configuration. Default: False halo_removal_latitude number of latitude points to be stripped using <start>:<stop> as the format. halo_removal_longitude number of longitude points to be stripped using <start>:<stop> as the format. Example Strip 5 points at the start and end for latitude and strip 10 points at the start and end for longitude: halo_removal_latitude = 5:-5 halo_removal_longitude = 10:-10 force_coordinate_rotation Set to True to enable coordination rotation if coordination system is not rotated by default. Default: False Examples Example [ misc ] atmos_timestep = 900 use_proc_dir = True no_overwrite = False halo_removal_latitude = 5:-5 halo_removal_longitude = 10:-10 force_coordinate_rotation = False","title":"Misc Section"},{"location":"tutorials/request/misc/#miscellany-section","text":"The misc in the request configuration contains any settings that do not fit in any other section.","title":"Miscellany Section"},{"location":"tutorials/request/misc/#configuration-values","text":"atmos_timestep atmospheric time step in seconds Default: The atmospheric time step that the current loaded plugin is provided. use_proc_dir write log files to the appropriate component directory in the proc directory as defined by the common section in the request configuration. Default: True no_overwrite do not overwrite files in data directory defined by the common section in the request configuration. Default: False halo_removal_latitude number of latitude points to be stripped using <start>:<stop> as the format. halo_removal_longitude number of longitude points to be stripped using <start>:<stop> as the format. Example Strip 5 points at the start and end for latitude and strip 10 points at the start and end for longitude: halo_removal_latitude = 5:-5 halo_removal_longitude = 10:-10 force_coordinate_rotation Set to True to enable coordination rotation if coordination system is not rotated by default. Default: False","title":"Configuration Values"},{"location":"tutorials/request/misc/#examples","text":"Example [ misc ] atmos_timestep = 900 use_proc_dir = True no_overwrite = False halo_removal_latitude = 5:-5 halo_removal_longitude = 10:-10 force_coordinate_rotation = False","title":"Examples"}]}